{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"Overview of GreatAI <p>Applying AI is becoming increasingly easier, but many case studies have shown that these applications are often deployed poorly. This may lead to suboptimal performance and to introducing unintended biases. GreatAI helps fix this by allowing you to easily transform your prototype AI code into production-ready software.</p> Case studies <p>\"There is a need to consider and adapt well established SE practices which have been ignored or had a very narrow focus in ML literature.\" \u2014 John et al.</p> <p>\"Finally, we have found that existing tools to aid Machine Learning development do not address the specificities of different projects, and thus, are seldom adopted by teams.\" \u2014 Haakman et al.</p> <p>\"Because a mature system might end up being (at most) 5% machine learning code and (at least) 95% glue code, it may be less costly to create a clean native solution rather than re-use a generic package.\" \u2014 Sculley et al.</p> <p>\"For example, practice 25 is very important for \"Traceability\", yet relatively weakly adopted. We expect that the results from this type of analysis can, in the future, provide useful guidance for practitioners in terms of aiding them to assess their rate of adoption for each practice and to create roadmaps for improving their processes.  \u2014 Serban et al.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> Save prediction traces of each prediction, including arguments and model versions</li> <li> Save feedback and merge it into a ground-truth database</li> <li> Version and store models and data on shared infrastructure (MongoDB GridFS, S3-compatible storage, shared volume)</li> <li> Automatically scaffolded custom REST API (and OpenAPI schema) for easy integration</li> <li> Input validation</li> <li> Sensible cache-policy</li> <li> Graceful error handling</li> <li> Seamless support for both synchronous and asynchronous inference methods</li> <li> Easy integration with remote GreatAI instances</li> <li> Built-in parallelisation (with support for multiprocessing, async, and mixed modes) for batch processing</li> <li> Well-tested utilities for common NLP tasks (cleaning, language-tagging, sentence-segmentation, etc.)</li> <li> A simple, unified configuration interface</li> <li> Fully-typed API for Pylance and MyPy support</li> <li> Auto-reload for development</li> <li> Docker support for deployment</li> <li> Deployable Jupyter Notebooks</li> <li> Dashboard for online monitoring and analysing traces</li> <li> Active support for Python 3.7, 3.8, 3.9, and 3.10</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Prometheus &amp; Grafana integration</li> <li> Well-tested feature extraction code for non-NLP data</li> <li> Support for direct file input</li> <li> Support for PostgreSQL</li> </ul>"},{"location":"#hello-world","title":"Hello world","text":"<pre><code>pip install great-ai\n</code></pre> demo.py<pre><code>from great_ai import GreatAI\n\n@GreatAI.create  #(1) \ndef greeter(name: str) -&gt; str:  #(2) \n    return f\"Hello {name}!\"\n</code></pre> <ol> <li> <p><code>@GreatAI.create</code> wraps your <code>greeter</code> function with a <code>GreatAI</code> instance. The function will behave very similarly but:</p> <ol> <li>its return value becomes a <code>Trace[str]</code>,</li> <li>it gets a <code>process_batch</code> method for supporting parallel execution,</li> <li>and it can be deployed using the <code>great-ai</code> command-line tool.</li> </ol> </li> <li> <p>Typing functions is recommended in general, however, not required for GreatAI to work.</p> </li> </ol> Note <p>In practice, <code>greeter</code> could be an inference function of some AI/ML application. But it could also just wrap a black-box solution of some SaaS. Either way, it is imperative to have continuous oversight of the services you provide and the data you process, especially in the context of AI/ML applications.</p> terminal<pre><code>great-ai demo.py\n</code></pre> <p>Navigate to localhost:6060 in your browser.</p> <p></p> <p>Success</p> <p>Your GreatAI service is ready for production use. Many of the SE4ML best practices are configured and implemented automatically. To have full control over your service and to understand what else you might need to do in your use case, continue reading this documentation.</p>"},{"location":"#why-is-this-great","title":"Why is this GREAT?","text":"<p>GreatAI fits between the prototype and deployment phases of your (or your organisation's) AI development lifecycle. This is highlighted in blue in the diagram. Here, several best practices can be automatically implemented, aiming to achieve the following attributes:</p> <ul> <li>General: use any Python library without restriction</li> <li>Robust: have error-handling and well-tested utilities out-of-the-box </li> <li>End-to-end: utilise end-to-end feedback as a built-in, first-class concept</li> <li>Automated: focus only on what actually requires your attention</li> <li>Trustworthy: deploy models that you and society can confidently trust</li> </ul>"},{"location":"#why-greatai","title":"Why GreatAI?","text":"<p>There are other existing solutions aiming to facilitate this phase. Amazon SageMaker and Seldon Core provide the most comprehensive suite of features. If you have the opportunity to use them, do that because they're great.</p> <p>However, research indicates that professionals rarely use them. This may be due to their inherent setup and operational complexity. GreatAI is designed to be as simple to use as possible. Its straightforward, high-level API and sensible default configuration make it extremely easy to start using. Despite its relative simplicity over Seldon Core, it still implements many of the SE4ML best practices, and thus, can meaningfully improve your deployment without requiring prohibitively great effort.</p> <p> Find it on PyPI</p> <p> Find it on DockerHub</p> <p> Check out the tutorial</p>"},{"location":"#production-use","title":"Production use","text":"<p>GreatAI has been battle-tested on the core platform services of ScoutinScience.</p> <p> </p>"},{"location":"explanation/","title":"Explanation","text":"<p>A lot more details and discussion about the problem context and approaches of GreatAI, along with its evaluation, can be found in my thesis.</p> <p>: Download</p>"},{"location":"examples/scibert/","title":"Summarising scientific publications from a tech-transfer perspective","text":"<p>This is a simplified example illustrating how <code>great-ai</code> is used in practice at ScoutinScience. The subpages show <code>great-ai</code> in action by going over the lifecycle of fine-tuning and deploying a BERT-based software service.</p> Propriety data <p>The purpose of this example is to show you different ways in which <code>great-ai</code> can assist you. The exact NLP task being solved is not central. Stemming from this and from the difficult nature of obtaining appropriate training data, the propriety dataset used for the experiments is not shared.</p>"},{"location":"examples/scibert/#objectives","title":"Objectives","text":"<ol> <li>You will see how the great_ai.utilities can integrate into your Data Science workflow.</li> <li>You will see how great_ai.large_file can be used to version and store your trained model.</li> <li>You will see how GreatAI should be used to prepare your model for a robust and responsible deployment.</li> <li>You will see multiple ways of customising your deployment.</li> </ol>"},{"location":"examples/scibert/#overview","title":"Overview","text":"<p>One of the core features of the ScoutinScience platform is summarising research papers from a tech-transfer perspective. In short, extractive summarisation is preferred using a binary classifier trained on clients' judgement of sentence interestingness. Thus, documents are sentences, and the expected output is a binary label showing whether a sentence is \"worthy\" of being in the tech-transfer summary. Explaining each decision is imperative since ScoutinScience embraces applying only explainable AI (XAI) methods wherever feasible.</p> <p>Success</p> <p>You are ready to start the tutorial. Feel free to return to the summary section once you're finished.</p> <p> Examine data</p> <p> Train model</p> <p> Deploy service</p>"},{"location":"examples/scibert/#summary","title":"Summary","text":""},{"location":"examples/scibert/#data-notebook","title":"Data notebook","text":"<p>We load and analyse the data by calculating inter-rater reliability and checking the feasibility of using an AI-based approach by testing the accuracy of a trivial baseline method.</p>"},{"location":"examples/scibert/#training-notebook","title":"Training notebook","text":"<p>We simply fine-tune SciBERT.</p> <p>After training and evaluating a model, it is exported using great_ai.save_model. For more info, check out the configuration how-to page.</p>"},{"location":"examples/scibert/#deployment-notebook","title":"Deployment notebook","text":"<p>We customise the GreatAI configuration, create custom caching for the model and implement an inference function that can be hardened by wrapping it in a GreatAI instance. We also extract the attention weights as a quasi-explanation.</p> <p>Finally, we test the model's inference function through the GreatAI dashboard. The only thing left is to deploy the hardened service properly.</p>"},{"location":"examples/scibert/#additional-files","title":"Additional files","text":"<p>There are some other files required for deploying the notebook. For example, the config file for S3 and MongoDB or a Dockerfile for building a custom image. These are gathered and shown on a separate page.</p>"},{"location":"examples/scibert/additional-files/","title":"Additional files in the repository","text":"<p>In order to give you a smooth experience while comprehending this example, all non-notebook files are presented on this page in one place. In reality, these files should be in your project's top-level directory.</p>"},{"location":"examples/scibert/additional-files/#configini","title":"config.ini","text":"config.ini<pre><code>ENVIRONMENT = DEVELOPMENT\nENVIRONMENT = ENV:ENVIRONMENT\n\nMONGO_CONNECTION_STRING=ENV:MONGO_CONNECTION_STRING\nMONGO_DATABASE=highlights\n\nAWS_REGION_NAME = eu-west-2\nAWS_ACCESS_KEY_ID = MY_DEFAULT_AWS_ACCESS_KEY_ID_FOR_DEVELOPMENT\nAWS_SECRET_ACCESS_KEY = MY_DEFAULT_AWS_SECRET_ACCESS_KEY_FOR_DEVELOPMENT\nLARGE_FILES_BUCKET_NAME = my-orgs-large-files\n\nAWS_REGION_NAME = ENV:AWS_REGION_NAME\nAWS_ACCESS_KEY_ID = ENV:AWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY = ENV:AWS_SECRET_ACCESS_KEY\nLARGE_FILES_BUCKET_NAME = ENV:LARGE_FILES_BUCKET_NAME\n</code></pre> <p>All necessary configuration which is read by great_ai.utilities.ConfigFile. This will resolve values starting with <code>ENV:</code> from your environment variables.</p>"},{"location":"examples/scibert/additional-files/#requirementstxt","title":"requirements.txt","text":"requirements.txt<pre><code>torch==1.12.0\ntransformers==4.20.1\nnumpy==1.23.0\n</code></pre> <p>Usually, it is recommended to pin (freeze) the library versions on which we depend. This file is referenced by the Dockerfile.</p>"},{"location":"examples/scibert/additional-files/#dockerfile","title":"Dockerfile","text":"Dockerfile<pre><code>FROM schmelczera/great-ai:v0.1.6\n\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir --requirement requirements.txt\n\nCOPY . ./\n\nRUN large-file --backend s3 --secrets s3.ini --cache scibert-highlights\n\nCMD [\"deploy.ipynb\"]\n</code></pre> <p>This is used by the CD pipeline to create the production deployment of the service.</p>"},{"location":"examples/scibert/additional-files/#dockerignore","title":".dockerignore","text":".dockerignore<pre><code>.cache\n.git\ndata\n.gitignore\n.env\n.vscode\n.dockerignore\nDockerfile\n.mypy_cache\n.gitignore\n**/__pycache__\n**/.DS_Store\nREADME.md\n</code></pre> <p>It is useful not to send, for example, the <code>.cache</code> folder used by LargeFile to the docker daemon; this will speed up your local build times substantially.</p> <p>.gitignore</p> <p>A very similar looking <code>.gitignore</code> file should also be present.</p>"},{"location":"examples/scibert/data/","title":"Explore data and feasibility of approach","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\nimport json\n\nannotations = []\nfor p in Path(\"data\").glob(\"*.json\"):\n    with open(p, encoding=\"utf-8\") as f:\n        print(p)\n        annotations.append(json.load(f))\n\nevaluations = {\n    sentence: [\n        annotation[sentence] for annotation in annotations if sentence in annotation\n    ]\n    for sentence in {\n        sentence for annotation in annotations for sentence in annotation.keys()\n    }\n}\n\nX = [s for s in evaluations.keys()]\ny = [int(sum(e) &gt; 0) for e in evaluations.values()]\n</pre> from pathlib import Path import json  annotations = [] for p in Path(\"data\").glob(\"*.json\"):     with open(p, encoding=\"utf-8\") as f:         print(p)         annotations.append(json.load(f))  evaluations = {     sentence: [         annotation[sentence] for annotation in annotations if sentence in annotation     ]     for sentence in {         sentence for annotation in annotations for sentence in annotation.keys()     } }  X = [s for s in evaluations.keys()] y = [int(sum(e) &gt; 0) for e in evaluations.values()] <pre>data/evaluation-experiment-2-stage #1-sa6a0y.json\ndata/evaluation-experiment-2-stage #1-2m6dmb.json\n</pre> <p>Save the compiled and processed data for later use using LargeFileS3.</p> In\u00a0[2]: Copied! <pre>from great_ai.large_file import LargeFileS3\nimport json\n\nLargeFileS3.configure_credentials_from_file(\"config.ini\")\n\nwith LargeFileS3(\"summary-train-dataset-small\", \"w\", encoding=\"utf-8\") as f:\n    json.dump((X, y), f)\n</pre> from great_ai.large_file import LargeFileS3 import json  LargeFileS3.configure_credentials_from_file(\"config.ini\")  with LargeFileS3(\"summary-train-dataset-small\", \"w\", encoding=\"utf-8\") as f:     json.dump((X, y), f) <pre>Copying file for summary-train-dataset-small-0\nCompressing summary-train-dataset-small-0\nUploading summary-train-dataset-small-0 to S3 as summary-train-dataset-small/0\nUploading summary-train-dataset-small-0.tar.gz 0.04/0.04 MB (100.0%)\n</pre> <p>Filter out sentences which don't have enough annotations.</p> In\u00a0[3]: Copied! <pre>y1 = [e[0] for e in evaluations.values() if len(e) == 2]\ny2 = [e[1] for e in evaluations.values() if len(e) == 2]\n</pre> y1 = [e[0] for e in evaluations.values() if len(e) == 2] y2 = [e[1] for e in evaluations.values() if len(e) == 2] <p>Calculate Cohen's kappa.</p> <p>It's a bit low but the task itself is pretty subjective so it's not all that surprising.</p> In\u00a0[4]: Copied! <pre>import sklearn.metrics\n\nsklearn.metrics.cohen_kappa_score(y1, y2)\n</pre> import sklearn.metrics  sklearn.metrics.cohen_kappa_score(y1, y2) Out[4]: <pre>0.3546448712421808</pre> <p>Can we train anything on this data?</p> <p>Let's try with a trivial SVM.</p> In\u00a0[5]: Copied! <pre>X = [s for s in evaluations.keys()]\ny = [int(sum(e) &gt; 0) for e in evaluations.values()]\n</pre> X = [s for s in evaluations.keys()] y = [int(sum(e) &gt; 0) for e in evaluations.values()] In\u00a0[6]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\n\n\nmodel = Pipeline(\n    steps=[\n        (\"vectorizer\", TfidfVectorizer(sublinear_tf=True, min_df=3, max_df=0.3)),\n        (\"classifier\", LinearSVC()),\n    ]\n)  # baseline model\n\ncross_val_score(model, X, y, cv=5)\n</pre> from sklearn.model_selection import train_test_split from sklearn.svm import LinearSVC from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import cross_val_score   model = Pipeline(     steps=[         (\"vectorizer\", TfidfVectorizer(sublinear_tf=True, min_df=3, max_df=0.3)),         (\"classifier\", LinearSVC()),     ] )  # baseline model  cross_val_score(model, X, y, cv=5) Out[6]: <pre>array([0.79, 0.75, 0.77, 0.69, 0.77])</pre> <p>The cross-validation shows promising accuracies. But accuracy isn't everything, therefore, we should investigate the accuracy metrics.</p> In\u00a0[7]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nmodel.fit(X_train, y_train)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) model.fit(X_train, y_train) Out[7]: <pre>Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(max_df=0.3, min_df=3, sublinear_tf=True)),\n                ('classifier', LinearSVC())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(max_df=0.3, min_df=3, sublinear_tf=True)),\n                ('classifier', LinearSVC())])</pre>TfidfVectorizer<pre>TfidfVectorizer(max_df=0.3, min_df=3, sublinear_tf=True)</pre>LinearSVC<pre>LinearSVC()</pre> In\u00a0[8]: Copied! <pre>y_predicted = model.predict(X_test)\nprint(\n    sklearn.metrics.classification_report(\n        [y &gt; 0 for y in y_test], [y &gt; 0 for y in y_predicted]\n    )\n)\nsklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n    [y &gt; 0 for y in y_test],\n    [y &gt; 0 for y in y_predicted],\n    xticks_rotation=\"vertical\",\n    values_format=\".2f\",\n)\nNone\n</pre> y_predicted = model.predict(X_test) print(     sklearn.metrics.classification_report(         [y &gt; 0 for y in y_test], [y &gt; 0 for y in y_predicted]     ) ) sklearn.metrics.ConfusionMatrixDisplay.from_predictions(     [y &gt; 0 for y in y_test],     [y &gt; 0 for y in y_predicted],     xticks_rotation=\"vertical\",     values_format=\".2f\", ) None <pre>              precision    recall  f1-score   support\n\n       False       0.78      0.78      0.78        51\n        True       0.78      0.78      0.78        49\n\n    accuracy                           0.78       100\n   macro avg       0.78      0.78      0.78       100\nweighted avg       0.78      0.78      0.78       100\n\n</pre> <p>We get an F1-score of 0.78 without any hyperparameter-optimisation; this task may be feasible to solve with AI.</p> <p>Next: Part 2</p>"},{"location":"examples/scibert/data/#explore-data-and-feasibility-of-approach","title":"Explore data and feasibility of approach\u00b6","text":"<p>We had asked our clients and in-house experts to annotate sentences using a rigorous guideline. The aim is to decide on which sentences they would like to see in a summary for a paper.</p> <p>The results are in JSON format, each annotator has a separate file. Let's load them.</p>"},{"location":"examples/scibert/deploy/","title":"Create an inference function","text":"In\u00a0[1]: Copied! <pre>from great_ai.utilities import ConfigFile\nfrom great_ai.large_file import LargeFileS3\nfrom great_ai import configure, MongoDbDriver\n\nconfiguration = ConfigFile(\"config.ini\")\n\nLargeFileS3.configure_credentials_from_file(configuration)\nMongoDbDriver.configure_credentials_from_file(configuration)\n\nconfigure(\n    dashboard_table_size=100,  # traces are small, we can show many\n    prediction_cache_size=4096,  # predictions are expensive, cache them\n)\n</pre> from great_ai.utilities import ConfigFile from great_ai.large_file import LargeFileS3 from great_ai import configure, MongoDbDriver  configuration = ConfigFile(\"config.ini\")  LargeFileS3.configure_credentials_from_file(configuration) MongoDbDriver.configure_credentials_from_file(configuration)  configure(     dashboard_table_size=100,  # traces are small, we can show many     prediction_cache_size=4096,  # predictions are expensive, cache them ) <pre>The value of `ENVIRONMENT` contains the \"ENV` prefix but `ENVIRONMENT` is not defined as an environment variable, using the default value defined above (`DEVELOPMENT`)\nEnvironment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\nMongoDbDriver has been already configured: skipping initialisation\nLargeFileS3 has been already configured: skipping initialisation\nGreatAI (v0.1.6): configured \u2705\n  \ud83d\udd29 tracing_database: MongoDbDriver\n  \ud83d\udd29 large_file_implementation: LargeFileS3\n  \ud83d\udd29 is_production: False\n  \ud83d\udd29 should_log_exception_stack: True\n  \ud83d\udd29 prediction_cache_size: 4096\n  \ud83d\udd29 dashboard_table_size: 100\nYou still need to check whether you follow all best practices before trusting your deployment.\n&gt; Find out more at https://se-ml.github.io/practices\n</pre> <p>For a pleasant developer experience, we create some typed models that will show up in the automatically generated OpenAPI schema specification and will also provide runtime type validation.</p> In\u00a0[2]: Copied! <pre>from typing import List\nfrom pydantic import BaseModel\n\n\nclass Attention(BaseModel):\n    weight: float\n    token: str\n\n\nclass EvaluatedSentence(BaseModel):\n    score: float\n    text: str\n    explanation: List[Attention]\n</pre> from typing import List from pydantic import BaseModel   class Attention(BaseModel):     weight: float     token: str   class EvaluatedSentence(BaseModel):     score: float     text: str     explanation: List[Attention] <p>Even though <code>@use_model</code> caches the remote files locally and it also handles deserialising objects, we only use it to store a directory. In this case, it gives back a path, the path to that directory. So, we need to load the files from that folder ourselves. In order to only load it once per process, we create a small model loader helper function.</p> <p>This is usually not needed, however, when we can outsmart <code>dill</code> so for optimisation purposes, we do it.</p> In\u00a0[3]: Copied! <pre>from great_ai import use_model\nfrom pathlib import Path\nfrom typing import Tuple\nfrom transformers import (\n    PreTrainedModel,\n    PreTrainedTokenizer,\n)\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n)\n\n_tokenizer: PreTrainedTokenizer = None\n_loaded_model: PreTrainedModel = None\n\n\n@use_model(\"scibert-highlights\", version=\"latest\", model_kwarg_name=\"model_path\")\ndef get_tokenizer_and_model(\n    model_path: Path, original_model: str = \"allenai/scibert_scivocab_uncased\"\n) -&gt; Tuple[PreTrainedTokenizer, PreTrainedModel]:\n    global _tokenizer, _loaded_model\n\n    if _tokenizer is None:\n        _tokenizer = AutoTokenizer.from_pretrained(original_model)\n\n    if _loaded_model is None:\n        config = AutoConfig.from_pretrained(\n            model_path, output_hidden_states=True, output_attentions=True\n        )\n        _loaded_model = AutoModelForSequenceClassification.from_pretrained(\n            model_path, config=config\n        )\n\n    return _tokenizer, _loaded_model\n</pre> from great_ai import use_model from pathlib import Path from typing import Tuple from transformers import (     PreTrainedModel,     PreTrainedTokenizer, ) from transformers import (     AutoConfig,     AutoModelForSequenceClassification,     AutoTokenizer, )  _tokenizer: PreTrainedTokenizer = None _loaded_model: PreTrainedModel = None   @use_model(\"scibert-highlights\", version=\"latest\", model_kwarg_name=\"model_path\") def get_tokenizer_and_model(     model_path: Path, original_model: str = \"allenai/scibert_scivocab_uncased\" ) -&gt; Tuple[PreTrainedTokenizer, PreTrainedModel]:     global _tokenizer, _loaded_model      if _tokenizer is None:         _tokenizer = AutoTokenizer.from_pretrained(original_model)      if _loaded_model is None:         config = AutoConfig.from_pretrained(             model_path, output_hidden_states=True, output_attentions=True         )         _loaded_model = AutoModelForSequenceClassification.from_pretrained(             model_path, config=config         )      return _tokenizer, _loaded_model <pre>Latest version of scibert-highlights is 0 (from versions: 0)\nFile scibert-highlights-0 found in cache\n</pre> <p>Finally, implement the inference function.</p> In\u00a0[4]: Copied! <pre>from great_ai import GreatAI\nfrom great_ai.utilities import clean\n\nimport re\nimport numpy as np\nimport torch\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n\n@GreatAI.create\ndef find_highlights(sentence: str) -&gt; EvaluatedSentence:\n    \"\"\"Get the interestingness prediction of the input sentence using SciBERT.\n\n    Run the SciBERT model in inference mode and evaluate the sentence.\n    Additionally, provide explanation in the form of the last layer's sum attention\n    between `[CLS]` and the other tokens.\n    \"\"\"\n\n    tokenizer, loaded_model = get_tokenizer_and_model()\n    sentence = clean(sentence, convert_to_ascii=True, remove_brackets=True)\n\n    tensors = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512)\n\n    with torch.inference_mode():\n        result: SequenceClassifierOutput = loaded_model(**tensors)\n        positive_likelihood = torch.nn.Softmax(dim=1)(result.logits)[0][1]\n    tokens = tensors[\"input_ids\"][0]\n\n    attentions = np.sum(result.attentions[-1].numpy()[0], axis=0)[0][1:-1]\n    # Tuple of `torch.FloatTensor` (one for each layer) of shape\n    # `(batch_size, num_heads, sequence_length, sequence_length)`.\n\n    explanation = []\n\n    token_attentions = list(zip(attentions, tokens[1:-1]))\n    for token in re.split(r\"([ .,])\", sentence):\n        token = token.strip()\n        if not token:\n            continue\n        bert_tokens = tokenizer(\n            token, return_tensors=\"pt\", truncation=True, max_length=512\n        )[\"input_ids\"][0][\n            1:-1\n        ]  # truncation=True needed to fix `RuntimeError: Already borrowed`\n        weight = 0\n        for t1 in bert_tokens:\n            if not token_attentions:\n                break\n            a, t2 = token_attentions.pop(0)\n            assert t1 == t2, sentence\n            weight += a\n        explanation.append(\n            Attention(\n                token=token if token in \".,\" else \" \" + token, weight=round(weight, 4)\n            )\n        )\n        if not token_attentions:\n            break\n\n    return EvaluatedSentence(\n        score=positive_likelihood, text=sentence, explanation=explanation\n    )\n</pre> from great_ai import GreatAI from great_ai.utilities import clean  import re import numpy as np import torch from transformers.modeling_outputs import SequenceClassifierOutput   @GreatAI.create def find_highlights(sentence: str) -&gt; EvaluatedSentence:     \"\"\"Get the interestingness prediction of the input sentence using SciBERT.      Run the SciBERT model in inference mode and evaluate the sentence.     Additionally, provide explanation in the form of the last layer's sum attention     between `[CLS]` and the other tokens.     \"\"\"      tokenizer, loaded_model = get_tokenizer_and_model()     sentence = clean(sentence, convert_to_ascii=True, remove_brackets=True)      tensors = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512)      with torch.inference_mode():         result: SequenceClassifierOutput = loaded_model(**tensors)         positive_likelihood = torch.nn.Softmax(dim=1)(result.logits)[0][1]     tokens = tensors[\"input_ids\"][0]      attentions = np.sum(result.attentions[-1].numpy()[0], axis=0)[0][1:-1]     # Tuple of `torch.FloatTensor` (one for each layer) of shape     # `(batch_size, num_heads, sequence_length, sequence_length)`.      explanation = []      token_attentions = list(zip(attentions, tokens[1:-1]))     for token in re.split(r\"([ .,])\", sentence):         token = token.strip()         if not token:             continue         bert_tokens = tokenizer(             token, return_tensors=\"pt\", truncation=True, max_length=512         )[\"input_ids\"][0][             1:-1         ]  # truncation=True needed to fix `RuntimeError: Already borrowed`         weight = 0         for t1 in bert_tokens:             if not token_attentions:                 break             a, t2 = token_attentions.pop(0)             assert t1 == t2, sentence             weight += a         explanation.append(             Attention(                 token=token if token in \".,\" else \" \" + token, weight=round(weight, 4)             )         )         if not token_attentions:             break      return EvaluatedSentence(         score=positive_likelihood, text=sentence, explanation=explanation     ) <p>A simple test to see everything works. Note that the models list is filled by the <code>@use_model</code> call even though it's not on the main inference function.</p> In\u00a0[5]: Copied! <pre>if __name__ == \"__main__\":\n    find_highlights(\n        \"Our solution has outperformed the state-of-the-art.\"\n    ), find_highlights(\"Their solution did not perform well.\")\n</pre> if __name__ == \"__main__\":     find_highlights(         \"Our solution has outperformed the state-of-the-art.\"     ), find_highlights(\"Their solution did not perform well.\") Out[5]: <pre>(Trace[EvaluatedSentence]({'created': '2022-07-16T18:47:29.581701',\n   'exception': None,\n   'feedback': None,\n   'logged_values': { 'arg:sentence:length': 51,\n                      'arg:sentence:value': 'Our solution has outperformed the '\n                                            'state-of-the-art.'},\n   'models': [{'key': 'scibert-highlights', 'version': 0}],\n   'original_execution_time_ms': 7127.2063,\n   'output': { 'explanation': [ {'token': ' Our', 'weight': 0.3993},\n                                {'token': ' solution', 'weight': 0.3481},\n                                {'token': ' has', 'weight': 0.2945},\n                                {'token': ' outperformed', 'weight': 0.4011},\n                                {'token': ' the', 'weight': 0.1484},\n                                {'token': ' state-of-the-art', 'weight': 0.5727},\n                                {'token': '.', 'weight': 7.775}],\n               'score': 0.9991180300712585,\n               'text': 'Our solution has outperformed the state-of-the-art.'},\n   'tags': ['find_highlights', 'online', 'development'],\n   'trace_id': '56e20e94-79df-4793-ae61-d20820ebe2d3'}),\n Trace[EvaluatedSentence]({'created': '2022-07-16T18:47:37.020275',\n   'exception': None,\n   'feedback': None,\n   'logged_values': { 'arg:sentence:length': 36,\n                      'arg:sentence:value': 'Their solution did not perform '\n                                            'well.'},\n   'models': [{'key': 'scibert-highlights', 'version': 0}],\n   'original_execution_time_ms': 170.7057,\n   'output': { 'explanation': [ {'token': ' Their', 'weight': 1.1475},\n                                {'token': ' solution', 'weight': 0.8205},\n                                {'token': ' did', 'weight': 0.3254},\n                                {'token': ' not', 'weight': 0.2921},\n                                {'token': ' perform', 'weight': 0.4293},\n                                {'token': ' well', 'weight': 0.2772},\n                                {'token': '.', 'weight': 4.4723}],\n               'score': 0.12305451184511185,\n               'text': 'Their solution did not perform well.'},\n   'tags': ['find_highlights', 'online', 'development'],\n   'trace_id': '7fcf8271-1738-4025-8305-d5a1e5100aea'}))</pre> <p>In this case, the service is built as a docker image, pushed to our image registry and subsequent rolling update is performed in the production cluster. To check out the Dockerimage, go to the additional files page.</p>"},{"location":"examples/scibert/deploy/#create-an-inference-function","title":"Create an inference function\u00b6","text":"<p>Everything is ready to wrap the previously trained model and deploy it.</p> <p>First, we need to configure the LargeFileBackend, the TracingDatabase and GreatAI.</p>"},{"location":"examples/scibert/train/","title":"Fine-tune SciBERT","text":"In\u00a0[1]: Copied! <pre>!pip install transformers datasets great-ai &gt; /dev/null\n</pre> !pip install transformers datasets great-ai &gt; /dev/null <p>Load the training data from S3. (We have uploaded this to S3 in the <code>data</code> notebook.)</p> In\u00a0[2]: Copied! <pre>from great_ai.large_file import LargeFileS3\nimport json\n\nLargeFileS3.configure_credentials_from_file(\"config.ini\")\n\nwith LargeFileS3(\"summary-train-dataset-small\", encoding=\"utf-8\") as f:\n    # splitting training and test data is done later by `datasets`\n    X, y = json.load(f)\n</pre> from great_ai.large_file import LargeFileS3 import json  LargeFileS3.configure_credentials_from_file(\"config.ini\")  with LargeFileS3(\"summary-train-dataset-small\", encoding=\"utf-8\") as f:     # splitting training and test data is done later by `datasets`     X, y = json.load(f) <pre>Latest version of summary-train-dataset-small is 0 (from versions: 0)\nFile summary-train-dataset-small-0 found in cache\n</pre> <p>Finetune SciBERT, for more info about this step, check out HuggingFace. If you're only here for <code>great-ai</code>, feel free to skip the next cell.</p> In\u00a0[22]: Copied! <pre>from transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n)\nfrom pathlib import Path\nimport numpy as np\nfrom datasets import Dataset, load_metric\n\nMODEL = \"allenai/scibert_scivocab_uncased\"\nBATCH_SIZE = 32\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\ndef tokenize_function(v):\n    return tokenizer(v[\"text\"])\n\n\ndataset = (\n    Dataset.from_dict({\"text\": X, \"label\": y})\n    .map(lambda v: tokenizer(v[\"text\"], truncation=True), batched=True)\n    .remove_columns(\"text\")\n    .train_test_split(test_size=0.2, shuffle=True)  # test is actually validation\n)\n\nf1_score = load_metric(\"f1\")\n\n\ndef compute_metrics(p):\n    pred, labels = p\n    pred = np.argmax(pred, axis=1)\n    return f1_score.compute(predictions=pred, references=labels)\n\n\ntraining_args = TrainingArguments(\n    output_dir=Path(\"models\"),\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    save_total_limit=5,\n    num_train_epochs=50,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    weight_decay=0.01,\n    metric_for_best_model=\"f1\",\n    load_best_model_at_end=True,\n)\n\nresult = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n).train()\n</pre> from transformers import (     AutoModelForSequenceClassification,     AutoTokenizer,     DataCollatorWithPadding,     Trainer,     TrainingArguments,     EarlyStoppingCallback, ) from pathlib import Path import numpy as np from datasets import Dataset, load_metric  MODEL = \"allenai/scibert_scivocab_uncased\" BATCH_SIZE = 32  tokenizer = AutoTokenizer.from_pretrained(MODEL) model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2) data_collator = DataCollatorWithPadding(tokenizer=tokenizer)   def tokenize_function(v):     return tokenizer(v[\"text\"])   dataset = (     Dataset.from_dict({\"text\": X, \"label\": y})     .map(lambda v: tokenizer(v[\"text\"], truncation=True), batched=True)     .remove_columns(\"text\")     .train_test_split(test_size=0.2, shuffle=True)  # test is actually validation )  f1_score = load_metric(\"f1\")   def compute_metrics(p):     pred, labels = p     pred = np.argmax(pred, axis=1)     return f1_score.compute(predictions=pred, references=labels)   training_args = TrainingArguments(     output_dir=Path(\"models\"),     per_device_train_batch_size=BATCH_SIZE,     per_device_eval_batch_size=BATCH_SIZE,     save_total_limit=5,     num_train_epochs=50,     save_strategy=\"epoch\",     evaluation_strategy=\"epoch\",     logging_strategy=\"epoch\",     weight_decay=0.01,     metric_for_best_model=\"f1\",     load_best_model_at_end=True, )  result = Trainer(     model=model,     args=training_args,     train_dataset=dataset[\"train\"],     eval_dataset=dataset[\"test\"],     data_collator=data_collator,     compute_metrics=compute_metrics,     callbacks=[EarlyStoppingCallback(early_stopping_patience=5)], ).train() <pre>  0%|          | 0/1 [00:00&lt;?, ?ba/s]</pre>        [130/650 01:43 &lt; 07:01, 1.23 it/s, Epoch 10/50]      Epoch Training Loss Validation Loss F1 1 0.586800 0.512138 0.719101 2 0.411600 0.416675 0.849057 3 0.245600 0.417070 0.864000 4 0.147800 0.575878 0.852459 5 0.056800 0.474259 0.896552 6 0.022500 0.754236 0.843137 7 0.001000 0.857636 0.834783 8 0.000500 0.920232 0.869565 9 0.000300 0.970790 0.877193 10 0.000300 0.948689 0.862385 <p> </p> <pre>...\nDeleting older checkpoint [models/checkpoint-39] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 32\nSaving model checkpoint to models/checkpoint-117\nConfiguration saved in models/checkpoint-117/config.json\nModel weights saved in models/checkpoint-117/pytorch_model.bin\nDeleting older checkpoint [models/checkpoint-52] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 32\nSaving model checkpoint to models/checkpoint-130\nConfiguration saved in models/checkpoint-130/config.json\nModel weights saved in models/checkpoint-130/pytorch_model.bin\nDeleting older checkpoint [models/checkpoint-78] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from models/checkpoint-65 (score: 0.896551724137931).\n</pre> <p>The best macro F1-score on the test set is 0.89 which is (not surprisingly) substantially more than the SVM achieved. We have a great model, it's time to deploy it. But first, we have to store it in a secure place.</p> In\u00a0[44]: Copied! <pre>from great_ai import save_model\n\n# save Torch model to local disk\nmodel.save_pretrained(\"pretrained\")\n\n# upload model from local disk to S3\n# (because the S3 credentials have been already set, `save_model` will use LargeFileS3)\nsave_model(\"pretrained\", key=\"scibert-highlights\")\n</pre> from great_ai import save_model  # save Torch model to local disk model.save_pretrained(\"pretrained\")  # upload model from local disk to S3 # (because the S3 credentials have been already set, `save_model` will use LargeFileS3) save_model(\"pretrained\", key=\"scibert-highlights\") <pre>Configuration saved in pretrained/config.json\nModel weights saved in pretrained/pytorch_model.bin\n</pre> <pre>  adding: pretrained/ (stored 0%)\n  adding: pretrained/config.json (deflated 49%)\n  adding: pretrained/pytorch_model.bin (deflated 7%)\n</pre> <p>Next: Part 3</p>"},{"location":"examples/scibert/train/#fine-tune-scibert","title":"Fine-tune SciBERT\u00b6","text":"<p>We are planning to do a simple classification task on scientific text. For that, SciBERT is an ideal model to fine-tune since it has been pretrained of academic publications.</p> <p>This notebook was updated so that it can run in Google Colab.</p> <p>First, we need to install the dependencies.</p>"},{"location":"examples/simple/data/","title":"Simple example: data engineering","text":"In\u00a0[1]: Copied! <pre>MAX_CHUNK_COUNT = 4\n</pre> MAX_CHUNK_COUNT = 4 In\u00a0[2]: Copied! <pre>import urllib.request\nfrom random import shuffle\n\nmanifest = (\n    urllib.request.urlopen(\n        \"https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/\"\n        \"open-corpus/2022-02-01/manifest.txt\"\n    )\n    .read()\n    .decode()\n)  # a list of available chunks separated by '\\n' characters\n\nlines = manifest.split()\nshuffle(lines)\nchunks = lines[:MAX_CHUNK_COUNT]\n\nf\"\"\"Processing {len(chunks)} out of the {\n    len(manifest.split())\n} available chunks\"\"\"\n</pre> import urllib.request from random import shuffle  manifest = (     urllib.request.urlopen(         \"https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/\"         \"open-corpus/2022-02-01/manifest.txt\"     )     .read()     .decode() )  # a list of available chunks separated by '\\n' characters  lines = manifest.split() shuffle(lines) chunks = lines[:MAX_CHUNK_COUNT]  f\"\"\"Processing {len(chunks)} out of the {     len(manifest.split()) } available chunks\"\"\" Out[2]: <pre>'Processing 4 out of the 6002 available chunks'</pre> In\u00a0[3]: Copied! <pre>from typing import List, Tuple\nimport json\nimport gzip\nfrom great_ai.utilities import (\n    simple_parallel_map,\n    clean,\n    is_english,\n    predict_language,\n    unchunk,\n)\n\n\ndef preprocess_chunk(chunk_key: str) -&gt; List[Tuple[str, List[str]]]:\n    response = urllib.request.urlopen(\n        f\"https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/\"\n        f\"open-corpus/2022-02-01/{chunk_key}\"\n    )  # a gzipped JSON Lines file\n\n    decompressed = gzip.decompress(response.read())\n    decoded = decompressed.decode()\n    chunk = [json.loads(line) for line in decoded.split(\"\\n\") if line]\n\n    # Transform\n    return [\n        (\n            clean(\n                f'{c[\"title\"]} {c[\"paperAbstract\"]} '\n                f'{c[\"journalName\"]} {c[\"venue\"]}',\n                convert_to_ascii=True,\n            ),  # The text is cleaned to remove common artifacts\n            c[\"fieldsOfStudy\"],\n        )  # Create pairs of `(text, [...domains])`\n        for c in chunk\n        if (c[\"fieldsOfStudy\"] and is_english(predict_language(c[\"paperAbstract\"])))\n    ]\n\n\npreprocessed_data = unchunk(\n    simple_parallel_map(preprocess_chunk, chunks, concurrency=4)\n)\n</pre> from typing import List, Tuple import json import gzip from great_ai.utilities import (     simple_parallel_map,     clean,     is_english,     predict_language,     unchunk, )   def preprocess_chunk(chunk_key: str) -&gt; List[Tuple[str, List[str]]]:     response = urllib.request.urlopen(         f\"https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/\"         f\"open-corpus/2022-02-01/{chunk_key}\"     )  # a gzipped JSON Lines file      decompressed = gzip.decompress(response.read())     decoded = decompressed.decode()     chunk = [json.loads(line) for line in decoded.split(\"\\n\") if line]      # Transform     return [         (             clean(                 f'{c[\"title\"]} {c[\"paperAbstract\"]} '                 f'{c[\"journalName\"]} {c[\"venue\"]}',                 convert_to_ascii=True,             ),  # The text is cleaned to remove common artifacts             c[\"fieldsOfStudy\"],         )  # Create pairs of `(text, [...domains])`         for c in chunk         if (c[\"fieldsOfStudy\"] and is_english(predict_language(c[\"paperAbstract\"])))     ]   preprocessed_data = unchunk(     simple_parallel_map(preprocess_chunk, chunks, concurrency=4) ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [04:22&lt;00:00, 65.51s/it] \n</pre> In\u00a0[4]: Copied! <pre>X, y = zip(*preprocessed_data)  # X is the input, y is the expected output\n</pre> X, y = zip(*preprocessed_data)  # X is the input, y is the expected output In\u00a0[5]: Copied! <pre>from great_ai import add_ground_truth\n\nadd_ground_truth(X, y, train_split_ratio=0.8, test_split_ratio=0.2)\n</pre> from great_ai import add_ground_truth  add_ground_truth(X, y, train_split_ratio=0.8, test_split_ratio=0.2) <pre>Environment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\nCannot find credentials files, defaulting to using ParallelTinyDbDriver\nThe selected tracing database (ParallelTinyDbDriver) is not recommended for production\nCannot find credentials files, defaulting to using LargeFileLocal\nGreatAI (v0.1.6): configured \u2705\n  \ud83d\udd29 tracing_database: ParallelTinyDbDriver\n  \ud83d\udd29 large_file_implementation: LargeFileLocal\n  \ud83d\udd29 is_production: False\n  \ud83d\udd29 should_log_exception_stack: True\n  \ud83d\udd29 prediction_cache_size: 512\n  \ud83d\udd29 dashboard_table_size: 50\nYou still need to check whether you follow all best practices before trusting your deployment.\n&gt; Find out more at https://se-ml.github.io/practices\n</pre>"},{"location":"examples/simple/data/#simple-example-data-engineering","title":"Simple example: data engineering\u00b6","text":"<p>Here, we solve a problem similar to the tutorial's but with an explainable Naive Bayes classifier and more best practices. In short, we train a domain classifier on the semantic scholar dataset by taking full advantage of <code>great-ai</code>. Subsequently, we create a production-ready deployment.</p> <p></p> <p>The blue boxes show the steps of a typical AI-development lifecycle implemented in this notebook.</p> <p>Since the true scope of <code>great-ai</code> is the phase between proof-of-concept code and production-ready service, it is predominantly used in the deployment notebook. Feel free to skip there, or continue reading if you'd like to see the full picture.</p>"},{"location":"examples/simple/data/#extract","title":"Extract\u00b6","text":"<p>This can be achieved by downloading a public dataset (such as in this case), or by having a Data Engineer setup and give us access to the organisation's data.</p> <p>In this example, we download the semantic scholar dataset from a public S3 bucket.</p>"},{"location":"examples/simple/data/#transform","title":"Transform\u00b6","text":"<ul> <li>Filter out non-English abstracts using <code>great_ai.utilities.predict_language</code></li> <li>Project it to only keep the necessary components (text and labels), clean the textual content using <code>great_ai.utilities.clean</code></li> <li>We will speed up processing using <code>great_ai.utilities.simple_parallel_map</code>.</li> </ul>"},{"location":"examples/simple/data/#load","title":"Load\u00b6","text":"<p>Upload the dataset (or a part of it) to a central repository using <code>great_ai.add_ground_truth</code>. This step automatically tags each data-point with a split label according to the ratios we set. Additional tags can be also given.</p>"},{"location":"examples/simple/data/#production-ready-backend","title":"Production-ready backend\u00b6","text":"<p>The MongoDB driver is automatically configured if  <code>mongo.ini</code> exists with the following scheme:</p> <pre>mongo_connection_string=mongodb://localhost:27017/\nmongo_database=my_great_ai_db\n</pre> <p>You can install MongoDB from here or use it as a service</p> <p>Otherwise, TinyDB is used which is just a local JSON file.</p>"},{"location":"examples/simple/data/#next-part-2","title":"Next: Part 2\u00b6","text":""},{"location":"examples/simple/deploy/","title":"Hardening and deployment","text":"In\u00a0[1]: Copied! <pre>import re\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom great_ai.utilities import clean\nfrom great_ai import (\n    MultiLabelClassificationOutput,\n    ClassificationOutput,\n    GreatAI,\n    use_model,\n    parameter,\n)\n\n\n@GreatAI.create\n@use_model(\"small-domain-prediction\", version=\"latest\")\n@parameter(\"target_confidence\", validate=lambda c: 0 &lt;= c &lt;= 100)\ndef predict_domain(\n    text: str, model: Pipeline, target_confidence: int = 50\n) -&gt; MultiLabelClassificationOutput:\n    \"\"\"\n    Predict the scientific domain of the input text.\n    Return labels until their sum likelihood is larger than `target_confidence`.\n    \"\"\"\n\n    preprocessed = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean(text, convert_to_ascii=True))\n    features = model.named_steps[\"vectorizer\"].transform([preprocessed])\n    prediction = model.named_steps[\"classifier\"].predict_proba(features)[0]\n\n    best_classes = sorted(enumerate(prediction), key=lambda v: v[1], reverse=True)\n\n    results = MultiLabelClassificationOutput()\n    for class_index, probability in best_classes:\n        results.labels.append(\n            get_label(\n                model=model,\n                features=features,\n                class_index=class_index,\n                probability=probability,\n            )\n        )\n\n        if sum(r.confidence for r in results.labels) &gt;= target_confidence:\n            break\n\n    return results\n\n\ndef get_label(\n    model: Pipeline, features: np.ndarray, class_index: int, probability: float\n) -&gt; ClassificationOutput:\n    return ClassificationOutput(\n        label=model.named_steps[\"classifier\"].classes_[class_index],\n        confidence=round(probability * 100),\n        explanation=[\n            word\n            for _, word in sorted(\n                (\n                    (weight, word)\n                    for weight, word, count in zip(\n                        model.named_steps[\"classifier\"].feature_log_prob_[class_index],\n                        model.named_steps[\"vectorizer\"].get_feature_names_out(),\n                        features.A[0],\n                    )\n                    if count &gt; 0\n                ),\n                reverse=True,\n            )\n        ][:5],\n    )\n</pre> import re import numpy as np from sklearn.pipeline import Pipeline from great_ai.utilities import clean from great_ai import (     MultiLabelClassificationOutput,     ClassificationOutput,     GreatAI,     use_model,     parameter, )   @GreatAI.create @use_model(\"small-domain-prediction\", version=\"latest\") @parameter(\"target_confidence\", validate=lambda c: 0 &lt;= c &lt;= 100) def predict_domain(     text: str, model: Pipeline, target_confidence: int = 50 ) -&gt; MultiLabelClassificationOutput:     \"\"\"     Predict the scientific domain of the input text.     Return labels until their sum likelihood is larger than `target_confidence`.     \"\"\"      preprocessed = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean(text, convert_to_ascii=True))     features = model.named_steps[\"vectorizer\"].transform([preprocessed])     prediction = model.named_steps[\"classifier\"].predict_proba(features)[0]      best_classes = sorted(enumerate(prediction), key=lambda v: v[1], reverse=True)      results = MultiLabelClassificationOutput()     for class_index, probability in best_classes:         results.labels.append(             get_label(                 model=model,                 features=features,                 class_index=class_index,                 probability=probability,             )         )          if sum(r.confidence for r in results.labels) &gt;= target_confidence:             break      return results   def get_label(     model: Pipeline, features: np.ndarray, class_index: int, probability: float ) -&gt; ClassificationOutput:     return ClassificationOutput(         label=model.named_steps[\"classifier\"].classes_[class_index],         confidence=round(probability * 100),         explanation=[             word             for _, word in sorted(                 (                     (weight, word)                     for weight, word, count in zip(                         model.named_steps[\"classifier\"].feature_log_prob_[class_index],                         model.named_steps[\"vectorizer\"].get_feature_names_out(),                         features.A[0],                     )                     if count &gt; 0                 ),                 reverse=True,             )         ][:5],     ) <pre>Environment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\nCannot find credentials files, defaulting to using ParallelTinyDbDriver\nThe selected tracing database (ParallelTinyDbDriver) is not recommended for production\nCannot find credentials files, defaulting to using LargeFileLocal\nGreatAI (v0.1.6): configured \u2705\n  \ud83d\udd29 tracing_database: ParallelTinyDbDriver\n  \ud83d\udd29 large_file_implementation: LargeFileLocal\n  \ud83d\udd29 is_production: False\n  \ud83d\udd29 should_log_exception_stack: True\n  \ud83d\udd29 prediction_cache_size: 512\n  \ud83d\udd29 dashboard_table_size: 50\nYou still need to check whether you follow all best practices before trusting your deployment.\n&gt; Find out more at https://se-ml.github.io/practices\nFetching cached versions of small-domain-prediction\nLatest version of small-domain-prediction is 2 (from versions: 0, 1, 2)\nFile small-domain-prediction-2 found in cache\n</pre> In\u00a0[2]: Copied! <pre>if __name__ == \"__main__\":\n    from great_ai import query_ground_truth\n    from sklearn import metrics\n\n    data = query_ground_truth(\"test\")\n\n    X = [d.input for d in data]\n    y_actual = [d.feedback for d in data]\n\n    y_predicted = [\n        d.output.labels[0].label\n        for d in predict_domain.process_batch(X, do_not_persist_traces=True)\n    ]\n    y_actual_aligned = [p if p in a else a[0] for p, a in zip(y_predicted, y_actual)]\n\n    import matplotlib.pyplot as plt\n\n    # Configure matplotlib to have nice, high-resolution charts\n    %matplotlib inline\n    plt.rcParams[\"figure.figsize\"] = (19, 18)\n    plt.rcParams[\"figure.facecolor\"] = \"white\"\n    plt.rcParams[\"font.size\"] = 16\n    plt.rcParams[\"axes.xmargin\"] = 0\n\n    print(metrics.classification_report(y_actual_aligned, y_predicted))\n    metrics.ConfusionMatrixDisplay.from_predictions(\n        y_true=y_actual_aligned,\n        y_pred=y_predicted,\n        xticks_rotation=\"vertical\",\n        normalize=\"pred\",\n        values_format=\".2f\",\n    )\n\n    plt.tight_layout()\n    plt.savefig(\"ss-confusion.png\", dpi=600)\n</pre> if __name__ == \"__main__\":     from great_ai import query_ground_truth     from sklearn import metrics      data = query_ground_truth(\"test\")      X = [d.input for d in data]     y_actual = [d.feedback for d in data]      y_predicted = [         d.output.labels[0].label         for d in predict_domain.process_batch(X, do_not_persist_traces=True)     ]     y_actual_aligned = [p if p in a else a[0] for p, a in zip(y_predicted, y_actual)]      import matplotlib.pyplot as plt      # Configure matplotlib to have nice, high-resolution charts     %matplotlib inline     plt.rcParams[\"figure.figsize\"] = (19, 18)     plt.rcParams[\"figure.facecolor\"] = \"white\"     plt.rcParams[\"font.size\"] = 16     plt.rcParams[\"axes.xmargin\"] = 0      print(metrics.classification_report(y_actual_aligned, y_predicted))     metrics.ConfusionMatrixDisplay.from_predictions(         y_true=y_actual_aligned,         y_pred=y_predicted,         xticks_rotation=\"vertical\",         normalize=\"pred\",         values_format=\".2f\",     )      plt.tight_layout()     plt.savefig(\"ss-confusion.png\", dpi=600) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12272/12272 [01:10&lt;00:00, 174.77it/s]\n</pre> <pre>                       precision    recall  f1-score   support\n\n                  Art       0.54      0.38      0.45       126\n              Biology       0.77      0.84      0.80      1215\n             Business       0.47      0.73      0.57       311\n            Chemistry       0.82      0.67      0.74      1205\n     Computer Science       0.77      0.76      0.76      1277\n            Economics       0.69      0.55      0.61       270\n          Engineering       0.55      0.52      0.53       754\nEnvironmental Science       0.56      0.55      0.55       227\n            Geography       0.54      0.39      0.45       276\n              Geology       0.74      0.67      0.70       265\n              History       0.35      0.18      0.24       140\n    Materials Science       0.72      0.81      0.76      1011\n          Mathematics       0.77      0.70      0.74       498\n             Medicine       0.96      0.77      0.86      2835\n           Philosophy       0.57      0.06      0.10        71\n              Physics       0.66      0.75      0.70       611\n    Political Science       0.44      0.61      0.51       291\n           Psychology       0.52      0.84      0.64       574\n            Sociology       0.33      0.59      0.42       315\n\n             accuracy                           0.71     12272\n            macro avg       0.62      0.60      0.59     12272\n         weighted avg       0.74      0.71      0.71     12272\n\n</pre>"},{"location":"examples/simple/deploy/#hardening-and-deployment","title":"Hardening and deployment\u00b6","text":"<p>Part 3: Create production inference function</p> <p></p> <p>The blue boxes show the steps implemented in this notebook.</p> <p>In Part 2, we trained our AI model. Now, it's time to create General Robust End-to-end Automated Trustworthy deployment.</p>"},{"location":"examples/simple/deploy/#create-the-inference-function","title":"Create the inference function\u00b6","text":"<p>Next to the prediction, we also return the top-n most influential words based on their weights.</p>"},{"location":"examples/simple/deploy/#check-accuracy-on-the-test-split","title":"Check accuracy on the test split\u00b6","text":"<p>Anything under <code>if __name__ == \"__main__\":</code> will not be run when the script is executed by the <code>great-ai</code> CLI app. This, combined with <code>query_ground_truth</code> and the <code>/traces/{trace_id}/feedback</code> endpoint are ideal for creating a continuous-integration job for checking the quality of the model before deployment.</p>"},{"location":"examples/simple/stacked-bars/","title":"Stacked bars","text":"In\u00a0[1]: Copied! <pre>from great_ai import query_ground_truth\n\ndata = query_ground_truth(\"train\")\n</pre> from great_ai import query_ground_truth  data = query_ground_truth(\"train\") <pre>Environment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\nCannot find credentials files, defaulting to using ParallelTinyDbDriver\nThe selected tracing database (ParallelTinyDbDriver) is not recommended for production\nCannot find credentials files, defaulting to using LargeFileLocal\nGreatAI (v0.1.6): configured \u2705\n  \ud83d\udd29 tracing_database: ParallelTinyDbDriver\n  \ud83d\udd29 large_file_implementation: LargeFileLocal\n  \ud83d\udd29 is_production: False\n  \ud83d\udd29 should_log_exception_stack: True\n  \ud83d\udd29 prediction_cache_size: 512\n  \ud83d\udd29 dashboard_table_size: 50\nYou still need to check whether you follow all best practices before trusting your deployment.\n&gt; Find out more at https://se-ml.github.io/practices\n</pre> In\u00a0[9]: Copied! <pre>from collections import Counter\nimport matplotlib.pyplot as plt\nfrom great_ai.utilities import unique\nimport numpy as np\nimport matplotlib.ticker as mtick\n\n\nfirst = Counter(d.feedback[0] for d in data if len(d.feedback) &gt; 0)\nsecond = Counter(d.feedback[1] for d in data if len(d.feedback) &gt; 1)\nthird = Counter(d.feedback[2] for d in data if len(d.feedback) &gt; 2)\n\ndomains = sorted(unique(domain for d in data for domain in d.feedback))\n\nfirst = [first[d] / len(data) for d in domains]\nsecond = [second[d] / len(data) for d in domains]\nthird = [third[d] / len(data) for d in domains]\n\n# Configure matplotlib to have nice, high-resolution charts\n%matplotlib inline\nplt.rcParams[\"figure.facecolor\"] = \"white\"\nplt.rcParams[\"font.size\"] = 20\nplt.rcParams[\"figure.figsize\"] = (14, 10)\n\nfig, ax = plt.subplots()\n\nplt.xticks(rotation=90)\n\nax.bar(domains, first, label=\"Primary domain\")\nax.bar(domains, second, label=\"Secondary domain\", bottom=first)\nax.bar(domains, third, label=\"Tertiary domain\", bottom=np.add(first, second))\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nax.legend()\nax.set_ylabel(\"Ratio of all publications\")\nfig.tight_layout()\nfig.savefig(\"ss-distribution.png\", dpi=500)\nNone\n</pre> from collections import Counter import matplotlib.pyplot as plt from great_ai.utilities import unique import numpy as np import matplotlib.ticker as mtick   first = Counter(d.feedback[0] for d in data if len(d.feedback) &gt; 0) second = Counter(d.feedback[1] for d in data if len(d.feedback) &gt; 1) third = Counter(d.feedback[2] for d in data if len(d.feedback) &gt; 2)  domains = sorted(unique(domain for d in data for domain in d.feedback))  first = [first[d] / len(data) for d in domains] second = [second[d] / len(data) for d in domains] third = [third[d] / len(data) for d in domains]  # Configure matplotlib to have nice, high-resolution charts %matplotlib inline plt.rcParams[\"figure.facecolor\"] = \"white\" plt.rcParams[\"font.size\"] = 20 plt.rcParams[\"figure.figsize\"] = (14, 10)  fig, ax = plt.subplots()  plt.xticks(rotation=90)  ax.bar(domains, first, label=\"Primary domain\") ax.bar(domains, second, label=\"Secondary domain\", bottom=first) ax.bar(domains, third, label=\"Tertiary domain\", bottom=np.add(first, second)) ax.yaxis.set_major_formatter(mtick.PercentFormatter(1)) ax.legend() ax.set_ylabel(\"Ratio of all publications\") fig.tight_layout() fig.savefig(\"ss-distribution.png\", dpi=500) None"},{"location":"examples/simple/train/","title":"Optimise and train a model","text":"In\u00a0[1]: Copied! <pre>from great_ai import query_ground_truth\n\ndata = query_ground_truth(\"train\")\nX = [d.input for d in data for domain in d.feedback]\ny = [domain for d in data for domain in d.feedback]\n</pre> from great_ai import query_ground_truth  data = query_ground_truth(\"train\") X = [d.input for d in data for domain in d.feedback] y = [domain for d in data for domain in d.feedback] <pre>Environment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\nCannot find credentials files, defaulting to using ParallelTinyDbDriver\nThe selected tracing database (ParallelTinyDbDriver) is not recommended for production\nCannot find credentials files, defaulting to using LargeFileLocal\nGreatAI (v0.1.6): configured \u2705\n  \ud83d\udd29 tracing_database: ParallelTinyDbDriver\n  \ud83d\udd29 large_file_implementation: LargeFileLocal\n  \ud83d\udd29 is_production: False\n  \ud83d\udd29 should_log_exception_stack: True\n  \ud83d\udd29 prediction_cache_size: 512\n  \ud83d\udd29 dashboard_table_size: 50\nYou still need to check whether you follow all best practices before trusting your deployment.\n&gt; Find out more at https://se-ml.github.io/practices\n</pre> In\u00a0[2]: Copied! <pre>from collections import Counter\nimport matplotlib.pyplot as plt\n\ndomains, counts = zip(*Counter(y).most_common())\n\n# Configure matplotlib to have nice, high-resolution charts\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (20, 5)\nplt.rcParams[\"figure.facecolor\"] = \"white\"\nplt.rcParams[\"font.size\"] = 12\n\nplt.xticks(rotation=90)\nplt.bar(domains, counts)\nNone\n</pre> from collections import Counter import matplotlib.pyplot as plt  domains, counts = zip(*Counter(y).most_common())  # Configure matplotlib to have nice, high-resolution charts %matplotlib inline plt.rcParams[\"figure.figsize\"] = (20, 5) plt.rcParams[\"figure.facecolor\"] = \"white\" plt.rcParams[\"font.size\"] = 12  plt.xticks(rotation=90) plt.bar(domains, counts) None In\u00a0[3]: Copied! <pre>from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef create_pipeline() -&gt; Pipeline:\n    return Pipeline(\n        steps=[\n            (\"vectorizer\", TfidfVectorizer(sublinear_tf=True)),\n            (\"classifier\", MultinomialNB()),\n        ]\n    )\n</pre> from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer   def create_pipeline() -&gt; Pipeline:     return Pipeline(         steps=[             (\"vectorizer\", TfidfVectorizer(sublinear_tf=True)),             (\"classifier\", MultinomialNB()),         ]     ) In\u00a0[4]: Copied! <pre>from sklearn.model_selection import GridSearchCV\nimport pandas as pd\n\noptimisation_pipeline = GridSearchCV(\n    create_pipeline(),\n    {\n        \"vectorizer__min_df\": [5, 20, 100],\n        \"vectorizer__max_df\": [0.05, 0.1],\n        \"classifier__alpha\": [0.5, 1],\n        \"classifier__fit_prior\": [True, False],\n    },\n    scoring=\"f1_macro\",\n    cv=3,\n    n_jobs=-1,\n    verbose=1,\n)\noptimisation_pipeline.fit(X, y)\n\nresults = pd.DataFrame(optimisation_pipeline.cv_results_)\nresults.sort_values(\"rank_test_score\")\n</pre> from sklearn.model_selection import GridSearchCV import pandas as pd  optimisation_pipeline = GridSearchCV(     create_pipeline(),     {         \"vectorizer__min_df\": [5, 20, 100],         \"vectorizer__max_df\": [0.05, 0.1],         \"classifier__alpha\": [0.5, 1],         \"classifier__fit_prior\": [True, False],     },     scoring=\"f1_macro\",     cv=3,     n_jobs=-1,     verbose=1, ) optimisation_pipeline.fit(X, y)  results = pd.DataFrame(optimisation_pipeline.cv_results_) results.sort_values(\"rank_test_score\") <pre>Fitting 3 folds for each of 24 candidates, totalling 72 fits\n</pre> Out[4]: mean_fit_time std_fit_time mean_score_time std_score_time param_classifier__alpha param_classifier__fit_prior param_vectorizer__max_df param_vectorizer__min_df params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score 7 14.549476 0.361685 8.476837 0.222398 0.5 False 0.05 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.518061 0.514842 0.511599 0.514834 0.002638 1 10 11.235289 0.130426 4.092868 0.082518 0.5 False 0.1 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.513897 0.515661 0.507867 0.512475 0.003337 2 19 7.383645 0.138110 4.130709 0.250048 1 False 0.05 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.496825 0.501045 0.496854 0.498241 0.001983 3 11 10.435154 0.305144 3.882101 0.128886 0.5 False 0.1 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.493247 0.497814 0.502245 0.497769 0.003674 4 8 13.643193 0.310696 4.173707 0.142980 0.5 False 0.05 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.489609 0.495207 0.498154 0.494323 0.003544 5 22 7.048340 0.050070 3.172948 0.152418 1 False 0.1 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.487456 0.493865 0.491157 0.490826 0.002627 6 23 7.564685 0.146092 2.374111 0.285026 1 False 0.1 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.485160 0.494039 0.490127 0.489776 0.003633 7 20 7.172353 0.212599 3.747219 0.130217 1 False 0.05 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.481303 0.490002 0.488269 0.486524 0.003759 8 6 14.276345 0.456576 8.318859 0.268701 0.5 False 0.05 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.482429 0.487735 0.484888 0.485017 0.002168 9 2 14.902358 0.737693 5.975091 0.171150 0.5 True 0.05 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.469598 0.474490 0.473637 0.472575 0.002134 10 9 12.677349 0.145143 4.374204 0.175674 0.5 False 0.1 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.468872 0.476451 0.470921 0.472082 0.003201 11 5 13.423686 0.482872 8.008324 0.442975 0.5 True 0.1 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.465726 0.474548 0.471879 0.470718 0.003694 12 1 13.819117 0.838347 6.161175 0.336590 0.5 True 0.05 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.463395 0.473982 0.471262 0.469546 0.004489 13 4 13.281476 0.588822 8.335852 0.254627 0.5 True 0.1 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.458734 0.468053 0.464418 0.463735 0.003835 14 14 7.282247 0.444940 3.567094 0.044519 1 True 0.05 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.438189 0.450160 0.446180 0.444843 0.004978 15 17 7.098797 0.196241 3.838628 0.091128 1 True 0.1 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.436488 0.444503 0.445900 0.442297 0.004147 16 18 7.492791 0.288889 3.843224 0.073438 1 False 0.05 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.428196 0.431945 0.430160 0.430100 0.001531 17 21 7.229826 0.099823 3.656332 0.073780 1 False 0.1 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.403130 0.410170 0.409801 0.407700 0.003235 18 13 7.158370 0.169818 3.765632 0.082924 1 True 0.05 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.399237 0.412872 0.407982 0.406697 0.005640 19 16 7.064643 0.119529 3.810983 0.125897 1 True 0.1 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.388060 0.399247 0.396325 0.394544 0.004737 20 0 13.749660 0.465174 6.407841 0.549166 0.5 True 0.05 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.384852 0.386487 0.386796 0.386045 0.000853 21 3 15.954147 0.318013 6.337361 0.261697 0.5 True 0.1 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.369785 0.375645 0.375858 0.373763 0.002814 22 12 7.120198 0.050452 3.833905 0.069540 1 True 0.05 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.277741 0.280564 0.285337 0.281214 0.003135 23 15 7.497707 0.183054 3.870714 0.062888 1 True 0.1 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.255578 0.263381 0.266184 0.261714 0.004487 24 In\u00a0[5]: Copied! <pre>from sklearn import set_config\n\nset_config(display=\"diagram\")\n\nclassifier = create_pipeline()\nclassifier.set_params(**optimisation_pipeline.best_params_)\nclassifier.fit(X, y)\n</pre> from sklearn import set_config  set_config(display=\"diagram\")  classifier = create_pipeline() classifier.set_params(**optimisation_pipeline.best_params_) classifier.fit(X, y) Out[5]: <pre>Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(max_df=0.05, min_df=20, sublinear_tf=True)),\n                ('classifier', MultinomialNB(alpha=0.5, fit_prior=False))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(max_df=0.05, min_df=20, sublinear_tf=True)),\n                ('classifier', MultinomialNB(alpha=0.5, fit_prior=False))])</pre>TfidfVectorizer<pre>TfidfVectorizer(max_df=0.05, min_df=20, sublinear_tf=True)</pre>MultinomialNB<pre>MultinomialNB(alpha=0.5, fit_prior=False)</pre> In\u00a0[6]: Copied! <pre>from great_ai import save_model\n\n\nsave_model(classifier, key=\"small-domain-prediction\", keep_last_n=5)\n</pre> from great_ai import save_model   save_model(classifier, key=\"small-domain-prediction\", keep_last_n=5) <pre>Fetching cached versions of small-domain-prediction\nCopying file for small-domain-prediction-2\nCompressing small-domain-prediction-2\nModel small-domain-prediction uploaded with version 2\n</pre> Out[6]: <pre>'small-domain-prediction:2'</pre>"},{"location":"examples/simple/train/#optimise-and-train-a-model","title":"Optimise and train a model\u00b6","text":"<p>The blue boxes show the steps implemented in this notebook.</p> <p>In the first part, we have cleaned and transformed our training data. We can now access this data using <code>great_ai.LargeFile</code>. Locally, it will gives us the cached version, otherwise, the latest version is downloaded from S3 or GridFS.</p> <p>In this part, we hyperparameter-optimise and train a simple Naive Bayes classifier which we then export for deployment using <code>great_ai.save_model</code>.</p>"},{"location":"examples/simple/train/#load-data-that-has-been-extracted-in-part-1","title":"Load data that has been extracted in part 1\u00b6","text":""},{"location":"examples/simple/train/#optimise-and-train-multinomial-naive-bayes-classifier","title":"Optimise and train Multinomial Naive Bayes classifier\u00b6","text":""},{"location":"examples/simple/train/#export-the-model-using-greatai","title":"Export the model using GreatAI\u00b6","text":""},{"location":"examples/simple/train/#next-part-3","title":"Next: Part 3\u00b6","text":""},{"location":"examples/simple-mag/train/","title":"Train a field of study (domain) classification of sentences on the MAG dataset","text":"In\u00a0[1]: Copied! <pre>import json\nfrom typing import Tuple\nfrom great_ai.utilities import clean, simple_parallel_map\nfrom tqdm.cli import tqdm\n\n\ndef preprocess(line: str) -&gt; Tuple[str, str]:\n    data_point = json.loads(line)\n\n    return (clean(data_point[\"text\"], convert_to_ascii=True), data_point[\"label\"])\n\n\nwith open(\"../../tutorial/data/train.txt\", encoding=\"utf-8\") as f:\n    training_data = simple_parallel_map(preprocess, f.readlines())\n\nX_train = [d[0] for d in training_data]\ny_train = [d[1] for d in training_data]\n\n\nwith open(\"../../tutorial/data/test.txt\", encoding=\"utf-8\") as f:\n    test_data = simple_parallel_map(preprocess, f.readlines())\n\nX_test = [d[0] for d in test_data]\ny_test = [d[1] for d in test_data]\n</pre> import json from typing import Tuple from great_ai.utilities import clean, simple_parallel_map from tqdm.cli import tqdm   def preprocess(line: str) -&gt; Tuple[str, str]:     data_point = json.loads(line)      return (clean(data_point[\"text\"], convert_to_ascii=True), data_point[\"label\"])   with open(\"../../tutorial/data/train.txt\", encoding=\"utf-8\") as f:     training_data = simple_parallel_map(preprocess, f.readlines())  X_train = [d[0] for d in training_data] y_train = [d[1] for d in training_data]   with open(\"../../tutorial/data/test.txt\", encoding=\"utf-8\") as f:     test_data = simple_parallel_map(preprocess, f.readlines())  X_test = [d[0] for d in test_data] y_test = [d[1] for d in test_data] <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 84000/84000 [00:16&lt;00:00, 4964.08it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22399/22399 [00:05&lt;00:00, 3847.04it/s]\n</pre> In\u00a0[2]: Copied! <pre>from collections import Counter\nimport matplotlib.pyplot as plt\n\ndomains, counts = zip(*Counter(y_train).most_common())\n\n# Configure matplotlib to have nice, high-resolution charts\n%matplotlib inline\nplt.rcParams[\"figure.facecolor\"] = \"white\"\nplt.rcParams[\"font.size\"] = 18\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nfig, ax = plt.subplots()\n\nplt.xticks(rotation=90)\nax.bar(domains, counts)\nax.set_ylabel(\"Count\")\nfig.tight_layout()\nfig.savefig(\"mag-distribution.png\", dpi=500)\nNone\n</pre> from collections import Counter import matplotlib.pyplot as plt  domains, counts = zip(*Counter(y_train).most_common())  # Configure matplotlib to have nice, high-resolution charts %matplotlib inline plt.rcParams[\"figure.facecolor\"] = \"white\" plt.rcParams[\"font.size\"] = 18 plt.rcParams[\"figure.figsize\"] = (8, 5)  fig, ax = plt.subplots()  plt.xticks(rotation=90) ax.bar(domains, counts) ax.set_ylabel(\"Count\") fig.tight_layout() fig.savefig(\"mag-distribution.png\", dpi=500) None In\u00a0[3]: Copied! <pre>from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef create_pipeline() -&gt; Pipeline:\n    return Pipeline(\n        steps=[\n            (\"vectorizer\", TfidfVectorizer(sublinear_tf=True)),\n            (\"classifier\", MultinomialNB()),\n        ]\n    )\n</pre> from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer   def create_pipeline() -&gt; Pipeline:     return Pipeline(         steps=[             (\"vectorizer\", TfidfVectorizer(sublinear_tf=True)),             (\"classifier\", MultinomialNB()),         ]     ) In\u00a0[4]: Copied! <pre>from sklearn.model_selection import GridSearchCV\nimport pandas as pd\n\noptimisation_pipeline = GridSearchCV(\n    create_pipeline(),\n    {\n        \"vectorizer__min_df\": [5, 20, 100],\n        \"vectorizer__max_df\": [0.05, 0.1],\n        \"classifier__alpha\": [0.5, 1],\n        \"classifier__fit_prior\": [True, False],\n    },\n    scoring=\"f1_macro\",\n    cv=3,\n    n_jobs=-1,\n    verbose=1,\n)\noptimisation_pipeline.fit(X_train, y_train)\n\nresults = pd.DataFrame(optimisation_pipeline.cv_results_)\nresults.sort_values(\"rank_test_score\")\n</pre> from sklearn.model_selection import GridSearchCV import pandas as pd  optimisation_pipeline = GridSearchCV(     create_pipeline(),     {         \"vectorizer__min_df\": [5, 20, 100],         \"vectorizer__max_df\": [0.05, 0.1],         \"classifier__alpha\": [0.5, 1],         \"classifier__fit_prior\": [True, False],     },     scoring=\"f1_macro\",     cv=3,     n_jobs=-1,     verbose=1, ) optimisation_pipeline.fit(X_train, y_train)  results = pd.DataFrame(optimisation_pipeline.cv_results_) results.sort_values(\"rank_test_score\") <pre>Fitting 3 folds for each of 24 candidates, totalling 72 fits\n</pre> Out[4]: mean_fit_time std_fit_time mean_score_time std_score_time param_classifier__alpha param_classifier__fit_prior param_vectorizer__max_df param_vectorizer__min_df params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score 12 2.352721 0.257936 1.357399 0.094580 1 True 0.05 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.677283 0.670398 0.665671 0.671118 0.004768 1 15 2.544958 0.051976 1.245658 0.137108 1 True 0.1 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.676970 0.670244 0.666123 0.671112 0.004470 2 18 2.472888 0.053978 1.447181 0.051170 1 False 0.05 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.675678 0.669321 0.665305 0.670101 0.004271 3 21 2.467637 0.005358 1.216254 0.048319 1 False 0.1 5 {'classifier__alpha': 1, 'classifier__fit_prio... 0.675569 0.669482 0.665225 0.670092 0.004245 4 3 2.477612 0.134880 1.428521 0.017417 0.5 True 0.1 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.675225 0.668912 0.665652 0.669930 0.003974 5 0 2.484810 0.067836 1.249979 0.209959 0.5 True 0.05 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.675264 0.668781 0.665394 0.669813 0.004095 6 9 2.452453 0.049376 1.279694 0.089200 0.5 False 0.1 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.674199 0.668005 0.664456 0.668887 0.004026 7 6 2.748628 0.098077 1.255213 0.101334 0.5 False 0.05 5 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.673996 0.668048 0.664572 0.668872 0.003891 8 13 2.371907 0.204820 1.375143 0.193984 1 True 0.05 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.656459 0.647433 0.644864 0.649585 0.004972 9 16 2.557486 0.184332 1.272137 0.133980 1 True 0.1 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.656196 0.646960 0.645436 0.649530 0.004754 10 4 2.362093 0.109796 1.344960 0.083562 0.5 True 0.1 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.654424 0.644690 0.644991 0.648035 0.004519 11 1 2.668012 0.131751 1.383441 0.049157 0.5 True 0.05 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.654622 0.644901 0.644425 0.647983 0.004699 12 19 2.517576 0.154192 1.374269 0.026773 1 False 0.05 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.655125 0.645586 0.643166 0.647959 0.005163 13 22 2.386240 0.069718 0.967638 0.076766 1 False 0.1 20 {'classifier__alpha': 1, 'classifier__fit_prio... 0.654254 0.645409 0.643514 0.647726 0.004680 14 7 2.416211 0.138001 1.316785 0.036402 0.5 False 0.05 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.652727 0.643691 0.642749 0.646389 0.004498 15 10 2.397768 0.244951 1.330007 0.008983 0.5 False 0.1 20 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.652290 0.642975 0.643421 0.646229 0.004290 16 14 2.590127 0.054396 1.155976 0.187602 1 True 0.05 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.587097 0.577411 0.580170 0.581560 0.004075 17 17 2.497556 0.085952 1.169808 0.061336 1 True 0.1 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.585713 0.577197 0.580763 0.581224 0.003492 18 2 2.467611 0.168399 1.313936 0.109695 0.5 True 0.05 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.586346 0.576987 0.579852 0.581062 0.003915 19 5 2.339322 0.092595 1.209515 0.132825 0.5 True 0.1 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.585634 0.576217 0.580320 0.580723 0.003855 20 20 2.644488 0.119967 1.238554 0.048871 1 False 0.05 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.581096 0.572833 0.576280 0.576736 0.003389 21 8 2.456030 0.072136 1.240463 0.092383 0.5 False 0.05 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.580841 0.572391 0.575731 0.576321 0.003475 22 23 2.335796 0.082338 0.786187 0.073499 1 False 0.1 100 {'classifier__alpha': 1, 'classifier__fit_prio... 0.579769 0.572373 0.576699 0.576280 0.003034 23 11 2.530331 0.107199 1.247842 0.078229 0.5 False 0.1 100 {'classifier__alpha': 0.5, 'classifier__fit_pr... 0.579743 0.571607 0.576352 0.575901 0.003337 24 In\u00a0[5]: Copied! <pre>from sklearn import set_config\n\nset_config(display=\"diagram\")\n\nclassifier = create_pipeline()\nclassifier.set_params(**optimisation_pipeline.best_params_)\nclassifier.fit(X_train, y_train)\n</pre> from sklearn import set_config  set_config(display=\"diagram\")  classifier = create_pipeline() classifier.set_params(**optimisation_pipeline.best_params_) classifier.fit(X_train, y_train) Out[5]: <pre>Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(max_df=0.05, min_df=5, sublinear_tf=True)),\n                ('classifier', MultinomialNB(alpha=1))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(max_df=0.05, min_df=5, sublinear_tf=True)),\n                ('classifier', MultinomialNB(alpha=1))])</pre>TfidfVectorizer<pre>TfidfVectorizer(max_df=0.05, min_df=5, sublinear_tf=True)</pre>MultinomialNB<pre>MultinomialNB(alpha=1)</pre> In\u00a0[12]: Copied! <pre>from sklearn import metrics\nimport matplotlib.pyplot as plt\n\ny_predicted = classifier.predict(X_test)\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (20, 18)\nplt.rcParams[\"figure.facecolor\"] = \"white\"\nplt.rcParams[\"font.size\"] = 22\nplt.rcParams[\"axes.xmargin\"] = 0\n\nprint(metrics.classification_report(y_test, y_predicted, digits=4))\nmetrics.ConfusionMatrixDisplay.from_predictions(\n    y_true=y_test,\n    y_pred=y_predicted,\n    xticks_rotation=\"vertical\",\n    normalize=\"pred\",\n    values_format=\".2f\",\n)\nplt.tight_layout()\nplt.savefig(\"mag-confusion.png\", dpi=600)\n</pre> from sklearn import metrics import matplotlib.pyplot as plt  y_predicted = classifier.predict(X_test)  %matplotlib inline plt.rcParams[\"figure.figsize\"] = (20, 18) plt.rcParams[\"figure.facecolor\"] = \"white\" plt.rcParams[\"font.size\"] = 22 plt.rcParams[\"axes.xmargin\"] = 0  print(metrics.classification_report(y_test, y_predicted, digits=4)) metrics.ConfusionMatrixDisplay.from_predictions(     y_true=y_test,     y_pred=y_predicted,     xticks_rotation=\"vertical\",     normalize=\"pred\",     values_format=\".2f\", ) plt.tight_layout() plt.savefig(\"mag-confusion.png\", dpi=600) <pre>              precision    recall  f1-score   support\n\n    business     0.6412    0.7258    0.6808      3198\n   economics     0.6635    0.7146    0.6881      3189\n   geography     0.7461    0.6791    0.7111      3207\n    medicine     0.8806    0.9021    0.8912      3187\n    politics     0.5563    0.5895    0.5724      3169\n  psychology     0.7654    0.6811    0.7208      3252\n   sociology     0.5165    0.4698    0.4921      3197\n\n    accuracy                         0.6803     22399\n   macro avg     0.6814    0.6803    0.6795     22399\nweighted avg     0.6817    0.6803    0.6796     22399\n\n</pre>"},{"location":"examples/simple-mag/train/#train-a-field-of-study-domain-classification-of-sentences-on-the-mag-dataset","title":"Train a field of study (domain) classification of sentences on the MAG dataset\u00b6","text":"<p>SciBERT achieves an F1-score of 0.6571 on this dataset. This notebook shows that better results can be achieved without even using transformers.</p>"},{"location":"how-to-guides/call-remote/","title":"How to call remote GreatAI instances","text":"<p>Microservices architecture (or SOA) work well with ML applications. This is because their interfaces are usually very narrow, while the functionality provided is quite comprehensive. Hence, drawing the boundaries of responsibilities is more straightforward in the case of ML services than in the case of more traditional business applications. For this reason, it is common to have a tree of models (preferably wrapped in GreatAI instances) communicating with each other.</p> <p>Although regular HTTP POST requests could be sent to each service's <code>/predict</code> endpoint, <code>great-ai</code> comes with two convenience functions: call_remote_great_ai and call_remote_great_ai_async to wrap this request. These provide you with some level of robustness and deserialisation.</p> <p>Inside notebooks</p> <p>The async variant, call_remote_great_ai_async, requires a running event loop while the synchronous variant disallows other running event-loops. Therefore, when running inside a Jupyter Notebook, always call call_remote_great_ai_async.</p>"},{"location":"how-to-guides/call-remote/#simple-example","title":"Simple example","text":"<p>Let's create two processes: a server and a client.</p>"},{"location":"how-to-guides/call-remote/#server","title":"Server","text":"server.py<pre><code>from great_ai import GreatAI\nfrom asyncio import sleep\n\n@GreatAI.create\nasync def slow_greeter(your_name):\n    await sleep(2)\n    return f'Hi {your_name}!'\n</code></pre> <p>Run this in development mode by executing <code>great-ai server.py</code> or <code>python3 -m great_ai server.py</code> if you're on Windows and <code>great-ai</code> is not in your <code>PATH</code>.</p>"},{"location":"how-to-guides/call-remote/#client","title":"Client","text":"client.py<pre><code>from great_ai import call_remote_great_ai\n\nnames = ['Oliv\u00e9r', 'Bal\u00e1zs', 'Andr\u00e1s']\n\nresults = [\n    call_remote_great_ai(\n        'http://localhost:6060',\n        {\n            'your_name': name\n        }\n    ).output #(1)\n    for name in names\n]\n\nprint(results)\n</code></pre> <ol> <li>Only return the outputs, so we don't clutter up the terminal.</li> </ol> <p>Run this script as a regular Python script by executing <code>python3 client.py</code>.</p> <p></p> <p>As you can see, everything worked as expected. There is one way to improve it, though.</p>"},{"location":"how-to-guides/call-remote/#an-async-example","title":"An <code>async</code> example","text":"<p>Let's send multiple requests simultaneously to speed up the overall execution time. To do this, we will use the call_remote_great_ai_async function.</p> Why is this possible? <p>Note that in <code>server.py</code>, the inference function is declared <code>async</code>. This means that multiple \"copies\" of it can run at the same time in the same thread. Since there is no CPU bottleneck, the server has a quite large throughput (requests responded to per second), but its latency will stay around 2 seconds due to the async <code>sleep</code> command.</p> <p>If your great-ai server is not <code>async</code>, higher throughput can be achieved by running multiple instances of it, either manually or by running it with multiple <code>uvicorn</code> workers like this: <code>ENVIRONMENT=production great-ai server.py --worker_count 4</code></p>"},{"location":"how-to-guides/call-remote/#async-client","title":"Async client","text":"async-client.py<pre><code>from great_ai import call_remote_great_ai_async\nimport asyncio\n\nnames = ['Oliv\u00e9r', 'Bal\u00e1zs', 'Andr\u00e1s']\n\nasync def main():\n    futures = [\n        call_remote_great_ai_async(\n            'http://localhost:6060',\n            {\n                'your_name': name\n            }\n        ) for name in names\n    ]\n\n    results = await asyncio.gather(*futures)\n    print([r.output for r in results])\n\nasyncio.run(main())\n</code></pre> <p>Replace <code>client.py</code> with this async client. Note that although async support is significantly more streamlined in recent Python versions, it still requires a bit more boilerplate than its synchronous counterpart.</p> <p></p> <p>This also works and might be considerably quicker in some use cases.</p>"},{"location":"how-to-guides/configure-service/","title":"How to configure GreatAI","text":"<p>GreatAI aims to provide reasonable defaults wherever possible. The current configuration is always prominently displayed (and updated) on the dashboard and in the command-line start-up banner.</p>"},{"location":"how-to-guides/configure-service/#using-great_aiconfigure","title":"Using great_ai.configure","text":"<p>You can override any of the default settings by calling great_ai.configure. If you don't call <code>configure</code>, the default settings are applied on the first call to most <code>great-ai</code> functions.</p> <p>Warning</p> <p>You must call great_ai.configure before calling (or decorating with) any other <code>great-ai</code> function. However, importing other functions before calling great_ai.configure is permitted.</p> configure-demo.py<pre><code>from great_ai import configure, RouteConfig\nimport logging\n\nconfigure(\n    version='1.0.0',\n    log_level=logging.INFO,\n    seed=2,\n    should_log_exception_stack=False, \n    prediction_cache_size=0,  #(1)\n    disable_se4ml_banner=True,\n    dashboard_table_size=200,\n    route_config=RouteConfig(  #(2)\n        feedback_endpoints_enabled=False,\n        dashboard_enabled=False\n    )\n)\n</code></pre> <ol> <li>Completely disable caching.</li> <li>The unspecified routes are enabled by default.</li> </ol>"},{"location":"how-to-guides/configure-service/#using-remote-storage","title":"Using remote storage","text":"<p>The only aspect that cannot be automated is choosing the backing storage for the database and file storage.</p> <p>Right now, you have 3 options for storing the models and large datasets: LargeFileLocal, LargeFileMongo, and LargeFileS3.</p> <p>Without explicit configuration, LargeFileLocal is selected by default. This one still version-controls your files but it only stores them in a local path (which of course can be a remote volume attached by NFS, HDFS, etc.).</p> <p>Important</p> <p>If your working directory contains a <code>mongo.ini</code> or <code>s3.ini</code> file, an attempt is made to auto-configure LargeFileMongo or LargeFileS3 respectively.</p> <p>To use LargeFileMongo or LargeFileS3 explicitly, configure them before calling any other <code>great-ai</code> function.</p>"},{"location":"how-to-guides/configure-service/#s3-compatible","title":"S3-compatible","text":"s3.ini<pre><code>aws_region_name = eu-west-2\naws_access_key_id = MY_AWS_ACCESS_KEY  # ENV:MY_AWS_ACCESS_KEY would also work\naws_secret_access_key = MY_AWS_SECRET_KEY\nlarge_files_bucket_name = bucket-for-models\n</code></pre> use-s3.py<pre><code>from great_ai.large_file import LargeFileS3\nfrom great_ai import save_model\n\nLargeFileS3.configure_credentials_from_file('s3.ini') #(1)\n\nmodel = [4, 3]\nsave_model(model, 'my-model')\n</code></pre> <ol> <li>This line isn't strictly necessary because if <code>s3.ini</code> (or <code>mongo.ini</code>) is available in the current working directory, they are automatically used to configure their respective LargeFile implementations/databases.</li> </ol> Departing from AWS <p>With the <code>aws_endpoint_url</code> argument, it is possible to use any other S3-compatible service such as Backblaze. In that case, it would be <code>aws_endpoint_url=https://s3.us-west-002.backblazeb2.com</code>.</p>"},{"location":"how-to-guides/configure-service/#gridfs","title":"GridFS","text":"<p>GridFS specifies how to store files in MongoDB. The official MongoDB server and many compatible implementations support it.</p> mongo.ini<pre><code>MONGO_CONNECTION_STRING=mongodb://localhost:27017  # this is the default value\n# if `MONGO_CONNECTION_STRING` is specified, this default is overridden\nMONGO_CONNECTION_STRING=ENV:MONGO_CONNECTION_STRING\n\nMONGO_DATABASE=my-database  # it is automatically created if it doesn't exist\n</code></pre> use-mongo.py<pre><code>from great_ai.large_file import LargeFileMongo\nfrom great_ai import save_model\n\nLargeFileMongo.configure_credentials_from_file('mongo.ini')\n\nmodel = [4, 3]\nsave_model(model, 'my-model')\n</code></pre> <p>Simplifying config files</p> <p>You can combine <code>mongo.ini</code> or <code>s3.ini</code> with your application's config file because the unneeded keys are ignored by the <code>configure_credentials_from_file</code> method.</p>"},{"location":"how-to-guides/configure-service/#using-a-database","title":"Using a database","text":"<p>By default, a thread-safe version of TinyDB is utilised for saving the prediction traces into a local file. Unfortunately, for most production needs, this method is not suitable.</p>"},{"location":"how-to-guides/configure-service/#mongodb","title":"MongoDB","text":"<p>Currently, only MongoDB is supported as a production-ready <code>TracingDatabase</code>. In order to use it, you have to either place a file named <code>mongo.ini</code> in your working directory or explicitly call either MongoDbDriver.configure_credentials_from_file or MongoDbDriver.configure_credentials.</p>"},{"location":"how-to-guides/create-service/","title":"How to create a GreatAI service","text":"<p>The core value of <code>great-ai</code> lies in its GreatAI class. To take advantage of it, you need to create an instance wrapping your code.</p> <p>Let's say that you have the following greeter function:</p> greeter.py<pre><code>def my_greeter_function(your_name):\n    return f'Hi {your_name}!'\n</code></pre> <p>You can simply decorate (wrap) this function using the @GreatAI.create factory.</p> greeter.py<pre><code>from great_ai import GreatAI\n\n@GreatAI.create\ndef greeter(your_name):\n    return f'Hi {your_name}!'\n</code></pre> Why not simply use <code>@GreatAI?</code> <p>The purpose of @GreatAI.create is simply to provide you with type-checking through MyPy, Pylance, and similar libraries. However, the overloading support for <code>__new__</code> is lacking in MyPy. Thus, a static factory method is used instead.</p>"},{"location":"how-to-guides/create-service/#with-types","title":"With types","text":"<p>Even though it's not required by GreatAI, type annotating your codebase can save you from lots of trivial mistakes; that's why it's highly advised. Simply add the expected types to your function's signature.</p> type_safe_greeter.py<pre><code>from great_ai import GreatAI\n\n@GreatAI.create\ndef type_safe_greeter(your_name: str) -&gt; str:\n    return f'Hi {your_name}!'\n</code></pre> <p>This not only allows you to statically type-check your code, but by default, GreatAI will check it during runtime as well using typeguard.</p>"},{"location":"how-to-guides/create-service/#with-async","title":"With async","text":"<p>Asynchronous code can result in immense performance gains in some instances. For example, you might rely on a third-party service, do database access, or call a remote GreatAI instance. In these cases, you can make your function <code>async</code> without any other changes.</p> async_greeter.py<pre><code>from great_ai import GreatAI\nfrom asyncio import sleep\n\n@GreatAI.create\nasync def async_greeter(your_name: str) -&gt; str:\n    await sleep(2)  # simulate IO-bound operation\n    return f'Hi {your_name}!'\n</code></pre>"},{"location":"how-to-guides/create-service/#with-decorators","title":"With decorators","text":"<p>GreatAI can decorate already decorated functions. The only restriction is that @GreatAI.create must come last. There are two built-in decorators that you can use to customise your function, but you can use any third-party decorator as well.</p>"},{"location":"how-to-guides/create-service/#using-use_model","title":"Using <code>@use_model</code>","text":"<p>If you have previously saved a model with save_model, you can inject it into your function by calling @use_model.</p> greeter_with_model.py<pre><code>from great_ai import GreatAI, use_model\n\n@GreatAI.create\n@use_model('name_of_my_model', version='latest')  #(1)\ndef type_safe_greeter(your_name: str, model) -&gt; str:\n    return f'Hi {your_name}!'\n\nassert type_safe_greeter('Andras').output == 'Hi Andras'\n</code></pre> <ol> <li>By default, the parameter named <code>model</code> will be replaced by the loaded model. This behaviour can be customised by setting the <code>model_kwarg_name</code>. This way, even multiple models can be injected into a single function.</li> </ol> <p>Important</p> <p>You must call @use_model before @GreatAI.create. Note that decorators are applied starting from the bottom-most one. Feel free to use @use_model in other places of the codebase, and it works equally well outside GreatAI services. </p>"},{"location":"how-to-guides/create-service/#using-parameter","title":"Using <code>@parameter</code>","text":"<p>If you wish to turn off logging or specify custom validation for your parameters, you can use the @parameter decorator.</p> <p>Note</p> <p>By default, all parameters that are not affected by an explicit @parameter or @use_model are automatically decorated with @parameter when @GreatAI.create is called.</p> greeter_with_validation.py<pre><code>from great_ai import GreatAI, use_model\n\n@GreatAI.create\n@parameter('your_name', disable_logging=True)\ndef type_safe_greeter(your_name: str, model) -&gt; str:\n    return f'Hi {your_name}!'\n\nassert type_safe_greeter('Andras').output == 'Hi Andras'\n</code></pre> <p>Important</p> <p>You must call @parameter before @GreatAI.create. Note that decorators are applied starting from the bottom-most one. Feel free to use @parameter in other places of the codebase, and it works equally well outside GreatAI services. </p>"},{"location":"how-to-guides/create-service/#complex-example","title":"Complex example","text":"<p>The following example summarises the options you have when instantiating a GreatAI service.</p> complex.py<pre><code>from great_ai import save_model, GreatAI, parameter, use_model, log_metric\n\nsave_model(4, 'secret-number')  #(1)\n\n@GreatAI.create\n@parameter('positive_number', validate=lambda n: n &gt; 0, disable_logging=True)\n@use_model('secret-number', version='latest', model_kwarg_name='secret')\ndef add_number(positive_number: int, secret: int) -&gt; int:\n    log_metric(\n        'log directly into the returned Trace', \n        positive_number * 2\n    )\n    return positive_number + secret\n\nassert add_number(1).output == 5\n</code></pre> <ol> <li>Refer to the configuration page for specifying where to store your models. </li> </ol>"},{"location":"how-to-guides/handle-training-data/","title":"How to manage training data","text":"<p>In order to simplify your training data management, <code>great-ai</code> provide two complementing approaches for inputting new data points.</p>"},{"location":"how-to-guides/handle-training-data/#upload-data","title":"Upload data","text":"<p>At the start of your experiments' first iteration, after you've gathered suitable samples for training, you can call great_ai.add_ground_truth. This automatically stores a timestamp and also allows you to assign tags to the data. Using these attributes, great_ai.query_ground_truth can be called to get a filtered view of the training data.</p> <p>Train-test-validation splits</p> <p>It is a best practice to lock away the test split of your data that is only used for the final quality assessment. This prevents you from accidentally training on it or inadvertently tuning the model to have the highest accuracy metrics on the test split. This, of course, may lead to dubious results; hence, care must be taken to avoid it.</p> <p>With great_ai.add_ground_truth, there is an option to tag the samples with <code>train</code>, <code>test</code>, and <code>validation</code> randomly, following a predefined distribution. This happens as soon as they're written in the database. Later, these can be queried by providing the name of the appropriate tags.</p> <p>The nice thing about this is that the 'input-expected output' pairs are stored as traces. Thus, they behave exactly like regular prediction traces.</p> <pre><code>from great_ai import add_ground_truth\n\nadd_ground_truth(\n    [1, 2],\n    ['odd', 'even'],\n    tags='my_tag',\n    train_split_ratio=1,  #(1)\n    test_split_ratio=1\n)\n</code></pre> <ol> <li>Note that the ratios don't have to add up to 1. They are just weights. There is also a <code>validation_split_ratio</code> which is 0 by default.</li> </ol> <pre><code>&gt;&gt;&gt; from great_ai import query_ground_truth\n&gt;&gt;&gt; query_ground_truth('my_tag')    \n[Trace[str]({'created': '2022-07-12T18:36:12.825706',\n   'exception': None,\n   'feedback': 'odd',  #(1)\n   'logged_values': {'input': 1},  #(2)\n   'models': [],\n   'original_execution_time_ms': 0.0,\n   'output': 'odd',\n   'tags': ['ground_truth', 'test', 'my_tag'], #(3) \n   'trace_id': '4fcf2ce6-a172-469d-94b2-874577655814'}),\n Trace[str]({'created': '2022-07-12T18:36:12.825706',\n   'exception': None,\n   'feedback': 'even',\n   'logged_values': {'input': 2},\n   'models': [],\n   'original_execution_time_ms': 0.0,\n   'output': 'even',\n   'tags': ['ground_truth', 'train', 'my_tag'],\n   'trace_id': 'abee0671-beb9-4284-8c3b-c65e5836ce38'})]\n</code></pre> <ol> <li>Expected output. This can also be accessed through the <code>.output</code> property.</li> <li>The input value is stored here.</li> <li>Notice how <code>ground_truth</code> is always included as a tag when using great_ai.add_ground_truth. </li> </ol>"},{"location":"how-to-guides/handle-training-data/#get-feedback","title":"Get feedback","text":"<p>After the initial data gathering, end-to-end feedback can also be integrated into the dataset. </p> <p>The scaffolded REST API contains endpoints for managing traces and their feedbacks.</p> <p></p> <p>When great_ai.query_ground_truth is executed, it implicitly filters for traces that have feedback. Therefore, both the <code>ground_truth</code> and the <code>online</code> traces that have received feedback are returned. No matter the origin of the data, it can be accessed using the same API.</p>"},{"location":"how-to-guides/handle-training-data/#remove-clutter","title":"Remove clutter","text":"<p>Traces can be deleted either through the REST API or by calling great_ai.delete_ground_truth. The latter provides the same interface as great_ai.query_ground_truth except it deletes the matched points.</p>"},{"location":"how-to-guides/install/","title":"Installation guide","text":"<p>Provided you already have Python3 (and pip) installed, simply execute:</p> <pre><code>pip install great-ai\n</code></pre> <p>Python 3.7 or later is required.</p> <p>This will work on all major operating systems.</p>"},{"location":"how-to-guides/install/#google-colab","title":"Google Colab","text":"<p>In order to use GreatAI in Google Colab, you need to downgrade <code>pyyaml</code> to a Colab compatible version. See related StackOverflow question.</p> <pre><code>pip install great-ai pyyaml==5.4.1\n</code></pre> <p>This will make GreatAI work in Colab.</p>"},{"location":"how-to-guides/install/#command-line-tools","title":"Command-line tools","text":"<p>After installation, <code>great-ai</code> and <code>large-file</code> are available as commands. The former is required for deploying your application, while the latter lets you manage models and datasets from your terminal.</p> Snakes &amp; kebabs <p>The library is called <code>great-ai</code>; therefore, its command-line entry point is also called <code>great-ai</code>. However, Python module names cannot contain hyphens, that's why you have to <code>import great_ai</code> with an underscore. The <code>great-ai</code> CLI tool is also available as <code>python3 -m great_ai</code>.</p> <p>To help with the confusion, a CLI executable called <code>great_ai</code> (and <code>large_file</code>) are also installed. Thus, if you prefer, you can always refer to GreatAI using its underscored name variant (<code>great_ai</code>).</p> <p>Windows</p> <p>On Windows, you might encounter a similar warning from <code>pip</code>:</p> <p><code>WARNING: The scripts great-ai.exe, great_ai.exe, large-file.exe and large_file.exe are installed in 'C:\\Users\\...\\Scripts', which is not on PATH.</code></p> <p><code>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</code></p> <p>This means that <code>great-ai.exe</code> and <code>large-file.exe</code> are not in your <code>PATH</code>. Either add their containing directory ('C:\\Users...\\Scripts' in this case) to your <code>PATH</code> or use <code>python3 -m great_ai</code> and <code>python3 -m great_ai.large_file</code> instead of the exe-s.</p>"},{"location":"how-to-guides/install/#update","title":"Update","text":"<p>If you wish to update to the latest version, execute:</p> <pre><code>pip install --upgrade great-ai\n</code></pre>"},{"location":"how-to-guides/install/#bleeding-edge","title":"Bleeding edge","text":"<p>You can also install the latest (usually unreleased) version from GitHub.</p> <pre><code>pip install --upgrade git+https://github.com/schmelczer/great-ai.git\n</code></pre> <p>Python 3.7 or later is required.</p>"},{"location":"how-to-guides/large-file/","title":"How to use LargeFiles","text":"<p>The functions save_model and @use_model wrap LargeFile instances. Hence, besides configuring LargeFile, users have few reasons to use LargeFiles directly.</p>"},{"location":"how-to-guides/large-file/#motivation","title":"Motivation","text":"<p>Often, especially when working with data-heavy applications, large files can proliferate in a repository. Version controlling them is an obvious next step. However, GitHub's git LFS implementation doesn't support deleting large files, making it easy for them to eat-up the LFS quota and explode the size of your repos.</p> <p>DVC is a viable alternative; however, it requires users to learn to use one more CLI tool.</p> Using LargeFile-s directly (usually not needed) <p>LargeFile doesn't require users to learn too much new. It is a nearly exact copy of Python's built-in <code>open()</code> function, with which users are undoubtedly already familiar.</p>"},{"location":"how-to-guides/large-file/#simple-example","title":"Simple example","text":"<pre><code>from great_ai.large_file import LargeFileS3\n\nLargeFileS3.configure_credentials({\n    \"aws_region_name\": \"your_region_like_eu-west-2\",\n    \"aws_access_key_id\": \"YOUR_ACCESS_KEY_ID\",\n    \"aws_secret_access_key\": \"YOUR_VERY_SECRET_ACCESS_KEY\",\n    \"large_files_bucket_name\": \"create_a_bucket_and_put_its_name_here\",\n})\n\n# Creates a new version and deletes the older version \n# leaving the three most recently used intact\nwith LargeFileS3(\"test.txt\", \"w\", keep_last_n=3) as f:\n    for i in range(100000):\n        f.write('test\\n')\n\n# The latest version is returned by default\n# but an optional `version` keyword argument can be provided as well\nwith LargeFileS3(\"test.txt\", \"r\") as f:  #(1)\n    print(f.readlines()[0])\n</code></pre> <ol> <li>The latest version is already in the local cache; no download is required.</li> </ol>"},{"location":"how-to-guides/large-file/#more-details","title":"More details","text":"<p><code>LargeFile</code> behaves like an opened file (in the background, it is a temp file after all). Binary reads and writes are supported along with the different keywords <code>open()</code> accepts.</p> <p>The local cache can be configured with these properties:</p> <pre><code>LargeFileS3.cache_path = Path('.cache')\nLargeFileS3.max_cache_size = \"30 GB\"\n</code></pre>"},{"location":"how-to-guides/large-file/#i-only-need-a-path","title":"I only need a path","text":"<p>In case you only need a path to the (proxy of the) remote file, this pattern can be applied:</p> <pre><code>path_to_model = LargeFileS3(\"folder-of-my-bert-model\", version=31).get()\n</code></pre> <p>This will first download the file/folder into your local cache folder. Then, it returns a <code>Path</code> object to the local version. Which can be turned into a string with <code>str(path_to_model)</code>.</p> <p>The same approach works for uploads:</p> <pre><code>LargeFileS3(\"folder-of-my-bert-model\").push('path_to_local/folder_or_file')\n</code></pre> <p>This way, both regular files and folders can be handled. The uploaded file is called folder-of-my-bert-model, the local name is ignored.</p> <p>Lastly, all version of the remote object can be deleted by calling <code>LargeFileS3(\"my-file\").delete()</code>. It will still reside in your local cache afterwards; its deletion will happen next time your local cache has to be pruned.</p>"},{"location":"how-to-guides/large-file/#from-the-command-line","title":"From the command-line","text":"<p>The main reason for using the <code>large-file</code> or <code>python3 -m great_ai.large_file</code> commands is to upload or download models from the terminal. For example, when building a docker image, it is best practice to cache the referred models.</p>"},{"location":"how-to-guides/large-file/#setup","title":"Setup","text":"<p>Create an .ini file (or use ~/.aws/credentials). It may look like this:</p> <pre><code>aws_region_name = your_region_like_eu-west-2\naws_access_key_id = YOUR_ACCESS_KEY_ID\naws_secret_access_key = YOUR_VERY_SECRET_ACCESS_KEY\nlarge_files_bucket_name = my_large_files\n</code></pre>"},{"location":"how-to-guides/large-file/#upload-some-files","title":"Upload some files","text":"<pre><code>large-file --backend s3 --secrets secrets.ini \\\n    --push my_first_file.json folder/my_second_file my_folder\n</code></pre> <p>Only the filename is used as the S3 name; the rest of the path is ignored.</p> <p>Using MongoDB</p> <p>The possible values for <code>--backend</code> are <code>s3</code>, <code>mongo</code>, and <code>local</code>. The latter doesn't need credentials. It only versions and stores your files in a local folder. MongoDB, on the other hand, requires a <code>mongo_connection_string</code> and a <code>mongo_database</code> to be specified. For storing large files, it uses the GridFS specification.</p>"},{"location":"how-to-guides/large-file/#download-some-files-to-the-local-cache","title":"Download some files to the local cache","text":"<p>This can be useful when building a Docker image, for example. This way, the files can already reside inside the container and need not be downloaded later.</p> <pre><code>large-file --backend s3 --secrets ~/.aws/credentials \\\n    --cache my_first_file.json:3 my_second_file my_folder:0\n</code></pre> <p>Versions may be specified by using <code>:</code>-s.</p>"},{"location":"how-to-guides/large-file/#delete-remote-files","title":"Delete remote files","text":"<pre><code>large-file --backend s3 --secrets ~/.aws/credentials \\\n    --delete my_first_file.json\n</code></pre>"},{"location":"how-to-guides/use-service/","title":"How to perform prediction with GreatAI","text":"<p>After creating a GreatAI service by wrapping your prediction function, and optionally configuring it, it's time to do some prediction.</p> <p>Let's take the following example:</p> greeter.py<pre><code>from great_ai import GreatAI\n\n@GreatAI.create\ndef greeter(your_name: str) -&gt; str:\n    return f'Hi {your_name}!'\n</code></pre>"},{"location":"how-to-guides/use-service/#one-off-prediction","title":"One-off prediction","text":"<p>Even though <code>greeter</code> is now an instance of GreatAI, you can continue using it as a regular function.</p> <pre><code>&gt;&gt;&gt; greeter('Bob')\nTrace[str]({'created': '2022-07-11T14:31:46.183764',\n  'exception': None,\n  'feedback': None,\n  'logged_values': {'arg:your_name:length': 3, 'arg:your_name:value': 'Bob'},\n  'models': [],\n  'original_execution_time_ms': 0.0381,\n  'output': 'Hi Bob!',\n  'tags': ['greeter', 'online', 'development'],\n  'trace_id': '7c284fd7-7f0d-4464-b5f8-3ef126df34af'})\n</code></pre> <p>As you can see, the original return value is wrapped in a Trace object (which is also persisted in your database of choice). You can access the original value under the <code>output</code> property.</p>"},{"location":"how-to-guides/use-service/#online-prediction","title":"Online prediction","text":"<p>Likely, the main way you would like to expose your model is through an HTTP API. @GreatAI.create scaffolds many REST API endpoints for your model and creates a FastAPI app available under GreatAI.app. This can be served using uvicorn or any other ASGI server.</p> <p>Since most ML code lives in Jupyter notebooks, therefore, deploying a notebook containing the inference function is supported. To achieve this, <code>uvicorn</code> is wrapped by the <code>great-ai</code> command-line utility, which \u2014 among others \u2014 takes care of feeding a notebook into <code>uvicorn</code>. It also supports auto-reloading.</p>"},{"location":"how-to-guides/use-service/#in-development","title":"In development","text":"<pre><code>great-ai greeter.py\n</code></pre> <p>Success</p> <p>Your model is accessible at localhost:6060.</p> <p>Some configuration options are also supported.</p> <pre><code>great-ai greeter.py --port 8000 --host 127.0.0.1 --timeout_keep_alive 10\n</code></pre> More options <p>For more options (but no Notebook support), simply use uvicorn for starting your app (available at <code>greeter.app</code>).</p>"},{"location":"how-to-guides/use-service/#in-production","title":"In production","text":"<p>There are three main approaches for deploying a GreatAI service.</p>"},{"location":"how-to-guides/use-service/#manual-deployment","title":"Manual deployment","text":"<p>The app is run in production-mode if the value of the <code>ENVIRONMENT</code> environment variable is set to <code>production</code>.</p> <pre><code>ENVIRONMENT=production great-ai greeter.py\n</code></pre> <p>Simply run <code>ENVIRONMENT=production great-ai deploy.ipynb</code> in the command-line of a production machine.</p> <p>This is the crudest approach; however, it might be fitting for some contexts.</p>"},{"location":"how-to-guides/use-service/#containerised-deployment","title":"Containerised deployment","text":"<p>Run the notebook directly in a container or create a service for it using your favourite container orchestrator.</p> <pre><code>docker run -p 6060:6060 --volume `pwd`:/app --rm \\\n  schmelczera/great-ai deploy.ipynb\n</code></pre> <p>You can replace <code>pwd</code> with the path to your code's folder.</p>"},{"location":"how-to-guides/use-service/#use-a-platform-as-a-service","title":"Use a Platform-as-a-Service","text":"<p>Similar to the previous approach, your code will run in a container. However, instead of manually managing it, you can just choose from a plethora of PaaS providers (such as AWS ECS, DO App platform, MLEM, Streamlit) that take a Docker image as a source and handle the rest of the deployment.</p> <p>To this end, you can also create a custom Docker image. It is especially useful if you have third-party dependencies, such as PyTorch or TensorFlow.</p> <pre><code>FROM schmelczera/great-ai:latest\n\n# Remove this block if you don't have a requirements.txt\nCOPY requirements.txt ./   \nRUN pip install --no-cache-dir --requirement requirements.txt\n\n# If you store your models in S3 or GridFS, it may be a \n# good idea to cache them in the image so that you don't\n# have to download it each time a container starts\nRUN large-file --backend s3 --secrets s3.ini --cache my-domain-predictor\n\n# Add your application code to the image\nCOPY . .\n\n# The default ENTRYPOINT is great-ai; specify its argument using CMD\nCMD [\"deploy.ipynb\"]\n</code></pre>"},{"location":"how-to-guides/use-service/#batch-prediction","title":"Batch prediction","text":"<p>Processing larger amounts of data on a single machine is made easy by the GreatAI's process_batch method. This relies on multiprocessing (parallel_map) to take full advantage of all available CPU-cores.</p> <pre><code>&gt;&gt;&gt; greeter.process_batch(['Alice', 'Bob'])\n[Trace[str]({'created': '2022-07-11T14:36:37.119183',\n   'exception': None,\n   'feedback': None,\n   'logged_values': {'arg:your_name:length': 5, 'arg:your_name:value': 'Alice'},\n   'models': [],\n   'original_execution_time_ms':  0.1251,\n   'output': 'Hi Alice!',\n   'tags': ['greeter', 'online', 'development'],\n   'trace_id': '90ffa15f-e839-41c4-8e7a-3211168bc138'}),\n Trace[str]({'created': '2022-07-11T14:36:37.166659',\n   'exception': None,\n   'feedback': None,\n   'logged_values': {'arg:your_name:length': 3, 'arg:your_name:value': 'Bob'},\n   'models': [],\n   'original_execution_time_ms':  0.0571,\n   'output': 'Hi Bob!',\n   'tags': ['greeter', 'online', 'development'],\n   'trace_id': 'f48e94c7-0815-48b3-a864-41349d3dae84'})]\n</code></pre>"},{"location":"reference/","title":"GreatAI reference","text":"<pre><code>from great_ai import *\n</code></pre>"},{"location":"reference/#core","title":"Core","text":""},{"location":"reference/#great_ai.GreatAI","title":"<code>GreatAI</code>","text":"<p>             Bases: <code>Generic[T, V]</code></p> <p>Wrapper for a prediction function providing the implementation of SE4ML best practices.</p> <p>Provides caching (with argument freezing), a TracingContext during execution, the scaffolding of HTTP endpoints using FastAPI and a dashboard using Dash.</p> <p>IMPORTANT: when a request is served from cache, no new trace is created. Thus, the same trace can be returned multiple times. If this is undesirable turn off caching using <code>configure(prediction_cache_size=0)</code>.</p> <p>Supports wrapping async and synchronous functions while also maintaining correct typing.</p> <p>Attributes:</p> Name Type Description <code>app</code> <p>FastAPI instance wrapping the scaffolded endpoints and the Dash app.</p> <code>version</code> <p>SemVer derived from the app's version and the model names and versions registered through use_model.</p> Source code in <code>great_ai/deploy/great_ai.py</code> <pre><code>class GreatAI(Generic[T, V]):\n    \"\"\"Wrapper for a prediction function providing the implementation of SE4ML best practices.\n\n    Provides caching (with argument freezing), a TracingContext during execution, the\n    scaffolding of HTTP endpoints using FastAPI and a dashboard using Dash.\n\n    IMPORTANT: when a request is served from cache, no new trace is created. Thus, the\n    same trace can be returned multiple times. If this is undesirable turn off caching\n    using `configure(prediction_cache_size=0)`.\n\n    Supports wrapping async and synchronous functions while also maintaining correct\n    typing.\n\n    Attributes:\n        app: FastAPI instance wrapping the scaffolded endpoints and the Dash app.\n        version: SemVer derived from the app's version and the model names and versions\n            registered through use_model.\n    \"\"\"\n\n    __name__: str  # help for MyPy\n    __doc__: str  # help for MyPy\n\n    def __init__(\n        self,\n        func: Callable[..., Union[V, Awaitable[V]]],\n    ):\n        \"\"\"Do not call this function directly, use GreatAI.create instead.\"\"\"\n\n        func = automatically_decorate_parameters(func)\n        get_function_metadata_store(func).is_finalised = True\n\n        self._cached_func = self._get_cached_traced_function(func)\n        self._wrapped_func = wraps(func)(freeze_arguments(self._cached_func))\n\n        wraps(func)(self)\n        self.__doc__ = (\n            f\"GreatAI wrapper for interacting with the `{self.__name__}` \"\n            + f\"function.\\n\\n{dedent(self.__doc__ or '')}\"\n        )\n\n        self.version = str(get_context().version)\n        flat_model_versions = \".\".join(f\"{k}-v{v}\" for k, v in model_versions)\n        if flat_model_versions:\n            self.version += f\"+{flat_model_versions}\"\n\n        self.app = FastAPI(\n            title=snake_case_to_text(self.__name__),\n            version=self.version,\n            description=self.__doc__\n            + f\"\\n\\nFind out more in the [dashboard]({DASHBOARD_PATH}).\",\n            docs_url=None,\n            redoc_url=None,\n        )\n\n        self._bootstrap_rest_api()\n\n    @overload\n    @staticmethod\n    def create(  # type: ignore\n        # Overloaded function signatures 1 and 2 overlap with incompatible return types\n        # https://github.com/python/mypy/issues/12759\n        func: Callable[..., Awaitable[V]],\n    ) -&gt; \"GreatAI[Awaitable[Trace[V]], V]\":\n        ...\n\n    @overload\n    @staticmethod\n    def create(\n        func: Callable[..., V],\n    ) -&gt; \"GreatAI[Trace[V], V]\":\n        ...\n\n    @staticmethod\n    def create(\n        func: Union[Callable[..., Awaitable[V]], Callable[..., V]],\n    ) -&gt; Union[\"GreatAI[Awaitable[Trace[V]], V]\", \"GreatAI[Trace[V], V]\"]:\n        \"\"\"Decorate a function by wrapping it in a GreatAI instance.\n\n        The function can be typed, synchronous or async. If it has\n        unwrapped parameters (parameters not affected by a\n        [@parameter][great_ai.parameter] or [@use_model][great_ai.use_model] decorator),\n        those will be automatically wrapped.\n\n        The return value is replaced by a Trace (or Awaitable[Trace]),\n        while the original return value is available under the `.output`\n        property.\n\n        For configuration options, see [great_ai.configure][].\n\n        Examples:\n            &gt;&gt;&gt; @GreatAI.create\n            ... def my_function(a):\n            ...     return a + 2\n            &gt;&gt;&gt; my_function(3).output\n            5\n\n            &gt;&gt;&gt; @GreatAI.create\n            ... def my_function(a: int) -&gt; int:\n            ...     return a + 2\n            &gt;&gt;&gt; my_function(3)\n            Trace[int]...\n\n            &gt;&gt;&gt; my_function('3').output\n            Traceback (most recent call last):\n                ...\n            TypeError: type of a must be int; got str instead\n\n        Args:\n            func: The prediction function that needs to be decorated.\n\n        Returns:\n            A GreatAI instance wrapping `func`.\n        \"\"\"\n\n        return GreatAI[Trace[V], V](\n            func,\n        )\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; T:\n        return self._wrapped_func(*args, **kwargs)\n\n    @overload\n    def process_batch(\n        self,\n        batch: Sequence[Tuple],\n        *,\n        concurrency: Optional[int] = None,\n        unpack_arguments: Literal[True],\n        do_not_persist_traces: bool = ...,\n    ) -&gt; List[Trace[V]]:\n        ...\n\n    @overload\n    def process_batch(\n        self,\n        batch: Sequence,\n        *,\n        concurrency: Optional[int] = None,\n        unpack_arguments: Literal[False] = ...,\n        do_not_persist_traces: bool = ...,\n    ) -&gt; List[Trace[V]]:\n        ...\n\n    def process_batch(\n        self,\n        batch: Sequence,\n        *,\n        concurrency: Optional[int] = None,\n        unpack_arguments: bool = False,\n        do_not_persist_traces: bool = False,\n    ) -&gt; List[Trace[V]]:\n        \"\"\"Map the wrapped function over a list of input_values (`batch`).\n\n        A wrapper over [parallel_map][great_ai.utilities.parallel_map.parallel_map.parallel_map]\n        providing type-safety and a progressbar through tqdm.\n\n        Args:\n            batch: A list of arguments for the original (wrapped) function. If the\n                function expects multiple arguments, provide a list of tuples and set\n                `unpack_arguments=True`.\n            concurrency: Number of processes to start. Don't set it too much higher than\n                the number of available CPU cores.\n            unpack_arguments: Expect a list of tuples and unpack the tuples before\n                giving them to the wrapped function.\n            do_not_persist_traces: Don't save the traces in the database. Useful for\n                evaluations run part of the CI.\n        \"\"\"\n\n        wrapped_function = self._wrapped_func\n\n        def inner(value: Any) -&gt; T:\n            return (\n                wrapped_function(*value, do_not_persist_traces=do_not_persist_traces)\n                if unpack_arguments\n                else wrapped_function(\n                    value, do_not_persist_traces=do_not_persist_traces\n                )\n            )\n\n        async def inner_async(value: Any) -&gt; T:\n            return await cast(\n                Awaitable,\n                (\n                    wrapped_function(\n                        *value, do_not_persist_traces=do_not_persist_traces\n                    )\n                    if unpack_arguments\n                    else wrapped_function(\n                        value, do_not_persist_traces=do_not_persist_traces\n                    )\n                ),\n            )\n\n        return list(\n            tqdm(\n                parallel_map(\n                    inner_async\n                    if get_function_metadata_store(self).is_asynchronous\n                    else inner,\n                    batch,\n                    concurrency=concurrency,\n                ),\n                total=len(batch),\n            )\n        )\n\n    @staticmethod\n    def _get_cached_traced_function(\n        func: Callable[..., Union[V, Awaitable[V]]]\n    ) -&gt; Callable[..., T]:\n        @lru_cache(maxsize=get_context().prediction_cache_size)\n        def func_in_tracing_context_sync(\n            *args: Any,\n            do_not_persist_traces: bool = False,\n            **kwargs: Any,\n        ) -&gt; T:\n            with TracingContext[V](\n                func.__name__, do_not_persist_traces=do_not_persist_traces\n            ) as t:\n                result = func(*args, **kwargs)\n                return cast(T, t.finalise(output=result))\n\n        @alru_cache(maxsize=get_context().prediction_cache_size)\n        async def func_in_tracing_context_async(\n            *args: Any,\n            do_not_persist_traces: bool = False,\n            **kwargs: Any,\n        ) -&gt; T:\n            with TracingContext[V](\n                func.__name__, do_not_persist_traces=do_not_persist_traces\n            ) as t:\n                result = await cast(Callable[..., Awaitable], func)(*args, **kwargs)\n                return cast(T, t.finalise(output=result))\n\n        return cast(\n            Callable[..., T],\n            (\n                func_in_tracing_context_async\n                if get_function_metadata_store(func).is_asynchronous\n                else func_in_tracing_context_sync\n            ),\n        )\n\n    def _bootstrap_rest_api(\n        self,\n    ) -&gt; None:\n        route_config = get_context().route_config\n\n        if route_config.prediction_endpoint_enabled:\n            bootstrap_prediction_endpoint(self.app, self._wrapped_func)\n\n        if route_config.docs_endpoints_enabled:\n            bootstrap_docs_endpoints(self.app)\n\n        if route_config.dashboard_enabled:\n            bootstrap_dashboard(\n                self.app,\n                function_name=self.__name__,\n                documentation=self.__doc__,\n            )\n\n        if route_config.trace_endpoints_enabled:\n            bootstrap_trace_endpoints(self.app)\n\n        if route_config.feedback_endpoints_enabled:\n            bootstrap_feedback_endpoints(self.app)\n\n        if route_config.meta_endpoints_enabled:\n            bootstrap_meta_endpoints(\n                self.app,\n                self._cached_func,\n                ApiMetadata(\n                    name=self.__name__,\n                    version=self.version,\n                    documentation=self.__doc__,\n                    configuration=get_context().to_flat_dict(),\n                ),\n            )\n</code></pre>"},{"location":"reference/#great_ai.GreatAI.create","title":"<code>create(func)</code>  <code>staticmethod</code>","text":"<p>Decorate a function by wrapping it in a GreatAI instance.</p> <p>The function can be typed, synchronous or async. If it has unwrapped parameters (parameters not affected by a @parameter or @use_model decorator), those will be automatically wrapped.</p> <p>The return value is replaced by a Trace (or Awaitable[Trace]), while the original return value is available under the <code>.output</code> property.</p> <p>For configuration options, see great_ai.configure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @GreatAI.create\n... def my_function(a):\n...     return a + 2\n&gt;&gt;&gt; my_function(3).output\n5\n</code></pre> <pre><code>&gt;&gt;&gt; @GreatAI.create\n... def my_function(a: int) -&gt; int:\n...     return a + 2\n&gt;&gt;&gt; my_function(3)\nTrace[int]...\n</code></pre> <pre><code>&gt;&gt;&gt; my_function('3').output\nTraceback (most recent call last):\n    ...\nTypeError: type of a must be int; got str instead\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Union[Callable[..., Awaitable[V]], Callable[..., V]]</code> <p>The prediction function that needs to be decorated.</p> required <p>Returns:</p> Type Description <code>Union[GreatAI[Awaitable[Trace[V]], V], GreatAI[Trace[V], V]]</code> <p>A GreatAI instance wrapping <code>func</code>.</p> Source code in <code>great_ai/deploy/great_ai.py</code> <pre><code>@staticmethod\ndef create(\n    func: Union[Callable[..., Awaitable[V]], Callable[..., V]],\n) -&gt; Union[\"GreatAI[Awaitable[Trace[V]], V]\", \"GreatAI[Trace[V], V]\"]:\n    \"\"\"Decorate a function by wrapping it in a GreatAI instance.\n\n    The function can be typed, synchronous or async. If it has\n    unwrapped parameters (parameters not affected by a\n    [@parameter][great_ai.parameter] or [@use_model][great_ai.use_model] decorator),\n    those will be automatically wrapped.\n\n    The return value is replaced by a Trace (or Awaitable[Trace]),\n    while the original return value is available under the `.output`\n    property.\n\n    For configuration options, see [great_ai.configure][].\n\n    Examples:\n        &gt;&gt;&gt; @GreatAI.create\n        ... def my_function(a):\n        ...     return a + 2\n        &gt;&gt;&gt; my_function(3).output\n        5\n\n        &gt;&gt;&gt; @GreatAI.create\n        ... def my_function(a: int) -&gt; int:\n        ...     return a + 2\n        &gt;&gt;&gt; my_function(3)\n        Trace[int]...\n\n        &gt;&gt;&gt; my_function('3').output\n        Traceback (most recent call last):\n            ...\n        TypeError: type of a must be int; got str instead\n\n    Args:\n        func: The prediction function that needs to be decorated.\n\n    Returns:\n        A GreatAI instance wrapping `func`.\n    \"\"\"\n\n    return GreatAI[Trace[V], V](\n        func,\n    )\n</code></pre>"},{"location":"reference/#great_ai.GreatAI.process_batch","title":"<code>process_batch(batch, *, concurrency=None, unpack_arguments=False, do_not_persist_traces=False)</code>","text":"<p>Map the wrapped function over a list of input_values (<code>batch</code>).</p> <p>A wrapper over parallel_map providing type-safety and a progressbar through tqdm.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Sequence</code> <p>A list of arguments for the original (wrapped) function. If the function expects multiple arguments, provide a list of tuples and set <code>unpack_arguments=True</code>.</p> required <code>concurrency</code> <code>Optional[int]</code> <p>Number of processes to start. Don't set it too much higher than the number of available CPU cores.</p> <code>None</code> <code>unpack_arguments</code> <code>bool</code> <p>Expect a list of tuples and unpack the tuples before giving them to the wrapped function.</p> <code>False</code> <code>do_not_persist_traces</code> <code>bool</code> <p>Don't save the traces in the database. Useful for evaluations run part of the CI.</p> <code>False</code> Source code in <code>great_ai/deploy/great_ai.py</code> <pre><code>def process_batch(\n    self,\n    batch: Sequence,\n    *,\n    concurrency: Optional[int] = None,\n    unpack_arguments: bool = False,\n    do_not_persist_traces: bool = False,\n) -&gt; List[Trace[V]]:\n    \"\"\"Map the wrapped function over a list of input_values (`batch`).\n\n    A wrapper over [parallel_map][great_ai.utilities.parallel_map.parallel_map.parallel_map]\n    providing type-safety and a progressbar through tqdm.\n\n    Args:\n        batch: A list of arguments for the original (wrapped) function. If the\n            function expects multiple arguments, provide a list of tuples and set\n            `unpack_arguments=True`.\n        concurrency: Number of processes to start. Don't set it too much higher than\n            the number of available CPU cores.\n        unpack_arguments: Expect a list of tuples and unpack the tuples before\n            giving them to the wrapped function.\n        do_not_persist_traces: Don't save the traces in the database. Useful for\n            evaluations run part of the CI.\n    \"\"\"\n\n    wrapped_function = self._wrapped_func\n\n    def inner(value: Any) -&gt; T:\n        return (\n            wrapped_function(*value, do_not_persist_traces=do_not_persist_traces)\n            if unpack_arguments\n            else wrapped_function(\n                value, do_not_persist_traces=do_not_persist_traces\n            )\n        )\n\n    async def inner_async(value: Any) -&gt; T:\n        return await cast(\n            Awaitable,\n            (\n                wrapped_function(\n                    *value, do_not_persist_traces=do_not_persist_traces\n                )\n                if unpack_arguments\n                else wrapped_function(\n                    value, do_not_persist_traces=do_not_persist_traces\n                )\n            ),\n        )\n\n    return list(\n        tqdm(\n            parallel_map(\n                inner_async\n                if get_function_metadata_store(self).is_asynchronous\n                else inner,\n                batch,\n                concurrency=concurrency,\n            ),\n            total=len(batch),\n        )\n    )\n</code></pre>"},{"location":"reference/#great_ai.configure","title":"<code>configure(*, version='0.0.1', log_level=DEBUG, seed=42, tracing_database_factory=None, large_file_implementation=None, should_log_exception_stack=None, prediction_cache_size=512, disable_se4ml_banner=False, dashboard_table_size=50, route_config=RouteConfig())</code>","text":"<p>Set the global configuration used by the great-ai library.</p> <p>You must call <code>configure</code> before calling (or decorating with) any other great-ai function.</p> <p>If <code>tracing_database_factory</code> or <code>large_file_implementation</code> is not specified, their default value is determined based on which TracingDatabase and LargeFile has been configured (e.g.: LargeFileS3.configure_credentials_from_file('s3.ini')), or whether there is any file named s3.ini or mongo.ini in the working directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; configure(prediction_cache_size=0)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>Union[int, str]</code> <p>The version of your application (using SemVer is recommended).</p> <code>'0.0.1'</code> <code>log_level</code> <code>int</code> <p>Set the default logging level of <code>logging</code>.</p> <code>DEBUG</code> <code>seed</code> <code>int</code> <p>Set seed of <code>random</code> (and <code>numpy</code> if installed) for reproducibility.</p> <code>42</code> <code>tracing_database_factory</code> <code>Optional[Type[TracingDatabaseDriver]]</code> <p>Specify a different TracingDatabaseDriver than the one already configured.</p> <code>None</code> <code>large_file_implementation</code> <code>Optional[Type[LargeFileBase]]</code> <p>Specify a different LargeFile than the one already configured.</p> <code>None</code> <code>should_log_exception_stack</code> <code>Optional[bool]</code> <p>Log the traces of unhandled exceptions.</p> <code>None</code> <code>prediction_cache_size</code> <code>int</code> <p>Size of the LRU cache applied over the prediction functions.</p> <code>512</code> <code>disable_se4ml_banner</code> <code>bool</code> <p>Turn off the warning about the importance of SE4ML best- practices.</p> <code>False</code> <code>dashboard_table_size</code> <code>int</code> <p>Number of rows to display in the dashboard's table.</p> <code>50</code> <code>route_config</code> <code>RouteConfig</code> <p>Enable or disable specific HTTP API endpoints.</p> <code>RouteConfig()</code> Source code in <code>great_ai/context.py</code> <pre><code>def configure(\n    *,\n    version: Union[int, str] = \"0.0.1\",\n    log_level: int = DEBUG,\n    seed: int = 42,\n    tracing_database_factory: Optional[Type[TracingDatabaseDriver]] = None,\n    large_file_implementation: Optional[Type[LargeFileBase]] = None,\n    should_log_exception_stack: Optional[bool] = None,\n    prediction_cache_size: int = 512,\n    disable_se4ml_banner: bool = False,\n    dashboard_table_size: int = 50,\n    route_config: RouteConfig = RouteConfig(),\n) -&gt; None:\n    \"\"\"Set the global configuration used by the great-ai library.\n\n    You must call `configure` before calling (or decorating with) any other great-ai\n    function.\n\n    If `tracing_database_factory` or `large_file_implementation` is not specified, their\n    default value is determined based on which TracingDatabase and LargeFile has been\n    configured (e.g.: LargeFileS3.configure_credentials_from_file('s3.ini')), or whether\n    there is any file named s3.ini or mongo.ini in the working directory.\n\n    Examples:\n        &gt;&gt;&gt; configure(prediction_cache_size=0)\n\n    Arguments:\n        version: The version of your application (using SemVer is recommended).\n        log_level: Set the default logging level of `logging`.\n        seed: Set seed of `random` (and `numpy` if installed) for reproducibility.\n        tracing_database_factory: Specify a different TracingDatabaseDriver than the one\n            already configured.\n        large_file_implementation: Specify a different LargeFile than the one already\n            configured.\n        should_log_exception_stack: Log the traces of unhandled exceptions.\n        prediction_cache_size: Size of the LRU cache applied over the prediction\n            functions.\n        disable_se4ml_banner: Turn off the warning about the importance of SE4ML best-\n            practices.\n        dashboard_table_size: Number of rows to display in the dashboard's table.\n        route_config: Enable or disable specific HTTP API endpoints.\n    \"\"\"\n\n    global _context\n    logger = get_logger(\"great_ai\", level=log_level)\n\n    if _context is not None:\n        logger.error(\n            \"Configuration has been already initialised, overwriting.\\n\"\n            + \"Make sure to call `configure()` before importing your application code.\"\n        )\n\n    is_production = _is_in_production_mode(logger=logger)\n\n    _set_seed(seed)\n\n    tracing_database_factory = _initialize_tracing_database(\n        tracing_database_factory, logger=logger\n    )\n    tracing_database = tracing_database_factory()\n\n    if not tracing_database.is_production_ready:\n        message = f\"\"\"The selected tracing database ({\n            tracing_database_factory.__name__\n        }) is not recommended for production\"\"\"\n\n        if is_production:\n            logger.error(message)\n        else:\n            logger.warning(message)\n\n    _context = Context(\n        version=version,\n        tracing_database=tracing_database,\n        large_file_implementation=_initialize_large_file(\n            large_file_implementation, logger=logger\n        ),\n        is_production=is_production,\n        logger=logger,\n        should_log_exception_stack=not is_production\n        if should_log_exception_stack is None\n        else should_log_exception_stack,\n        prediction_cache_size=prediction_cache_size,\n        dashboard_table_size=dashboard_table_size,\n        route_config=route_config,\n    )\n\n    logger.info(f\"GreatAI (v{__version__}): configured \u2705\")\n    for k, v in get_context().to_flat_dict().items():\n        logger.info(f\"{LIST_ITEM_PREFIX}{k}: {v}\")\n\n    if not is_production and not disable_se4ml_banner:\n        logger.warning(\n            \"You still need to check whether you follow all best practices before \"\n            \"trusting your deployment.\"\n        )\n        logger.warning(f\"&gt; Find out more at {SE4ML_WEBSITE}\")\n</code></pre>"},{"location":"reference/#great_ai.save_model","title":"<code>save_model(model, key, *, keep_last_n=None)</code>","text":"<p>Save (and optionally serialise) a model in order to use by <code>use_model</code>.</p> <p>The <code>model</code> can be a Path or string representing a path in which case the local file/folder is read and saved using the current LargeFile implementation. In case <code>model</code> is an object, it is serialised using <code>dill</code> before uploading it.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from great_ai import use_model\n&gt;&gt;&gt; save_model(3, 'my_number')\n'my_number:...'\n</code></pre> <pre><code>&gt;&gt;&gt; @use_model('my_number')\n... def my_function(a, model):\n...     return a + model\n&gt;&gt;&gt; my_function(4)\n7\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[Path, str, object]</code> <p>The object or path to be uploaded.</p> required <code>key</code> <code>str</code> <p>The model's name.</p> required <code>keep_last_n</code> <code>Optional[int]</code> <p>If specified, remove old models and only keep the latest n. Directly passed to LargeFile.</p> <code>None</code> <p>Returns:     The key and version of the saved model separated by a colon. Example: \"key:version\"</p> Source code in <code>great_ai/models/save_model.py</code> <pre><code>def save_model(\n    model: Union[Path, str, object], key: str, *, keep_last_n: Optional[int] = None\n) -&gt; str:\n    \"\"\"Save (and optionally serialise) a model in order to use by `use_model`.\n\n    The `model` can be a Path or string representing a path in which case the\n    local file/folder is read and saved using the current LargeFile implementation.\n    In case `model` is an object, it is serialised using `dill` before uploading it.\n\n    Examples:\n            &gt;&gt;&gt; from great_ai import use_model\n            &gt;&gt;&gt; save_model(3, 'my_number')\n            'my_number:...'\n\n            &gt;&gt;&gt; @use_model('my_number')\n            ... def my_function(a, model):\n            ...     return a + model\n            &gt;&gt;&gt; my_function(4)\n            7\n\n    Args:\n        model: The object or path to be uploaded.\n        key: The model's name.\n        keep_last_n: If specified, remove old models and only keep the latest n. Directly passed to LargeFile.\n    Returns:\n        The key and version of the saved model separated by a colon. Example: \"key:version\"\n    \"\"\"\n    file = get_context().large_file_implementation(\n        name=key, mode=\"wb\", keep_last_n=keep_last_n\n    )\n\n    if isinstance(model, Path) or isinstance(model, str):\n        file.push(model)\n    else:\n        with file as f:\n            dump(model, f)\n\n    get_context().logger.info(f\"Model {key} uploaded with version {file.version}\")\n\n    return f\"{key}:{file.version}\"\n</code></pre>"},{"location":"reference/#great_ai.use_model","title":"<code>use_model(key, *, version='latest', model_kwarg_name='model')</code>","text":"<p>Inject a model into a function.</p> <p>Load a model specified by <code>key</code> and <code>version</code> using the currently active <code>LargeFile</code> implementation. If it's a single object, it is deserialised using <code>dill</code>. If it's a directory of files, a <code>pathlib.Path</code> instance is given.</p> <p>By default, the function's <code>model</code> parameter is replaced by the loaded model. This can be customised by changing <code>model_kwarg_name</code>. Multiple models can be loaded by decorating the same function with <code>use_model</code> multiple times.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from great_ai import save_model\n&gt;&gt;&gt; save_model(3, 'my_number')\n'my_number:...'\n&gt;&gt;&gt; @use_model('my_number')\n... def my_function(a, model):\n...     return a + model\n&gt;&gt;&gt; my_function(4)\n7\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The model's name as stored by the LargeFile implementation.</p> required <code>version</code> <code>Union[int, Literal['latest']]</code> <p>The model's version as stored by the LargeFile implementation.</p> <code>'latest'</code> <code>model_kwarg_name</code> <code>str</code> <p>the parameter to use for injecting the loaded model</p> <code>'model'</code> <p>Returns:     A decorator for model injection.</p> Source code in <code>great_ai/models/use_model.py</code> <pre><code>def use_model(\n    key: str,\n    *,\n    version: Union[int, Literal[\"latest\"]] = \"latest\",\n    model_kwarg_name: str = \"model\",\n) -&gt; Callable[[F], F]:\n    \"\"\"Inject a model into a function.\n\n    Load a model specified by `key` and `version` using the currently active `LargeFile`\n    implementation. If it's a single object, it is deserialised using `dill`. If it's a\n    directory of files, a `pathlib.Path` instance is given.\n\n    By default, the function's `model` parameter is replaced by the loaded model. This\n    can be customised by changing `model_kwarg_name`. Multiple models can be loaded by\n    decorating the same function with `use_model` multiple times.\n\n    Examples:\n            &gt;&gt;&gt; from great_ai import save_model\n            &gt;&gt;&gt; save_model(3, 'my_number')\n            'my_number:...'\n            &gt;&gt;&gt; @use_model('my_number')\n            ... def my_function(a, model):\n            ...     return a + model\n            &gt;&gt;&gt; my_function(4)\n            7\n\n    Args:\n        key: The model's name as stored by the LargeFile implementation.\n        version: The model's version as stored by the LargeFile implementation.\n        model_kwarg_name: the parameter to use for injecting the loaded model\n    Returns:\n        A decorator for model injection.\n    \"\"\"\n\n    assert (\n        isinstance(version, int) or version == \"latest\"\n    ), \"Only integers or the string literal `latest` is allowed as a version\"\n\n    model, actual_version = _load_model(\n        key=key,\n        version=None if version == \"latest\" else version,\n    )\n\n    def decorator(func: F) -&gt; F:\n        assert_function_is_not_finalised(func)\n\n        store = get_function_metadata_store(func)\n        store.model_parameter_names.append(model_kwarg_name)\n\n        @wraps(func)\n        def wrapper(*args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n            tracing_context = TracingContext.get_current_tracing_context()\n            if tracing_context:\n                tracing_context.log_model(Model(key=key, version=actual_version))\n            return func(*args, **kwargs, **{model_kwarg_name: model})\n\n        return cast(F, wrapper)\n\n    return decorator\n</code></pre>"},{"location":"reference/#great_ai.parameter","title":"<code>parameter(parameter_name, *, validate=lambda : True, disable_logging=False)</code>","text":"<p>Control the validation and logging of function parameters.</p> <p>Basically, a parameter decorator. Unfortunately, Python does not have that concept, thus, it's a method decorator that expects the name of the to-be-decorated parameter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @parameter('a')\n... def my_function(a: int):\n...     return a + 2\n&gt;&gt;&gt; my_function(4)\n6\n&gt;&gt;&gt; my_function('3')\nTraceback (most recent call last):\n    ...\nTypeError: type of a must be int; got str instead\n</code></pre> <pre><code>&gt;&gt;&gt; @parameter('positive_a', validate=lambda v: v &gt; 0)\n... def my_function(positive_a: int):\n...     return a + 2\n&gt;&gt;&gt; my_function(-1)\nTraceback (most recent call last):\n    ...\ngreat_ai.errors.argument_validation_error.ArgumentValidationError: ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>Name of parameter to consider.</p> required <code>validate</code> <code>Callable[[Any], bool]</code> <p>Optional validate to run against the concrete argument. ArgumentValidationError is thrown when the return value is False.</p> <code>lambda : True</code> <code>disable_logging</code> <code>bool</code> <p>Do not save the value in any active TracingContext.</p> <code>False</code> <p>Returns:     A decorator for argument validation.</p> Source code in <code>great_ai/parameters/parameter.py</code> <pre><code>def parameter(\n    parameter_name: str,\n    *,\n    validate: Callable[[Any], bool] = lambda _: True,\n    disable_logging: bool = False,\n) -&gt; Callable[[F], F]:\n    \"\"\"Control the validation and logging of function parameters.\n\n    Basically, a parameter decorator. Unfortunately, Python does not have that concept,\n    thus, it's a method decorator that expects the name of the to-be-decorated\n    parameter.\n\n    Examples:\n        &gt;&gt;&gt; @parameter('a')\n        ... def my_function(a: int):\n        ...     return a + 2\n        &gt;&gt;&gt; my_function(4)\n        6\n        &gt;&gt;&gt; my_function('3')\n        Traceback (most recent call last):\n            ...\n        TypeError: type of a must be int; got str instead\n\n        &gt;&gt;&gt; @parameter('positive_a', validate=lambda v: v &gt; 0)\n        ... def my_function(positive_a: int):\n        ...     return a + 2\n        &gt;&gt;&gt; my_function(-1)\n        Traceback (most recent call last):\n            ...\n        great_ai.errors.argument_validation_error.ArgumentValidationError: ...\n\n    Args:\n        parameter_name: Name of parameter to consider.\n        validate: Optional validate to run against the concrete argument.\n            ArgumentValidationError is thrown when the return value is False.\n        disable_logging: Do not save the value in any active TracingContext.\n    Returns:\n        A decorator for argument validation.\n    \"\"\"\n\n    def decorator(func: F) -&gt; F:\n        get_function_metadata_store(func).input_parameter_names.append(parameter_name)\n        assert_function_is_not_finalised(func)\n\n        actual_name = f\"arg:{parameter_name}\"\n\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Dict[str, Any]) -&gt; Any:\n            arguments = get_arguments(func, args, kwargs)\n            argument = arguments.get(parameter_name)\n\n            expected_type = func.__annotations__.get(parameter_name)\n\n            if expected_type is not None:\n                check_type(parameter_name, argument, expected_type)\n\n            if not validate(argument):\n                raise ArgumentValidationError(\n                    f\"\"\"Argument {parameter_name} in {\n                        func.__name__\n                    } did not pass validation\"\"\"\n                )\n\n            context = TracingContext.get_current_tracing_context()\n            if context and not disable_logging:\n                context.log_value(name=f\"{actual_name}:value\", value=argument)\n                if isinstance(argument, str):\n                    context.log_value(name=f\"{actual_name}:length\", value=len(argument))\n\n            return func(*args, **kwargs)\n\n        return cast(F, wrapper)\n\n    return decorator\n</code></pre>"},{"location":"reference/#great_ai.log_metric","title":"<code>log_metric(argument_name, value, disable_logging=False)</code>","text":"<p>Log a key (argument_name)-value pair that is persisted inside the trace.</p> <p>The name of the function from where this is called is also stored.</p> <p>Parameters:</p> Name Type Description Default <code>argument_name</code> <code>str</code> <p>The key for storing the value.</p> required <code>value</code> <code>Any</code> <p>Value to log. Must be JSON-serialisable.</p> required <code>disable_logging</code> <code>bool</code> <p>If True, only persist in trace but don't show in console</p> <code>False</code> Source code in <code>great_ai/parameters/log_metric.py</code> <pre><code>def log_metric(argument_name: str, value: Any, disable_logging: bool = False) -&gt; None:\n    \"\"\"Log a key (argument_name)-value pair that is persisted inside the trace.\n\n    The name of the function from where this is called is also stored.\n\n    Args:\n        argument_name: The key for storing the value.\n        value: Value to log. Must be JSON-serialisable.\n        disable_logging: If True, only persist in trace but don't show in console\n    \"\"\"\n\n    tracing_context = TracingContext.get_current_tracing_context()\n    try:\n        caller = inspect.stack()[1].function\n        actual_name = f\"metric:{caller}:{argument_name}\"\n    except:\n        # inspect might not work in notebooks\n        actual_name = f\"metric:{argument_name}\"\n\n    if tracing_context:\n        tracing_context.log_value(name=actual_name, value=value)\n\n    if not disable_logging:\n        get_context().logger.info(f\"{actual_name}={value}\")\n</code></pre>"},{"location":"reference/#remote-calls","title":"Remote calls","text":""},{"location":"reference/#great_ai.call_remote_great_ai","title":"<code>call_remote_great_ai(base_uri, data, retry_count=4, timeout_in_seconds=300, model_class=None)</code>","text":"<p>Communicate with a GreatAI object through an HTTP request.</p> <p>Wrapper over <code>call_remote_great_ai_async</code> making it synchronous. For more info, see <code>call_remote_great_ai_async</code>.</p> Source code in <code>great_ai/remote/call_remote_great_ai.py</code> <pre><code>def call_remote_great_ai(\n    base_uri: str,\n    data: Mapping[str, Any],\n    retry_count: int = 4,\n    timeout_in_seconds: Optional[int] = 300,\n    model_class: Optional[Type[T]] = None,\n) -&gt; Trace[T]:\n    \"\"\"Communicate with a GreatAI object through an HTTP request.\n\n    Wrapper over `call_remote_great_ai_async` making it synchronous. For more info, see\n    `call_remote_great_ai_async`.\n    \"\"\"\n    try:\n        asyncio.get_running_loop()\n        raise Exception(\n            f\"Already running in an event loop, you have to call `{call_remote_great_ai_async.__name__}`\"\n        )\n    except RuntimeError:\n        pass\n\n    future = call_remote_great_ai_async(\n        base_uri=base_uri,\n        data=data,\n        retry_count=retry_count,\n        timeout_in_seconds=timeout_in_seconds,\n        model_class=model_class,\n    )\n\n    return asyncio.run(future)\n</code></pre>"},{"location":"reference/#great_ai.call_remote_great_ai_async","title":"<code>call_remote_great_ai_async(base_uri, data, retry_count=4, timeout_in_seconds=300, model_class=None)</code>  <code>async</code>","text":"<p>Communicate with a GreatAI object through an HTTP request.</p> <p>Send a POST request using httpx to implement a remote call. Error-handling and retries are provided by httpx.</p> <p>The return value is inflated into a Trace. If <code>model_class</code> is specified, the original output is deserialised.</p> <p>Parameters:</p> Name Type Description Default <code>base_uri</code> <code>str</code> <p>Address of the remote instance, example: 'http://localhost:6060'</p> required <code>data</code> <code>Mapping[str, Any]</code> <p>The input sent as a json to the remote instance.</p> required <code>retry_count</code> <code>int</code> <p>Retry on any HTTP communication failure.</p> <code>4</code> <code>timeout_in_seconds</code> <code>Optional[int]</code> <p>Overall permissible max length of the request. <code>None</code> means no timeout.</p> <code>300</code> <code>model_class</code> <code>Optional[Type[T]]</code> <p>A subtype of BaseModel to be used for deserialising the <code>.output</code> of the trace.</p> <code>None</code> Source code in <code>great_ai/remote/call_remote_great_ai_async.py</code> <pre><code>async def call_remote_great_ai_async(\n    base_uri: str,\n    data: Mapping[str, Any],\n    retry_count: int = 4,\n    timeout_in_seconds: Optional[int] = 300,\n    model_class: Optional[Type[T]] = None,\n) -&gt; Trace[T]:\n    \"\"\"Communicate with a GreatAI object through an HTTP request.\n\n    Send a POST request using [httpx](https://www.python-httpx.org/) to implement a\n    remote call. Error-handling and retries are provided by httpx.\n\n    The return value is inflated into a Trace. If `model_class` is specified, the\n    original output is deserialised.\n\n    Args:\n        base_uri: Address of the remote instance, example: 'http://localhost:6060'\n        data: The input sent as a json to the remote instance.\n        retry_count: Retry on any HTTP communication failure.\n        timeout_in_seconds: Overall permissible max length of the request. `None` means\n            no timeout.\n        model_class: A subtype of BaseModel to be used for deserialising the `.output`\n            of the trace.\n    \"\"\"\n\n    if base_uri.endswith(\"/\"):\n        base_uri = base_uri[:-1]\n\n    if not base_uri.endswith(\"/predict\"):\n        base_uri = f\"{base_uri}/predict\"\n\n    transport = httpx.AsyncHTTPTransport(retries=retry_count)\n\n    try:\n        async with httpx.AsyncClient(\n            transport=transport, timeout=timeout_in_seconds\n        ) as client:\n            response = await client.post(base_uri, json=data)\n            try:\n                response.raise_for_status()\n            except Exception:\n                raise RemoteCallError(\n                    f\"Unexpected status code, reason: {response.text}\"\n                )\n    except Exception as e:\n        raise RemoteCallError from e\n\n    try:\n        trace = response.json()\n    except Exception:\n        raise RemoteCallError(\n            f\"JSON parsing failed {response.text}\",\n        )\n    try:\n        if model_class is not None:\n            trace[\"output\"] = model_class.parse_obj(trace[\"output\"])\n        return Trace.parse_obj(trace)\n    except Exception:\n        raise RemoteCallError(\"Could not parse Trace\")\n</code></pre>"},{"location":"reference/#ground-truth","title":"Ground-truth","text":""},{"location":"reference/#great_ai.add_ground_truth","title":"<code>add_ground_truth(inputs, expected_outputs, *, tags=[], train_split_ratio=1, test_split_ratio=0, validation_split_ratio=0)</code>","text":"<p>Add training data (with optional train-test splitting).</p> <p>Add and tag data-points, wrap them into traces. The <code>inputs</code> are available via the <code>.input</code> property, while <code>expected_outputs</code> under both the <code>.output</code> and <code>.feedback</code> properties.</p> <p>All generated traces are tagged with <code>ground_truth</code> by default. Additional tags can be also provided. Using the <code>split_ratio</code> arguments, tags can be given randomly with a user-defined distribution. Only the ratio of the splits matter, they don't have to add up to 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; add_ground_truth(\n...    [1, 2, 3],\n...    ['odd', 'even', 'odd'],\n...    tags='my_tag',\n...    train_split_ratio=1,\n...    test_split_ratio=1,\n...    validation_split_ratio=0.5,\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; add_ground_truth(\n...    [1, 2],\n...    ['odd', 'even', 'odd'],\n...    tags='my_tag',\n...    train_split_ratio=1,\n...    test_split_ratio=1,\n...    validation_split_ratio=0.5,\n... )\nTraceback (most recent call last):\n    ...\nAssertionError: The length of the inputs and expected_outputs must be equal\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Iterable[Any]</code> <p>The inputs. (X in scikit-learn)</p> required <code>expected_outputs</code> <code>Iterable[T]</code> <p>The ground-truth values corresponding to the inputs. (y in scikit-learn)</p> required <code>tags</code> <code>Union[List[str], str]</code> <p>A single tag or a list of tags to append to each generated trace's tags.</p> <code>[]</code> <code>train_split_ratio</code> <code>float</code> <p>The probability-weight of giving each trace the <code>train</code> tag.</p> <code>1</code> <code>test_split_ratio</code> <code>float</code> <p>The probability-weight of giving each trace the <code>test</code> tag.</p> <code>0</code> <code>validation_split_ratio</code> <code>float</code> <p>The probability-weight of giving each trace the <code>validation</code> tag.</p> <code>0</code> Source code in <code>great_ai/tracing/add_ground_truth.py</code> <pre><code>def add_ground_truth(\n    inputs: Iterable[Any],\n    expected_outputs: Iterable[T],\n    *,\n    tags: Union[List[str], str] = [],\n    train_split_ratio: float = 1,\n    test_split_ratio: float = 0,\n    validation_split_ratio: float = 0\n) -&gt; None:\n    \"\"\"Add training data (with optional train-test splitting).\n\n    Add and tag data-points, wrap them into traces. The `inputs` are available via the\n    `.input` property, while `expected_outputs` under both the `.output` and `.feedback`\n    properties.\n\n    All generated traces are tagged with `ground_truth` by default. Additional tags can\n    be also provided. Using the `split_ratio` arguments, tags can be given randomly with\n    a user-defined distribution. Only the ratio of the splits matter, they don't have to\n    add up to 1.\n\n    Examples:\n        &gt;&gt;&gt; add_ground_truth(\n        ...    [1, 2, 3],\n        ...    ['odd', 'even', 'odd'],\n        ...    tags='my_tag',\n        ...    train_split_ratio=1,\n        ...    test_split_ratio=1,\n        ...    validation_split_ratio=0.5,\n        ... )\n\n        &gt;&gt;&gt; add_ground_truth(\n        ...    [1, 2],\n        ...    ['odd', 'even', 'odd'],\n        ...    tags='my_tag',\n        ...    train_split_ratio=1,\n        ...    test_split_ratio=1,\n        ...    validation_split_ratio=0.5,\n        ... )\n        Traceback (most recent call last):\n            ...\n        AssertionError: The length of the inputs and expected_outputs must be equal\n\n    Args:\n        inputs: The inputs. (X in scikit-learn)\n        expected_outputs: The ground-truth values corresponding to the inputs. (y in\n            scikit-learn)\n        tags: A single tag or a list of tags to append to each generated trace's tags.\n        train_split_ratio: The probability-weight of giving each trace the `train` tag.\n        test_split_ratio: The probability-weight of giving each trace the `test` tag.\n        validation_split_ratio: The probability-weight of giving each trace the\n            `validation` tag.\n    \"\"\"\n\n    inputs = list(inputs)\n    expected_outputs = list(expected_outputs)\n    assert len(inputs) == len(\n        expected_outputs\n    ), \"The length of the inputs and expected_outputs must be equal\"\n\n    tags = tags if isinstance(tags, list) else [tags]\n\n    sum_ratio = train_split_ratio + test_split_ratio + validation_split_ratio\n    assert sum_ratio &gt; 0, \"The sum of the split ratios must be a positive number\"\n\n    train_split_ratio /= sum_ratio\n    test_split_ratio /= sum_ratio\n    validation_split_ratio /= sum_ratio\n\n    values = list(zip(inputs, expected_outputs))\n    shuffle(values)\n\n    split_tags = (\n        [TRAIN_SPLIT_TAG_NAME] * ceil(train_split_ratio * len(inputs))\n        + [TEST_SPLIT_TAG_NAME] * ceil(test_split_ratio * len(inputs))\n        + [VALIDATION_SPLIT_TAG_NAME] * ceil(validation_split_ratio * len(inputs))\n    )\n    shuffle(split_tags)\n\n    created = datetime.utcnow().isoformat()\n    traces = [\n        cast(\n            Trace[T],\n            Trace(  # avoid ValueError: \"Trace\" object has no field \"__orig_class__\"\n                trace_id=str(uuid4()),\n                created=created,\n                original_execution_time_ms=0,\n                logged_values=X if isinstance(X, dict) else {\"input\": X},\n                models=[],\n                output=y,\n                feedback=y,\n                exception=None,\n                tags=[GROUND_TRUTH_TAG_NAME, split_tag, *tags],\n            ),\n        )\n        for ((X, y), split_tag) in zip(values, split_tags)\n    ]\n\n    get_context().tracing_database.save_batch(traces)\n</code></pre>"},{"location":"reference/#great_ai.query_ground_truth","title":"<code>query_ground_truth(conjunctive_tags=[], *, since=None, until=None, return_max_count=None)</code>","text":"<p>Return training samples.</p> <p>Combines, filters, and returns data-points that have been either added by <code>add_ground_truth</code> or were the result of a prediction after which their trace got feedback through the RESP API-s <code>/traces/{trace_id}/feedback</code> endpoint (end-to-end feedback).</p> <p>Filtering can be used to only return points matching all given tags (or the single given tag) and by time of creation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; query_ground_truth()\n[...]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>conjunctive_tags</code> <code>Union[List[str], str]</code> <p>Single tag or a list of tags which the returned traces have to match. The relationship between the tags is conjunctive (AND).</p> <code>[]</code> <code>since</code> <code>Optional[datetime]</code> <p>Only return traces created after the given timestamp. <code>None</code> means no filtering.</p> <code>None</code> <code>until</code> <code>Optional[datetime]</code> <p>Only return traces created before the given timestamp. <code>None</code> means no filtering.</p> <code>None</code> <code>return_max_count</code> <code>Optional[int]</code> <p>Return at-most this many traces. (take, limit)</p> <code>None</code> Source code in <code>great_ai/tracing/query_ground_truth.py</code> <pre><code>def query_ground_truth(\n    conjunctive_tags: Union[List[str], str] = [],\n    *,\n    since: Optional[datetime] = None,\n    until: Optional[datetime] = None,\n    return_max_count: Optional[int] = None\n) -&gt; List[Trace]:\n    \"\"\"Return training samples.\n\n    Combines, filters, and returns data-points that have been either added by\n    `add_ground_truth` or were the result of a prediction after which their trace got\n    feedback through the RESP API-s `/traces/{trace_id}/feedback` endpoint\n    (end-to-end feedback).\n\n    Filtering can be used to only return points matching all given tags (or the single\n    given tag) and by time of creation.\n\n    Examples:\n        &gt;&gt;&gt; query_ground_truth()\n        [...]\n\n    Args:\n        conjunctive_tags: Single tag or a list of tags which the returned traces have to\n            match. The relationship between the tags is conjunctive (AND).\n        since: Only return traces created after the given timestamp. `None` means no\n            filtering.\n        until: Only return traces created before the given timestamp. `None` means no\n            filtering.\n        return_max_count: Return at-most this many traces. (take, limit)\n    \"\"\"\n\n    tags = (\n        conjunctive_tags if isinstance(conjunctive_tags, list) else [conjunctive_tags]\n    )\n    db = get_context().tracing_database\n\n    items, length = db.query(\n        conjunctive_tags=tags,\n        since=since,\n        until=until,\n        take=return_max_count,\n        has_feedback=True,\n    )\n    return items\n</code></pre>"},{"location":"reference/#great_ai.delete_ground_truth","title":"<code>delete_ground_truth(conjunctive_tags=[], *, since=None, until=None)</code>","text":"<p>Delete traces matching the given criteria.</p> <p>Takes the same arguments as <code>query_ground_truth</code> but instead of returning them, it simply deletes them.</p> <p>You can rely on the efficiency of the delete's implementation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; delete_ground_truth(['train', 'test', 'validation'])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>conjunctive_tags</code> <code>Union[List[str], str]</code> <p>Single tag or a list of tags which the deleted traces have to match. The relationship between the tags is conjunctive (AND).</p> <code>[]</code> <code>since</code> <code>Optional[datetime]</code> <p>Only delete traces created after the given timestamp. <code>None</code> means no filtering.</p> <code>None</code> <code>until</code> <code>Optional[datetime]</code> <p>Only delete traces created before the given timestamp. <code>None</code> means no filtering.</p> <code>None</code> Source code in <code>great_ai/tracing/delete_ground_truth.py</code> <pre><code>def delete_ground_truth(\n    conjunctive_tags: Union[List[str], str] = [],\n    *,\n    since: Optional[datetime] = None,\n    until: Optional[datetime] = None,\n) -&gt; None:\n    \"\"\"Delete traces matching the given criteria.\n\n    Takes the same arguments as `query_ground_truth` but instead of returning them,\n    it simply deletes them.\n\n    You can rely on the efficiency of the delete's implementation.\n\n    Examples:\n        &gt;&gt;&gt; delete_ground_truth(['train', 'test', 'validation'])\n\n    Args:\n        conjunctive_tags: Single tag or a list of tags which the deleted traces have to\n            match. The relationship between the tags is conjunctive (AND).\n        since: Only delete traces created after the given timestamp. `None` means no\n            filtering.\n        until: Only delete traces created before the given timestamp. `None` means no\n            filtering.\n    \"\"\"\n\n    tags = (\n        conjunctive_tags if isinstance(conjunctive_tags, list) else [conjunctive_tags]\n    )\n    db = get_context().tracing_database\n\n    items, length = db.query(\n        conjunctive_tags=tags, until=until, since=since, has_feedback=True\n    )\n\n    db.delete_batch([i.trace_id for i in items])\n</code></pre>"},{"location":"reference/#tracing-databases","title":"Tracing databases","text":""},{"location":"reference/#great_ai.TracingDatabaseDriver","title":"<code>TracingDatabaseDriver</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Interface expected from a database to be used for storing traces.</p> Source code in <code>great_ai/persistence/tracing_database_driver.py</code> <pre><code>class TracingDatabaseDriver(ABC):\n    \"\"\"Interface expected from a database to be used for storing traces.\"\"\"\n\n    is_production_ready: bool\n    initialized: bool = False\n\n    @classmethod\n    def configure_credentials_from_file(\n        cls,\n        secrets: Union[Path, str, ConfigFile],\n    ) -&gt; None:\n        if not isinstance(secrets, ConfigFile):\n            secrets = ConfigFile(secrets)\n        cls.configure_credentials(**{k.lower(): v for k, v in secrets.items()})\n\n    @classmethod\n    def configure_credentials(\n        cls,\n    ) -&gt; None:\n        cls.initialized = True\n\n    @abstractmethod\n    def save(self, document: Trace) -&gt; str:\n        pass\n\n    @abstractmethod\n    def save_batch(\n        self,\n        documents: List[Trace],\n    ) -&gt; List[str]:\n        pass\n\n    @abstractmethod\n    def get(self, id: str) -&gt; Optional[Trace]:\n        pass\n\n    @abstractmethod\n    def query(\n        self,\n        *,\n        skip: int = 0,\n        take: Optional[int] = None,\n        conjunctive_filters: Sequence[Filter] = [],\n        conjunctive_tags: Sequence[str] = [],\n        until: Optional[datetime] = None,\n        since: Optional[datetime] = None,\n        has_feedback: Optional[bool] = None,\n        sort_by: Sequence[SortBy] = []\n    ) -&gt; Tuple[List[Trace], int]:\n        pass\n\n    @abstractmethod\n    def update(self, id: str, new_version: Trace) -&gt; None:\n        pass\n\n    @abstractmethod\n    def delete(self, id: str) -&gt; None:\n        pass\n\n    @abstractmethod\n    def delete_batch(\n        self,\n        ids: List[str],\n    ) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/#great_ai.MongoDbDriver","title":"<code>MongoDbDriver</code>","text":"<p>             Bases: <code>TracingDatabaseDriver</code></p> <p>TracingDatabaseDriver implementation using MongoDB as a backend.</p> <p>A production-ready database driver suitable for efficiently handling semi-structured data.</p> <p>Checkout MongoDB Atlas for a hosted MongoDB solution.</p> Source code in <code>great_ai/persistence/mongodb_driver.py</code> <pre><code>class MongoDbDriver(TracingDatabaseDriver):\n    \"\"\"TracingDatabaseDriver implementation using MongoDB as a backend.\n\n    A production-ready database driver suitable for efficiently handling semi-structured\n    data.\n\n    Checkout [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/register) for a hosted\n    MongoDB solution.\n    \"\"\"\n\n    is_production_ready = True\n\n    mongo_connection_string: str\n    mongo_database: str\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        if self.mongo_connection_string is None or self.mongo_database is None:\n            raise ValueError(\n                \"Please configure the MongoDB access options by calling \"\n                \"MongoDbDriver.configure_credentials\"\n            )\n\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            client[self.mongo_database].traces.create_index(\n                [(\"tags\", ASCENDING), (\"created\", DESCENDING)], background=True\n            )\n\n    @classmethod\n    def configure_credentials(  # type: ignore\n        cls,\n        *,\n        mongo_connection_string: str,\n        mongo_database: str,\n        **_: Any,\n    ) -&gt; None:\n        \"\"\"Configure the connection details for MongoDB.\n\n        Args:\n            mongo_connection_string: For example:\n                'mongodb://my_user:my_pass@localhost:27017'\n            mongo_database: Name of the database to use. If doesn't exist, it is\n                created and initialised.\n        \"\"\"\n        cls.mongo_connection_string = mongo_connection_string\n        cls.mongo_database = mongo_database\n        super().configure_credentials()\n\n    def save(self, trace: Trace) -&gt; str:\n        serialized = trace.to_flat_dict()\n        serialized[\"_id\"] = trace.trace_id\n\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            return client[self.mongo_database].traces.insert_one(serialized).inserted_id\n\n    def save_batch(self, documents: List[Trace]) -&gt; List[str]:\n        serialized = [d.to_flat_dict() for d in documents]\n        for s in serialized:\n            s[\"_id\"] = s[\"trace_id\"]\n\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            return (\n                client[self.mongo_database]\n                .traces.insert_many(serialized, ordered=False)\n                .inserted_ids\n            )\n\n    def get(self, id: str) -&gt; Optional[Trace]:\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            value = client[self.mongo_database].traces.find_one(id)\n\n        if value:\n            value = Trace.parse_obj(value)\n\n        return value\n\n    def _get_operator(self, filter: Filter) -&gt; str:\n        if filter.operator == \"contains\" and not isinstance(filter.value, str):\n            return operator_mapping[\"=\"]\n        return operator_mapping[filter.operator]\n\n    def query(\n        self,\n        *,\n        skip: int = 0,\n        take: Optional[int] = None,\n        conjunctive_filters: Sequence[Filter] = [],\n        conjunctive_tags: Sequence[str] = [],\n        since: Optional[datetime] = None,\n        until: Optional[datetime] = None,\n        has_feedback: Optional[bool] = None,\n        sort_by: Sequence[SortBy] = [],\n    ) -&gt; Tuple[List[Trace], int]:\n\n        query: Dict[str, Any] = {\n            \"filter\": {},\n        }\n\n        and_query: List[Dict[str, Any]] = []\n        and_query.extend({\"tags\": tag} for tag in conjunctive_tags)\n        and_query.extend(\n            {f.property: {self._get_operator(f): f.value}} for f in conjunctive_filters\n        )\n        if not and_query:\n            and_query.append({})\n\n        if since:\n            and_query.append({\"created\": {\"$gte\": since}})\n\n        if until:\n            and_query.append({\"created\": {\"$lte\": until}})\n\n        if has_feedback is not None:\n            and_query.append(\n                {\"feedback\": {\"$ne\": None}} if has_feedback else {\"feedback\": None}\n            )\n        query[\"filter\"][\"$and\"] = and_query\n\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            count = client[self.mongo_database].traces.count_documents(**query)\n\n            if skip:\n                query[\"skip\"] = skip\n\n            if take:\n                query[\"limit\"] = take\n\n            query[\"sort\"] = [\n                (col.column_id, 1 if col.direction == \"asc\" else -1) for col in sort_by\n            ]\n\n            with client[self.mongo_database].traces.find(**query) as cursor:\n                documents = [Trace[Any].parse_obj(t) for t in cursor]\n        return documents, count\n\n    def update(self, id: str, new_version: Trace) -&gt; None:\n        serialized = new_version.to_flat_dict()\n        serialized[\"_id\"] = new_version.trace_id\n\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            client[self.mongo_database].traces.update_one({\"_id\": id}, serialized)\n\n    def delete(self, id: str) -&gt; None:\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            client[self.mongo_database].traces.delete_one({\"_id\": id})\n\n    def delete_batch(self, ids: List[str]) -&gt; None:\n        with MongoClient[Any](self.mongo_connection_string) as client:\n            for c in chunk(\n                ids, chunk_size=10000\n            ):  # avoid: 'delete' command document too large\n                delete_filter = {\"_id\": {\"$in\": c}}\n                client[self.mongo_database].traces.delete_many(delete_filter)\n</code></pre>"},{"location":"reference/#great_ai.MongoDbDriver.configure_credentials","title":"<code>configure_credentials(*, mongo_connection_string, mongo_database, **_)</code>  <code>classmethod</code>","text":"<p>Configure the connection details for MongoDB.</p> <p>Parameters:</p> Name Type Description Default <code>mongo_connection_string</code> <code>str</code> <p>For example: 'mongodb://my_user:my_pass@localhost:27017'</p> required <code>mongo_database</code> <code>str</code> <p>Name of the database to use. If doesn't exist, it is created and initialised.</p> required Source code in <code>great_ai/persistence/mongodb_driver.py</code> <pre><code>@classmethod\ndef configure_credentials(  # type: ignore\n    cls,\n    *,\n    mongo_connection_string: str,\n    mongo_database: str,\n    **_: Any,\n) -&gt; None:\n    \"\"\"Configure the connection details for MongoDB.\n\n    Args:\n        mongo_connection_string: For example:\n            'mongodb://my_user:my_pass@localhost:27017'\n        mongo_database: Name of the database to use. If doesn't exist, it is\n            created and initialised.\n    \"\"\"\n    cls.mongo_connection_string = mongo_connection_string\n    cls.mongo_database = mongo_database\n    super().configure_credentials()\n</code></pre>"},{"location":"reference/#great_ai.ParallelTinyDbDriver","title":"<code>ParallelTinyDbDriver</code>","text":"<p>             Bases: <code>TracingDatabaseDriver</code></p> <p>TracingDatabaseDriver with TinyDB as a backend.</p> <p>Saves the database as a JSON into a single file. Highly inefficient on inserting, not advised for production use.</p> <p>A multiprocessing lock protects the database file to avoid parallelisation issues.</p> Source code in <code>great_ai/persistence/parallel_tinydb_driver.py</code> <pre><code>class ParallelTinyDbDriver(TracingDatabaseDriver):\n    \"\"\"TracingDatabaseDriver with TinyDB as a backend.\n\n    Saves the database as a JSON into a single file. Highly inefficient on inserting,\n    not advised for production use.\n\n    A multiprocessing lock protects the database file to avoid parallelisation issues.\n    \"\"\"\n\n    is_production_ready = False\n    path_to_db = Path(DEFAULT_TRACING_DB_FILENAME)\n\n    def save(self, trace: Trace) -&gt; str:\n        return self._safe_execute(lambda db: db.insert(trace.dict()))\n\n    def save_batch(self, documents: List[Trace]) -&gt; List[str]:\n        traces = [d.dict() for d in documents]\n        return self._safe_execute(lambda db: db.insert_multiple(traces))\n\n    def get(self, id: str) -&gt; Optional[Trace]:\n        value = self._safe_execute(lambda db: db.get(lambda d: d[\"trace_id\"] == id))\n        if value:\n            value = Trace.parse_obj(value)\n        return value\n\n    def query(\n        self,\n        *,\n        skip: int = 0,\n        take: Optional[int] = None,\n        conjunctive_filters: Sequence[Filter] = [],\n        conjunctive_tags: Sequence[str] = [],\n        since: Optional[datetime] = None,\n        until: Optional[datetime] = None,\n        has_feedback: Optional[bool] = None,\n        sort_by: Sequence[SortBy] = []\n    ) -&gt; Tuple[List[Trace], int]:\n        def does_match(d: Dict[str, Any]) -&gt; bool:\n            return (\n                not set(conjunctive_tags) - set(d[\"tags\"])\n                and (since is None or datetime.fromisoformat(d[\"created\"]) &gt;= since)\n                and (until is None or datetime.fromisoformat(d[\"created\"]) &lt;= until)\n                and (\n                    has_feedback is None or has_feedback == (d[\"feedback\"] is not None)\n                )\n            )\n\n        documents = self._safe_execute(lambda db: db.search(does_match))\n        if not documents:\n            return [], 0\n\n        df = pd.DataFrame([Trace.parse_obj(d).to_flat_dict() for d in documents])\n\n        for f in conjunctive_filters:\n            operator = f.operator.lower()\n            if operator in operator_mapping:\n                df = df.loc[\n                    getattr(df[f.property], operator_mapping[f.operator])(f.value)\n                ]\n            elif operator == \"contains\":\n                df = df.loc[\n                    df[f.property].str.contains(\n                        str(int(f.value)) if isinstance(f.value, float) else f.value,\n                        case=False,\n                    )\n                ]\n\n        if sort_by:\n            df.sort_values(\n                [col.column_id for col in sort_by],\n                ascending=[col.direction == \"asc\" for col in sort_by],\n                inplace=True,\n            )\n\n        count = len(df)\n        result = df.iloc[skip:] if take is None else df.iloc[skip : skip + take]\n        return [Trace.parse_obj(trace) for _, trace in result.iterrows()], count\n\n    def update(self, id: str, new_version: Trace) -&gt; None:\n        self._safe_execute(\n            lambda db: db.update(new_version.dict(), lambda d: d[\"trace_id\"] == id)\n        )\n\n    def delete(self, id: str) -&gt; None:\n        self._safe_execute(lambda db: db.remove(lambda d: d[\"trace_id\"] == id))\n\n    def delete_batch(self, ids: List[str]) -&gt; None:\n        with lock:\n            with TinyDB(self.path_to_db) as db:\n                for id in ids:\n                    db.remove(lambda d: d[\"trace_id\"] == id)\n\n    def _safe_execute(self, func: Callable[[TinyDB], Any]) -&gt; Any:\n        with lock:\n            with TinyDB(self.path_to_db) as db:\n                return func(db)\n</code></pre>"},{"location":"reference/large-file/","title":"LargeFile","text":"<pre><code>from great_ai.large_file import *\n</code></pre> <p>For more details about using LargeFiles, check out the how to guide.</p>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileLocal","title":"<code>LargeFileLocal</code>","text":"<p>             Bases: <code>LargeFileBase</code></p> <p>LargeFile implementation using local filesystem as a backend.</p> <p>Store large files remotely using the familiar API of <code>open()</code>. With built-in versioning, pruning and local cache.</p> <p>IMPORTANT: If LargeFileLocal.max_cache_size is too small, it won't be enough to store all your files and they can end up deleted.</p> <p>See parent for more details.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; LargeFileLocal.cache_path = Path(\".cache\")\n</code></pre> <pre><code>&gt;&gt;&gt; LargeFileLocal.max_cache_size = \"30GB\"\n</code></pre> <pre><code>&gt;&gt;&gt; with LargeFileLocal(\"my_test.txt\", \"w\", keep_last_n=2) as f:\n...     f.write('test')\n4\n</code></pre> <pre><code>&gt;&gt;&gt; with LargeFileLocal(\"my_test.txt\") as f:\n...     print(f.read())\ntest\n</code></pre> Source code in <code>great_ai/large_file/large_file/large_file_local.py</code> <pre><code>class LargeFileLocal(LargeFileBase):\n    \"\"\"LargeFile implementation using local filesystem as a backend.\n\n    Store large files remotely using the familiar API of `open()`. With built-in\n    versioning, pruning and local cache.\n\n    IMPORTANT: If LargeFileLocal.max_cache_size is too small, it won't be enough to\n    store all your files and they can end up deleted.\n\n    See parent for more details.\n\n    Examples:\n        &gt;&gt;&gt; LargeFileLocal.cache_path = Path(\".cache\")\n\n        &gt;&gt;&gt; LargeFileLocal.max_cache_size = \"30GB\"\n\n        &gt;&gt;&gt; with LargeFileLocal(\"my_test.txt\", \"w\", keep_last_n=2) as f:\n        ...     f.write('test')\n        4\n\n        &gt;&gt;&gt; with LargeFileLocal(\"my_test.txt\") as f:\n        ...     print(f.read())\n        test\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        mode: str = \"r\",\n        *,\n        buffering: int = -1,\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n        version: Optional[int] = None,\n        keep_last_n: Optional[int] = None,\n    ):\n        super().__init__(\n            name,\n            mode,\n            buffering=buffering,\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n            version=version,\n            keep_last_n=keep_last_n,\n            cache_only_mode=True,\n        )\n        super().configure_credentials()\n\n    def _find_remote_instances(self) -&gt; List[DataInstance]:\n        return []\n\n    def _download(\n        self, remote_path: Any, local_path: Path, hide_progress: bool\n    ) -&gt; None:\n        # This will never be called because the file must be in the cache\n        raise NotImplementedError()\n\n    def _upload(self, local_path: Path, hide_progress: bool) -&gt; None:\n        pass  # the \"upload\" is already done py the parent's caching mechanism\n\n    def _delete_old_remote_versions(self) -&gt; None:\n        if self._keep_last_n is not None:\n            for i in (\n                self._instances[: -self._keep_last_n]\n                if self._keep_last_n &gt; 0\n                else self._instances\n            ):\n                logger.info(\n                    f\"Removing old version (keep_last_n={self._keep_last_n}): {i.remote_path}\"\n                )\n                i.remote_path.unlink()\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileS3","title":"<code>LargeFileS3</code>","text":"<p>             Bases: <code>LargeFileBase</code></p> <p>LargeFile implementation using S3-compatible storage as a backend.</p> <p>Store large files remotely using the familiar API of <code>open()</code>. With built-in versioning, pruning and local cache.</p> <p>See parent for more details.</p> Source code in <code>great_ai/large_file/large_file/large_file_s3.py</code> <pre><code>class LargeFileS3(LargeFileBase):\n    \"\"\"LargeFile implementation using S3-compatible storage as a backend.\n\n    Store large files remotely using the familiar API of `open()`. With built-in\n    versioning, pruning and local cache.\n\n    See parent for more details.\n    \"\"\"\n\n    region_name = None\n    access_key_id = None\n    secret_access_key = None\n    bucket_name = None\n    endpoint_url = None\n\n    @classmethod\n    def configure_credentials(  # type: ignore\n        cls,\n        *,\n        aws_region_name: str,\n        aws_access_key_id: str,\n        aws_secret_access_key: str,\n        large_files_bucket_name: str,\n        aws_endpoint_url: Optional[str] = None,\n        **_: Any,\n    ) -&gt; None:\n        cls.region_name = aws_region_name\n        cls.access_key_id = aws_access_key_id\n        cls.secret_access_key = aws_secret_access_key\n        cls.bucket_name = large_files_bucket_name\n        cls.endpoint_url = aws_endpoint_url\n        super().configure_credentials()\n\n    @cached_property\n    def _client(self) -&gt; boto3.client:\n        if (\n            self.region_name is None\n            or self.access_key_id is None\n            or self.secret_access_key is None\n            or self.bucket_name is None\n        ):\n            raise ValueError(\n                \"Please configure the S3 access options by calling LargeFileS3.configure_credentials or set cache_only_mode=True in the constructor.\"\n            )\n\n        return boto3.client(\n            \"s3\",\n            aws_access_key_id=self.access_key_id,\n            aws_secret_access_key=self.secret_access_key,\n            region_name=self.region_name,\n            endpoint_url=self.endpoint_url,\n        )\n\n    def _find_remote_instances(self) -&gt; List[DataInstance]:\n        logger.debug(f\"Fetching S3 versions of {self._name}\")\n\n        found_objects = self._client.list_objects_v2(\n            Bucket=self.bucket_name, Prefix=self._name\n        )\n        return (\n            [\n                DataInstance(\n                    name=o[\"Key\"].split(S3_NAME_VERSION_SEPARATOR)[0],\n                    version=int(o[\"Key\"].split(S3_NAME_VERSION_SEPARATOR)[-1]),\n                    remote_path=o[\"Key\"],\n                )\n                for o in found_objects[\"Contents\"]\n                if o[\"Key\"].split(S3_NAME_VERSION_SEPARATOR)[0] == self._name\n            ]\n            if \"Contents\" in found_objects\n            else []\n        )\n\n    def _download(\n        self, remote_path: Any, local_path: Path, hide_progress: bool\n    ) -&gt; None:\n        logger.info(f\"Downloading {remote_path} from S3\")\n\n        size = self._client.head_object(Bucket=self.bucket_name, Key=remote_path)[\n            \"ContentLength\"\n        ]\n\n        self._client.download_file(\n            Bucket=self.bucket_name,\n            Key=remote_path,\n            Filename=str(local_path),\n            Callback=None\n            if hide_progress\n            else DownloadProgressBar(name=str(remote_path), size=size, logger=logger),\n        )\n\n    def _upload(self, local_path: Path, hide_progress: bool) -&gt; None:\n        key = f\"{self._name}/{self.version}\"\n        logger.info(f\"Uploading {self._local_name} to S3 as {key}\")\n\n        self._client.upload_file(\n            Filename=str(local_path),\n            Bucket=self.bucket_name,\n            Key=key,\n            Callback=None\n            if hide_progress\n            else UploadProgressBar(path=local_path, logger=logger),\n        )\n\n    def _delete_old_remote_versions(self) -&gt; None:\n        if self._keep_last_n is not None:\n            for i in (\n                self._instances[: -self._keep_last_n]\n                if self._keep_last_n &gt; 0\n                else self._instances\n            ):\n                logger.info(\n                    f\"Removing old version from S3 (keep_last_n={self._keep_last_n}): {i.remote_path}\"\n                )\n                self._client.delete_object(Bucket=self.bucket_name, Key=i.remote_path)\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileMongo","title":"<code>LargeFileMongo</code>","text":"<p>             Bases: <code>LargeFileBase</code></p> <p>LargeFile implementation using GridFS (MongoDB) as a backend.</p> <p>Store large files remotely using the familiar API of <code>open()</code>. With built-in versioning, pruning and local cache.</p> <p>See parent for more details.</p> Source code in <code>great_ai/large_file/large_file/large_file_mongo.py</code> <pre><code>class LargeFileMongo(LargeFileBase):\n    \"\"\"LargeFile implementation using GridFS (MongoDB) as a backend.\n\n    Store large files remotely using the familiar API of `open()`. With built-in\n    versioning, pruning and local cache.\n\n    See parent for more details.\n    \"\"\"\n\n    mongo_connection_string = None\n    mongo_database = None\n\n    @classmethod\n    def configure_credentials(  # type: ignore\n        cls,\n        *,\n        mongo_connection_string: str,\n        mongo_database: str,\n        **_: Any,\n    ) -&gt; None:\n        cls.mongo_connection_string = mongo_connection_string\n        cls.mongo_database = mongo_database\n        super().configure_credentials()\n\n    @cached_property\n    def _client(self) -&gt; GridFSBucket:\n        if self.mongo_connection_string is None or self.mongo_database is None:\n            raise ValueError(\n                \"Please configure the MongoDB access options by calling LargeFileMongo.configure_credentials or set cache_only_mode=True in the constructor.\"\n            )\n\n        db: Database = MongoClient(self.mongo_connection_string)[self.mongo_database]\n        return GridFSBucket(db)\n\n    def _find_remote_instances(self) -&gt; List[DataInstance]:\n        logger.debug(f\"Fetching Mongo (GridFS) versions of {self._name}\")\n\n        return [\n            DataInstance(\n                name=MONGO_NAME_VERSION_SEPARATOR.join(\n                    f.name.split(MONGO_NAME_VERSION_SEPARATOR)[:-1]\n                ),\n                version=int(f.name.split(MONGO_NAME_VERSION_SEPARATOR)[-1]),\n                remote_path=(f._id, f.length),\n                origin=\"mongodb\",\n            )\n            for f in self._client.find(\n                {\n                    \"filename\": re.compile(\n                        re.escape(self._name + MONGO_NAME_VERSION_SEPARATOR) + \".*\"\n                    )\n                }\n            )\n        ]\n\n    def _download(\n        self, remote_path: Any, local_path: Path, hide_progress: bool\n    ) -&gt; None:\n        logger.info(f\"Downloading {remote_path[0]} from Mongo (GridFS)\")\n\n        progress = (\n            DownloadProgressBar(\n                name=str(remote_path[0]), size=remote_path[1], logger=logger\n            )\n            if not hide_progress\n            else None\n        )\n        with self._client.open_download_stream(remote_path[0]) as stream:\n            with open(local_path, \"wb\") as f:\n                while True:\n                    content = stream.read(DEFAULT_CHUNK_SIZE)\n                    f.write(content)\n\n                    if progress:\n                        progress(len(content))\n                    if len(content) &lt; DEFAULT_CHUNK_SIZE:\n                        break\n\n    def _upload(self, local_path: Path, hide_progress: bool) -&gt; None:\n        logger.info(f\"Uploading {local_path} to Mongo (GridFS)\")\n\n        progress = (\n            UploadProgressBar(path=local_path, logger=logger)\n            if not hide_progress\n            else None\n        )\n        with self._client.open_upload_stream(\n            f\"{self._name}{MONGO_NAME_VERSION_SEPARATOR}{self.version}\"\n        ) as stream:\n            with open(local_path, \"rb\") as f:\n                while True:\n                    content = f.read(DEFAULT_CHUNK_SIZE)\n                    stream.write(content)\n\n                    if progress:\n                        progress(len(content))\n                    if len(content) &lt; DEFAULT_CHUNK_SIZE:\n                        break\n\n    def _delete_old_remote_versions(self) -&gt; None:\n        if self._keep_last_n is not None:\n            for i in (\n                self._instances[: -self._keep_last_n]\n                if self._keep_last_n &gt; 0\n                else self._instances\n            ):\n                logger.info(\n                    f\"Removing old version from MongoDB (GridFS) (keep_last_n={self._keep_last_n}): {i.name}{MONGO_NAME_VERSION_SEPARATOR}{i.version}\"\n                )\n                self._client.delete(i.remote_path[0])\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase","title":"<code>LargeFileBase</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base for LargeFile implementations with different backends.</p> <p>Store large files remotely using the familiar API of <code>open()</code>. With built-in versioning, pruning and local cache.</p> <p>By default, files are stored in the \".cache\" folder and the least recently used is deleted after the overall size reaches 30 GBs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; LargeFileBase.cache_path = Path(\".cache\")\n</code></pre> <pre><code>&gt;&gt;&gt; LargeFileBase.max_cache_size = \"30GB\"\n</code></pre> <p>Attributes:</p> Name Type Description <code>initialized</code> <p>Tell whether <code>configure_credentials</code> or <code>configure_credentials_from_file</code> has been already called.</p> <code>cache_path</code> <p>Storage location for cached files.</p> <code>max_cache_size</code> <code>Optional[str]</code> <p>Delete files until the folder at <code>cache_path</code> is smaller than this value. Examples: \"5 GB\", \"10MB\", \"0.3 TB\". Set to <code>None</code> for no automatic cache-pruning.</p> Source code in <code>great_ai/large_file/large_file/large_file_base.py</code> <pre><code>class LargeFileBase(ABC):\n    \"\"\"Base for LargeFile implementations with different backends.\n\n    Store large files remotely using the familiar API of `open()`. With built-in\n    versioning, pruning and local cache.\n\n    By default, files are stored in the \".cache\" folder and the least recently used is\n    deleted after the overall size reaches 30 GBs.\n\n    Examples:\n        &gt;&gt;&gt; LargeFileBase.cache_path = Path(\".cache\")\n\n        &gt;&gt;&gt; LargeFileBase.max_cache_size = \"30GB\"\n\n    Attributes:\n        initialized: Tell whether `configure_credentials` or\n            `configure_credentials_from_file` has been already called.\n        cache_path: Storage location for cached files.\n        max_cache_size: Delete files until the folder at `cache_path` is smaller than\n            this value. Examples: \"5 GB\", \"10MB\", \"0.3 TB\". Set to `None` for no\n            automatic cache-pruning.\n    \"\"\"\n\n    initialized = False\n    cache_path = Path(\".cache\")\n    max_cache_size: Optional[str] = \"30GB\"\n\n    def __init__(\n        self,\n        name: str,\n        mode: str = \"r\",\n        *,\n        buffering: int = -1,\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n        version: Optional[int] = None,\n        keep_last_n: Optional[int] = None,\n        cache_only_mode: bool = False,\n    ):\n        clean_name = re.sub(r\"[^!a-zA-Z0-9._-]+\", \"\", name)\n        if clean_name != name:\n            raise ValueError(\n                f\"Given name contains illegal characters, consider changing it to: `{clean_name}`\"\n            )\n\n        self._name = name\n        self._version = version\n        self._mode = mode\n        self._keep_last_n = keep_last_n\n        self._cache_only_mode = cache_only_mode\n\n        self._buffering = buffering\n        self._encoding = encoding\n\n        if errors is not None and sys.version_info[1] &lt; 8:\n            raise RuntimeError(\n                \"The `errors` kwarg is only supported in 3.8 &lt;= Python versions.\"\n            )\n        self._errors = errors\n\n        self._newline = newline\n\n        LargeFileBase.cache_path.mkdir(parents=True, exist_ok=True)\n\n        self._find_instances()\n        self._check_mode_and_set_version()\n\n    @classmethod\n    def configure_credentials_from_file(\n        cls,\n        secrets: Union[Path, str, ConfigFile],\n    ) -&gt; None:\n        \"\"\"Load file and feed its content to `configure_credentials`.\n\n        Extra keys are ignored.\n        \"\"\"\n\n        if not isinstance(secrets, ConfigFile):\n            secrets = ConfigFile(secrets)\n        cls.configure_credentials(**{k.lower(): v for k, v in secrets.items()})\n\n    @classmethod\n    def configure_credentials(cls, **kwargs: str) -&gt; None:\n        \"\"\"Configure required credentials for the LargeFile backend.\"\"\"\n\n        cls.initialized = True\n\n    def __enter__(self) -&gt; IO:\n        params = dict(\n            mode=self._mode,\n            buffering=self._buffering,\n            encoding=self._encoding,\n            newline=self._newline,\n            delete=False,\n            prefix=\"large_file-\",\n        )\n\n        if sys.version_info[1] &gt;= 8:\n            params[\"errors\"] = self._errors\n\n        self._file: IO[Any] = (\n            tempfile.NamedTemporaryFile(**params)  # type: ignore\n            if \"w\" in self._mode\n            else open(\n                self.get(),\n                mode=self._mode,\n                buffering=self._buffering,\n                encoding=self._encoding,\n                newline=self._newline,\n                errors=self._errors,\n            )\n        )\n\n        return self._file\n\n    def __exit__(\n        self,\n        type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        traceback: Optional[TracebackType],\n    ) -&gt; Literal[False]:\n        self._file.close()\n\n        if type is None:\n            if \"w\" in self._mode:\n                self.push(Path(self._file.name))\n                os.unlink(self._file.name)\n        else:\n            logger.exception(\"Could not finish operation.\")\n\n        return False\n\n    @property\n    def version(self) -&gt; int:\n        \"\"\"Numeric version of the file proxied by this LargeFile instance.\"\"\"\n\n        return cast(int, self._version)\n\n    @lru_cache(1)\n    def get(self, hide_progress: bool = False) -&gt; Path:\n        \"\"\"Return path to the proxy of a file (or directory).\n\n        If not available in the local cache, an attempt is made to download it.\n\n        Args:\n            hide_progress: Do not show a progress update after each 10% of progress.\n        \"\"\"\n\n        remote_path = next(\n            i.remote_path for i in self._instances if i.version == self._version\n        )\n\n        destination = self.cache_path / self._local_name\n        if not destination.exists():\n            logger.info(f\"File {self._local_name} does not exist locally\")\n\n            with tempfile.TemporaryDirectory() as tmp:\n                local_root_path = Path(tmp)\n                tmp_file_archive = (\n                    local_root_path / f\"{self._local_name}{ARCHIVE_EXTENSION}\"\n                )\n                self._download(\n                    remote_path, tmp_file_archive, hide_progress=hide_progress\n                )\n\n                logger.info(f\"Decompressing {self._local_name}\")\n                shutil.unpack_archive(str(tmp_file_archive), tmp, COMPRESSION_ALGORITHM)\n                shutil.move(str(local_root_path / self._local_name), str(destination))\n        else:\n            logger.info(f\"File {self._local_name} found in cache\")\n\n        return destination\n\n    def push(self, path: Union[Path, str], hide_progress: bool = False) -&gt; None:\n        \"\"\"Upload a file (or directory) as a new version of `key`.\n\n        The file/directory is compressed before upload.\n\n        Args:\n            hide_progress: Do not show a progress update after each 10% of progress.\n        \"\"\"\n\n        if isinstance(path, str):\n            path = Path(path)\n\n        with tempfile.TemporaryDirectory() as tmp:\n            if path.is_file():\n                logger.info(f\"Copying file for {self._local_name}\")\n                copy: Any = shutil.copy\n            else:\n                logger.info(f\"Copying directory for {self._local_name}\")\n                copy = shutil.copytree\n\n            try:\n                # Make local copy in the cache\n                shutil.rmtree(self.cache_path / self._local_name, ignore_errors=True)\n                copy(str(path), str(self.cache_path / self._local_name))\n            except shutil.SameFileError:\n                pass  # No worries\n\n            copy(str(path), str(Path(tmp) / self._local_name))\n\n            with tempfile.TemporaryDirectory() as tmp2:\n                # A directory has to be zipped and it cannot contain the output of the zipping\n                logger.info(f\"Compressing {self._local_name}\")\n                shutil.make_archive(\n                    str(Path(tmp2) / self._local_name),\n                    COMPRESSION_ALGORITHM,\n                    tmp,\n                )\n\n                file_to_be_uploaded = (\n                    Path(tmp2) / f\"{self._local_name}{ARCHIVE_EXTENSION}\"\n                )\n                self._upload(file_to_be_uploaded, hide_progress=hide_progress)\n\n        self.clean_up()\n\n    def delete(self) -&gt; None:\n        \"\"\"Delete all versions of the files under this `key`.\"\"\"\n\n        self._keep_last_n = 0\n        self._delete_old_remote_versions()\n\n    @property\n    def versions_pretty(self) -&gt; str:\n        \"\"\"Formatted string of all available versions.\"\"\"\n        return \", \".join((str(i.version) for i in self._instances))\n\n    def clean_up(self) -&gt; None:\n        \"\"\"Delete local and remote versions according to currently set cache and retention policy.\"\"\"\n\n        self._delete_old_remote_versions()\n        self._prune_cache()\n\n    @property\n    def _local_name(self) -&gt; str:\n        return f\"{self._name}{CACHE_NAME_VERSION_SEPARATOR}{self.version}\"\n\n    def _find_instances(self) -&gt; None:\n        if self._cache_only_mode:\n            self._instances = self._find_instances_from_cache()\n        else:\n            self._instances = self._find_remote_instances()\n\n        self._instances = sorted(self._instances, key=lambda i: i.version)\n\n    def _find_instances_from_cache(self) -&gt; List[DataInstance]:\n        logger.info(f\"Fetching cached versions of {self._name}\")\n\n        candidates = [\n            DataInstance(\n                name=CACHE_NAME_VERSION_SEPARATOR.join(\n                    f.name.split(CACHE_NAME_VERSION_SEPARATOR)[:-1]\n                ),\n                version=int(f.name.split(CACHE_NAME_VERSION_SEPARATOR)[-1]),\n                remote_path=f,\n            )\n            for f in self.cache_path.glob(\n                f\"{self._name}{CACHE_NAME_VERSION_SEPARATOR}*\"\n            )\n        ]\n\n        return [c for c in candidates if c.name == self._name]\n\n    def _check_mode_and_set_version(self) -&gt; None:\n        if \"+\" in self._mode:\n            raise ValueError(\n                f\"File mode `{self._mode}` is not allowed3, remove the `+`.\"\n            )\n\n        if \"w\" in self._mode:\n            if self._version is not None:\n                raise ValueError(\"Providing a version is not allowed in write mode.\")\n\n            self._version = self._instances[-1].version + 1 if self._instances else 0\n\n        elif \"r\" in self._mode:\n            if not self._instances:\n                raise FileNotFoundError(\n                    f\"File {self._name} not found. No versions are available.\"\n                )\n\n            if self._version is None:\n                self._version = self._instances[-1].version\n                logger.info(\n                    f\"Latest version of {self._name} is {self._version} \"\n                    + f\"(from versions: {self.versions_pretty})\"\n                )\n            elif self._version not in [i.version for i in self._instances]:\n                raise FileNotFoundError(\n                    f\"File {self._name} not found with version {self._version}. \"\n                    + f\"(from versions: {self.versions_pretty})\"\n                )\n        else:\n            raise ValueError(\"Unsupported file mode.\")\n\n    def _prune_cache(self) -&gt; None:\n        self.cache_path.mkdir(parents=True, exist_ok=True)\n\n        if self.max_cache_size is None:\n            return\n\n        allowed_size = human_readable_to_byte(self.max_cache_size)\n        assert allowed_size &gt;= 0\n\n        least_recently_read = sorted(\n            [f for f in self.cache_path.glob(\"*\")], key=lambda f: f.stat().st_atime\n        )\n\n        while sum(os.path.getsize(f) for f in least_recently_read) &gt; allowed_size:\n            file = least_recently_read.pop(0)\n            logger.info(\n                f\"Deleting file from cache to meet quota (max_cache_size={self.max_cache_size}): {file}\"\n            )\n            os.unlink(file)\n\n    @abstractmethod\n    def _find_remote_instances(self) -&gt; List[DataInstance]:\n        pass\n\n    @abstractmethod\n    def _download(\n        self, remote_path: Any, local_path: Path, hide_progress: bool\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def _upload(self, local_path: Path, hide_progress: bool) -&gt; None:\n        pass\n\n    @abstractmethod\n    def _delete_old_remote_versions(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.version","title":"<code>version: int</code>  <code>property</code>","text":"<p>Numeric version of the file proxied by this LargeFile instance.</p>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.versions_pretty","title":"<code>versions_pretty: str</code>  <code>property</code>","text":"<p>Formatted string of all available versions.</p>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.clean_up","title":"<code>clean_up()</code>","text":"<p>Delete local and remote versions according to currently set cache and retention policy.</p> Source code in <code>great_ai/large_file/large_file/large_file_base.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"Delete local and remote versions according to currently set cache and retention policy.\"\"\"\n\n    self._delete_old_remote_versions()\n    self._prune_cache()\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.configure_credentials","title":"<code>configure_credentials(**kwargs)</code>  <code>classmethod</code>","text":"<p>Configure required credentials for the LargeFile backend.</p> Source code in <code>great_ai/large_file/large_file/large_file_base.py</code> <pre><code>@classmethod\ndef configure_credentials(cls, **kwargs: str) -&gt; None:\n    \"\"\"Configure required credentials for the LargeFile backend.\"\"\"\n\n    cls.initialized = True\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.configure_credentials_from_file","title":"<code>configure_credentials_from_file(secrets)</code>  <code>classmethod</code>","text":"<p>Load file and feed its content to <code>configure_credentials</code>.</p> <p>Extra keys are ignored.</p> Source code in <code>great_ai/large_file/large_file/large_file_base.py</code> <pre><code>@classmethod\ndef configure_credentials_from_file(\n    cls,\n    secrets: Union[Path, str, ConfigFile],\n) -&gt; None:\n    \"\"\"Load file and feed its content to `configure_credentials`.\n\n    Extra keys are ignored.\n    \"\"\"\n\n    if not isinstance(secrets, ConfigFile):\n        secrets = ConfigFile(secrets)\n    cls.configure_credentials(**{k.lower(): v for k, v in secrets.items()})\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.delete","title":"<code>delete()</code>","text":"<p>Delete all versions of the files under this <code>key</code>.</p> Source code in <code>great_ai/large_file/large_file/large_file_base.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"Delete all versions of the files under this `key`.\"\"\"\n\n    self._keep_last_n = 0\n    self._delete_old_remote_versions()\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.get","title":"<code>get(hide_progress=False)</code>  <code>cached</code>","text":"<p>Return path to the proxy of a file (or directory).</p> <p>If not available in the local cache, an attempt is made to download it.</p> <p>Parameters:</p> Name Type Description Default <code>hide_progress</code> <code>bool</code> <p>Do not show a progress update after each 10% of progress.</p> <code>False</code> Source code in <code>great_ai/large_file/large_file/large_file_base.py</code> <pre><code>@lru_cache(1)\ndef get(self, hide_progress: bool = False) -&gt; Path:\n    \"\"\"Return path to the proxy of a file (or directory).\n\n    If not available in the local cache, an attempt is made to download it.\n\n    Args:\n        hide_progress: Do not show a progress update after each 10% of progress.\n    \"\"\"\n\n    remote_path = next(\n        i.remote_path for i in self._instances if i.version == self._version\n    )\n\n    destination = self.cache_path / self._local_name\n    if not destination.exists():\n        logger.info(f\"File {self._local_name} does not exist locally\")\n\n        with tempfile.TemporaryDirectory() as tmp:\n            local_root_path = Path(tmp)\n            tmp_file_archive = (\n                local_root_path / f\"{self._local_name}{ARCHIVE_EXTENSION}\"\n            )\n            self._download(\n                remote_path, tmp_file_archive, hide_progress=hide_progress\n            )\n\n            logger.info(f\"Decompressing {self._local_name}\")\n            shutil.unpack_archive(str(tmp_file_archive), tmp, COMPRESSION_ALGORITHM)\n            shutil.move(str(local_root_path / self._local_name), str(destination))\n    else:\n        logger.info(f\"File {self._local_name} found in cache\")\n\n    return destination\n</code></pre>"},{"location":"reference/large-file/#great_ai.large_file.LargeFileBase.push","title":"<code>push(path, hide_progress=False)</code>","text":"<p>Upload a file (or directory) as a new version of <code>key</code>.</p> <p>The file/directory is compressed before upload.</p> <p>Parameters:</p> Name Type Description Default <code>hide_progress</code> <code>bool</code> <p>Do not show a progress update after each 10% of progress.</p> <code>False</code> Source code in <code>great_ai/large_file/large_file/large_file_base.py</code> <pre><code>def push(self, path: Union[Path, str], hide_progress: bool = False) -&gt; None:\n    \"\"\"Upload a file (or directory) as a new version of `key`.\n\n    The file/directory is compressed before upload.\n\n    Args:\n        hide_progress: Do not show a progress update after each 10% of progress.\n    \"\"\"\n\n    if isinstance(path, str):\n        path = Path(path)\n\n    with tempfile.TemporaryDirectory() as tmp:\n        if path.is_file():\n            logger.info(f\"Copying file for {self._local_name}\")\n            copy: Any = shutil.copy\n        else:\n            logger.info(f\"Copying directory for {self._local_name}\")\n            copy = shutil.copytree\n\n        try:\n            # Make local copy in the cache\n            shutil.rmtree(self.cache_path / self._local_name, ignore_errors=True)\n            copy(str(path), str(self.cache_path / self._local_name))\n        except shutil.SameFileError:\n            pass  # No worries\n\n        copy(str(path), str(Path(tmp) / self._local_name))\n\n        with tempfile.TemporaryDirectory() as tmp2:\n            # A directory has to be zipped and it cannot contain the output of the zipping\n            logger.info(f\"Compressing {self._local_name}\")\n            shutil.make_archive(\n                str(Path(tmp2) / self._local_name),\n                COMPRESSION_ALGORITHM,\n                tmp,\n            )\n\n            file_to_be_uploaded = (\n                Path(tmp2) / f\"{self._local_name}{ARCHIVE_EXTENSION}\"\n            )\n            self._upload(file_to_be_uploaded, hide_progress=hide_progress)\n\n    self.clean_up()\n</code></pre>"},{"location":"reference/utilities/","title":"Utilities","text":"<pre><code>from great_ai.utilities import *\n</code></pre>"},{"location":"reference/utilities/#nlp-tools","title":"NLP tools","text":"<p>Well-tested tools that can be used in production with confidence. The toolbox of feature-extraction functions is expected to grow to cover other domains as well.</p>"},{"location":"reference/utilities/#great_ai.utilities.clean.clean","title":"<code>clean(text, ignore_xml=False, ignore_latex=False, remove_brackets=False, convert_to_ascii=False)</code>","text":"<p>Clean all XML, LaTeX, PDF-extraction, and Unicode artifacts from the text.</p> <p>The cleaning is quite heavy-weight and can be destructive. However, when working with text, this is usually required to achieve sufficient cleanliness before further processing.</p> <p>Optionally, the text can be turned into ASCII. Carefully consider whether this is absolutely needed for your use-case.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean('&lt;h2 color=\"red\"&gt;Bj\\\\\"{o}rn is \\t \\\\textit{happy} \ud83d\ude42 &amp;lt;3&lt;/h2&gt;')\n'Bj\u00f6rn is happy \ud83d\ude42 &lt;3'\n</code></pre> <pre><code>&gt;&gt;&gt; clean(\n...    '&lt;h2 color=\"red\"&gt;Bj\\\\\"{o}rn    is \\t \\\\textit{happy} \ud83d\ude42 &amp;lt;3&lt;/h2&gt;',\n...    convert_to_ascii=True\n... )\n'Bjorn is happy &lt;3'\n</code></pre> <pre><code>&gt;&gt;&gt; clean(\n... '&lt;h2 color=\"red\"&gt;Bj\\\\\"{o}rn       is \\t \\\\textit{happy} \ud83d\ude42 &amp;lt;3&lt;/h2&gt;',\n... ignore_xml=True\n... )\n'&lt;h2 color=\"red\"&gt;Bj\u00f6rn is happy \ud83d\ude42 lt;3&lt;/h2&gt;'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be cleaned.</p> required <code>ignore_xml</code> <code>bool</code> <p>Do not process/remove XML-tags.</p> <code>False</code> <code>ignore_latex</code> <code>bool</code> <p>Do not process/remove LaTeX-tags.</p> <code>False</code> <code>remove_brackets</code> <code>bool</code> <p>Do not remove brackets ([])</p> <code>False</code> <code>convert_to_ascii</code> <code>bool</code> <p>Strip (or convert) non-ascii characters.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The cleaned input text with sensibly collapsed whitespace and optionally no markup.</p> Source code in <code>great_ai/utilities/clean.py</code> <pre><code>def clean(\n    text: str,\n    ignore_xml: bool = False,\n    ignore_latex: bool = False,\n    remove_brackets: bool = False,\n    convert_to_ascii: bool = False,\n) -&gt; str:\n    \"\"\"Clean all XML, LaTeX, PDF-extraction, and Unicode artifacts from the text.\n\n    The cleaning is quite heavy-weight and can be destructive. However, when working\n    with text, this is usually required to achieve sufficient cleanliness before further\n    processing.\n\n    Optionally, the text can be turned into ASCII. Carefully consider whether this is\n    absolutely needed for your use-case.\n\n    Examples:\n        &gt;&gt;&gt; clean('&lt;h2 color=\"red\"&gt;Bj\\\\\\\\\"{o}rn is \\\\t \\\\\\\\textit{happy} \ud83d\ude42 &amp;lt;3&lt;/h2&gt;')\n        'Bj\u00f6rn is happy \ud83d\ude42 &lt;3'\n\n        &gt;&gt;&gt; clean(\n        ...    '&lt;h2 color=\"red\"&gt;Bj\\\\\\\\\"{o}rn    is \\\\t \\\\\\\\textit{happy} \ud83d\ude42 &amp;lt;3&lt;/h2&gt;',\n        ...    convert_to_ascii=True\n        ... )\n        'Bjorn is happy &lt;3'\n\n        &gt;&gt;&gt; clean(\n        ... '&lt;h2 color=\"red\"&gt;Bj\\\\\\\\\"{o}rn       is \\\\t \\\\\\\\textit{happy} \ud83d\ude42 &amp;lt;3&lt;/h2&gt;',\n        ... ignore_xml=True\n        ... )\n        '&lt;h2 color=\"red\"&gt;Bj\u00f6rn is happy \ud83d\ude42 lt;3&lt;/h2&gt;'\n\n    Args:\n        text: Text to be cleaned.\n        ignore_xml: Do not process/remove XML-tags.\n        ignore_latex: Do not process/remove LaTeX-tags.\n        remove_brackets: Do not remove brackets ([])\n        convert_to_ascii: Strip (or convert) non-ascii characters.\n\n    Returns:\n        The cleaned input text with sensibly collapsed whitespace and optionally no\n            markup.\n    \"\"\"\n\n    if not ignore_xml:\n        text = re.sub(r\"&lt;[^&gt;]*&gt;\", \" \", text)\n        text = html.unescape(text)\n\n    if not ignore_latex:\n        text = text.replace(\"%\", \"\\\\%\")  # escape LaTeX comments before parsing as LaTeX\n\n        try:\n            text = latex.latex_to_text(text, tolerant_parsing=True, strict_braces=False)\n            text = text.replace(\"%s\", \" \")\n        except:\n            logger.exception(\"Latex parsing error\")\n\n    if convert_to_ascii:\n        text = unicodedata.normalize(\"NFKD\", text)\n\n        try:\n            text.encode(\"ASCII\", errors=\"strict\")\n        except UnicodeEncodeError:\n            text = \"\".join([c for c in text if not unicodedata.combining(c)])\n            text = unidecode.unidecode(text)\n\n    text = re.sub(\n        r\"\\b[a-zA-Z](?:[\\t ]+[a-zA-Z]\\b)+\", lambda m: re.sub(r\"[\\t ]\", \"\", m[0]), text\n    )  # A R T I C L E =&gt; ARTICLE\n\n    if remove_brackets:\n        text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n\n    # fix hypens: break- word =&gt; break-word\n    text = re.sub(r\"(\\S)-\\s+\", r\"\\1-\", text)\n    text = re.sub(r\"\\s+-(\\S)\", r\"-\\1\", text)\n\n    # collapse whitespace\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # fix punctuation\n    text = re.sub(rf\" ([{joined_left_punctuations}])\", r\"\\1\", text)\n    text = re.sub(rf\"([{joined_right_punctuations}]) \", r\"\\1\", text)\n\n    text = text.strip()\n\n    return text\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.get_sentences.get_sentences","title":"<code>get_sentences(text, ignore_partial=False, true_case=False, remove_punctuation=False)</code>","text":"<p>Return the list of sentences found in the input text.</p> <p>Use syntok to segment the sentences. Further processing can be enabled with optional arguments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_sentences('This is a sentence. This is a half')\n['This is a sentence.', 'This is a half']\n</code></pre> <pre><code>&gt;&gt;&gt; get_sentences('This is a sentence. This is a half', ignore_partial=True)\n['This is a sentence.']\n</code></pre> <pre><code>&gt;&gt;&gt; get_sentences('I like Apple.', true_case=True, remove_punctuation=True)\n['i like Apple']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be segmented into sentences.</p> required <code>ignore_partial</code> <code>bool</code> <p>Filter out sentences that are not capitalised/don't end with a punctuation.</p> <code>False</code> <code>true_case</code> <code>bool</code> <p>Crude method: lowercase the first word of each sentence.</p> <code>False</code> <code>remove_punctuation</code> <code>bool</code> <p>Remove all kinds of punctuation.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>The found sentences (with partial sentences optionally filtered out).</p> Source code in <code>great_ai/utilities/get_sentences.py</code> <pre><code>def get_sentences(\n    text: str,\n    ignore_partial: bool = False,\n    true_case: bool = False,\n    remove_punctuation: bool = False,\n) -&gt; List[str]:\n    \"\"\"Return the list of sentences found in the input text.\n\n    Use [syntok](https://github.com/fnl/syntok) to segment the sentences. Further\n    processing can be enabled with optional arguments.\n\n    Examples:\n        &gt;&gt;&gt; get_sentences('This is a sentence. This is a half')\n        ['This is a sentence.', 'This is a half']\n\n        &gt;&gt;&gt; get_sentences('This is a sentence. This is a half', ignore_partial=True)\n        ['This is a sentence.']\n\n        &gt;&gt;&gt; get_sentences('I like Apple.', true_case=True, remove_punctuation=True)\n        ['i like Apple']\n\n    Args:\n        text: Text to be segmented into sentences.\n        ignore_partial: Filter out sentences that are not capitalised/don't end with a\n            punctuation.\n        true_case: Crude method: lowercase the first word of each sentence.\n        remove_punctuation: Remove all kinds of punctuation.\n\n    Returns:\n        The found sentences (with partial sentences optionally filtered out).\n    \"\"\"\n\n    tokenizer = Tokenizer(\n        emit_hyphen_or_underscore_sep=True, replace_not_contraction=False\n    )\n    token_stream = tokenizer.tokenize(text)\n\n    def process(sentence: str) -&gt; str:\n        if true_case:\n            sentence = sentence[0].lower() + sentence[1:]  # very crude method\n        if remove_punctuation:\n            sentence = re.sub(punctuations_pattern, \" \", sentence)\n        return sentence.strip()\n\n    sentences = [\n        process(tokenizer.to_text(sentence)) for sentence in segment(token_stream)\n    ]\n\n    if ignore_partial:\n        sentences = [\n            sentence\n            for sentence in sentences\n            if sentence[0].isupper() and sentence[-1] in sentence_ending_punctuations\n        ]\n\n    return sentences\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.language.predict_language.predict_language","title":"<code>predict_language(text)</code>","text":"<p>Predict the language code from text.</p> <p>A thin wrapper over langcodes for convenient language tagging.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predict_language('This is a sentence.')\n'en'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Optional[str]</code> <p>Text used for prediction.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The predicted language code (en, en-US) or <code>und</code> if a prediction could not be made.</p> Source code in <code>great_ai/utilities/language/predict_language.py</code> <pre><code>def predict_language(text: Optional[str]) -&gt; str:\n    \"\"\"Predict the language code from text.\n\n    A thin wrapper over [langcodes](https://github.com/rspeer/langcodes) for convenient\n    language tagging.\n\n    Examples:\n        &gt;&gt;&gt; predict_language('This is a sentence.')\n        'en'\n\n    Args:\n        text: Text used for prediction.\n\n    Returns:\n        The predicted language code (en, en-US) or `und` if a prediction could not be\n            made.\n    \"\"\"\n\n    if not text:\n        return Language.make().to_tag()\n\n    try:\n        language_code = detect(text)\n    except LangDetectException:\n        return Language.make().to_tag()\n\n    return Language.get(language_code).to_tag()\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.language.english_name_of_language.english_name_of_language","title":"<code>english_name_of_language(language_code)</code>","text":"<p>Human-friendly English name of language from its <code>language_code</code>.</p> <p>A thin wrapper over langcodes for convenient language tagging.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; english_name_of_language('en-US')\n'English (United States)'\n</code></pre> <pre><code>&gt;&gt;&gt; english_name_of_language('und')\n'Unknown language'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>language_code</code> <code>Optional[str]</code> <p>Language code, for example, returned by great_ai.utilities.language.predict_language.predict_language.</p> required <p>Returns:</p> Type Description <code>str</code> <p>English name of language.</p> Source code in <code>great_ai/utilities/language/english_name_of_language.py</code> <pre><code>def english_name_of_language(language_code: Optional[str]) -&gt; str:\n    \"\"\"Human-friendly English name of language from its `language_code`.\n\n    A thin wrapper over [langcodes](https://github.com/rspeer/langcodes) for convenient\n    language tagging.\n\n    Examples:\n        &gt;&gt;&gt; english_name_of_language('en-US')\n        'English (United States)'\n\n        &gt;&gt;&gt; english_name_of_language('und')\n        'Unknown language'\n\n    Args:\n        language_code: Language code, for example, returned by\n            [great_ai.utilities.language.predict_language.predict_language][].\n\n    Returns:\n        English name of language.\n    \"\"\"\n\n    if not language_code:\n        language_code = \"und\"\n\n    return Language.get(language_code).display_name()\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.language.is_english.is_english","title":"<code>is_english(language_code)</code>","text":"<p>Decide whether the <code>language_code</code> is of an English language.</p> <p>A thin wrapper over langcodes for convenient language tagging.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_english('en-US')\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; is_english(None)\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_english('und')\nFalse\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>language_code</code> <code>Optional[str]</code> <p>Language code, for example, returned by `great_ai.utilities.language.predict_language.predict_language.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Boolean indicating whether it's English.</p> Source code in <code>great_ai/utilities/language/is_english.py</code> <pre><code>def is_english(language_code: Optional[str]) -&gt; bool:\n    \"\"\"Decide whether the `language_code` is of an English language.\n\n    A thin wrapper over [langcodes](https://github.com/rspeer/langcodes) for convenient\n    language tagging.\n\n    Examples:\n        &gt;&gt;&gt; is_english('en-US')\n        True\n\n        &gt;&gt;&gt; is_english(None)\n        False\n\n        &gt;&gt;&gt; is_english('und')\n        False\n\n    Args:\n        language_code: Language code, for example, returned by\n            `[great_ai.utilities.language.predict_language.predict_language][].\n\n    Returns:\n        Boolean indicating whether it's English.\n    \"\"\"\n    if not language_code:\n        language_code = \"und\"\n\n    language_code = standardize_tag(language_code)\n    return tag_distance(language_code, \"en\") &lt; 15\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.evaluate_ranking.evaluate_ranking.evaluate_ranking","title":"<code>evaluate_ranking(expected, actual_scores, target_recall, title='', disable_interpolation=False, axes=None, output_svg=None, reverse_order=False, plot=True)</code>","text":"<p>Render the Precision-Recall curve of a ranking.</p> <p>And improved version of scikit-learn's PR-curve</p> <p>Parameters:</p> Name Type Description Default <code>expected</code> <code>List[T]</code> <p>Expected ordering of the elements (rank if it's an integer, alphabetical if a string)</p> required <code>actual_scores</code> <code>List[float]</code> <p>Actual ranking scores (need not be on the same scale as <code>expected</code>)</p> required <code>title</code> <code>Optional[str]</code> <p>Title of the plot.</p> <code>''</code> <code>disable_interpolation</code> <code>bool</code> <p>Do not interpolate.</p> <code>False</code> <code>axes</code> <code>Optional[Axes]</code> <p>Matplotlib axes for plotting inside a subplot.</p> <code>None</code> <code>output_svg</code> <code>Optional[Path]</code> <p>If specified, save the chart as an svg to the given Path.</p> <code>None</code> <code>reverse_order</code> <code>bool</code> <p>Reverse the ranking specified by <code>expected</code>.</p> <code>False</code> <code>plot</code> <code>bool</code> <p>Display a plot on the screen.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[T, float]</code> <p>Precision values at given recall.</p> Source code in <code>great_ai/utilities/evaluate_ranking/evaluate_ranking.py</code> <pre><code>def evaluate_ranking(\n    expected: List[T],\n    actual_scores: List[float],\n    target_recall: float,\n    title: Optional[str] = \"\",\n    disable_interpolation: bool = False,\n    axes: Optional[plt.Axes] = None,\n    output_svg: Optional[Path] = None,\n    reverse_order: bool = False,\n    plot: bool = True,\n) -&gt; Dict[T, float]:\n    \"\"\"Render the Precision-Recall curve of a ranking.\n\n    And improved version of scikit-learn's [PR-curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py)\n\n    Args:\n        expected: Expected ordering of the elements\n            (rank if it's an integer, alphabetical if a string)\n        actual_scores: Actual ranking scores (need not be on the same scale as\n            `expected`)\n        title: Title of the plot.\n        disable_interpolation: Do not interpolate.\n        axes: Matplotlib axes for plotting inside a subplot.\n        output_svg: If specified, save the chart as an svg to the given Path.\n        reverse_order: Reverse the ranking specified by `expected`.\n        plot: Display a plot on the screen.\n\n    Returns:\n        Precision values at given recall.\n    \"\"\"\n\n    assert 0 &lt;= target_recall &lt;= 1\n\n    if plot and axes is None:\n        fig = plt.figure(figsize=(10, 10))\n        fig.patch.set_facecolor(\"white\")\n        ax = plt.axes()\n    else:\n        ax = axes\n\n    classes = sorted(unique(expected), reverse=reverse_order)\n    str_classes = [str(c) for c in classes]\n\n    with matplotlib.rc_context({\"font.size\": 20}):\n        if plot:\n            ax.set_xmargin(0)\n\n            draw_f1_iso_lines(axes=ax)\n\n        results: Dict[T, float] = {}\n        for i in range(len(classes) - 1):\n            binarized_expected = [\n                (v &lt; classes[i]) if reverse_order else (v &gt; classes[i])\n                for v in expected\n            ]\n\n            sorted_expected_actual = sorted(\n                zip(binarized_expected, actual_scores), key=lambda v: v[1], reverse=True\n            )\n            precision = []\n            recall = []\n            correct = 0\n            for all, (e, score) in enumerate(sorted_expected_actual, start=1):\n                correct += int(e)\n                precision.append(correct / all)\n                recall.append(all / len(sorted_expected_actual))\n\n            if not disable_interpolation:\n                for j in range(len(precision) - 2, -1, -1):\n                    precision[j] = max(precision[j], precision[j + 1])\n\n            closest_recall_index = np.argmin(np.abs(np.array(recall) - target_recall))\n            precision_at_closest_recall = precision[closest_recall_index]\n            average_precision = average_precision_score(\n                binarized_expected, actual_scores\n            )\n            results[classes[i]] = precision_at_closest_recall\n\n            if plot:\n                ax.plot(\n                    recall,\n                    precision,\n                    label=f\"{'|'.join(str_classes[:i + 1])} \u2194 {'|'.join(str_classes[i+1:])} (P@{target_recall:.2f}={precision_at_closest_recall:.2f}, AP={average_precision:.2f})\",\n                )\n\n        if plot:\n            ax.legend(loc=\"upper right\")\n            ax.axvline(x=target_recall, linestyle=\"--\", color=\"#55c6bb\", linewidth=2.0)\n\n            if title is None:\n                title = \"Ranking evaluation\"\n\n            ax.set_title(f'{title} ({\" &lt; \".join(str_classes)})', pad=20)\n\n            ax.set_xlabel(\"Recall\")\n            ax.set_ylabel(\"Precision\")\n\n            ax.set_xticks([target_recall] + sorted(ax.get_xticks()))\n\n        if plot and output_svg is None:\n            if axes is None:\n                plt.show()\n        elif output_svg:\n            plt.savefig(output_svg, format=\"svg\")\n\n    return results\n</code></pre>"},{"location":"reference/utilities/#parallel-processing","title":"Parallel processing","text":"<p>Multiprocessing and multithreading-based parallelism with support for <code>async</code> functions. Its main purpose is to implement great_ai.GreatAI.process_batch, however, the parallel processing functions are also convenient for covering other types of mapping needs with a friendlier API than joblib or multiprocess.</p>"},{"location":"reference/utilities/#great_ai.utilities.simple_parallel_map","title":"<code>simple_parallel_map(func, input_values, *, chunk_size=None, concurrency=None)</code>","text":"<p>Execute a map operation on an list mimicking the API of the built-in <code>map()</code>.</p> <p>A thin-wrapper over parallel_map. For more options, consult the documentation of parallel_map.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; list(simple_parallel_map(math.sqrt, [9, 4, 1]))\n[3.0, 2.0, 1.0]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[T], Union[V, Awaitable[V]]]</code> <p>The function that should be applied to each element of <code>input_values</code>. It can <code>async</code>, in that case, a new event loop is started for each chunk.</p> required <code>input_values</code> <code>Sequence[T]</code> <p>An iterable of items that <code>func</code> is applied to.</p> required <code>chunk_size</code> <code>Optional[int]</code> <p>Tune the number of items processed in each step. Larger numbers result in smaller communication overhead but less parallelism at the start and end. If <code>chunk_size</code> has a <code>__len__</code> property, the <code>chunk_size</code> is calculated automatically if not given.</p> <code>None</code> <code>concurrency</code> <code>Optional[int]</code> <p>Number of new processes to start. Shouldn't be too much more than the number of physical cores.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[V]</code> <p>An iterable of results obtained from applying <code>func</code> to each input value.</p> <p>Raises:</p> Type Description <code>WorkerException</code> <p>If there was an error in the <code>func</code> function in a background process.</p> Source code in <code>great_ai/utilities/parallel_map/simple_parallel_map.py</code> <pre><code>def simple_parallel_map(\n    func: Callable[[T], Union[V, Awaitable[V]]],\n    input_values: Sequence[T],\n    *,\n    chunk_size: Optional[int] = None,\n    concurrency: Optional[int] = None,\n) -&gt; List[V]:\n    \"\"\"Execute a map operation on an list mimicking the API of the built-in `map()`.\n\n    A thin-wrapper over [parallel_map][great_ai.utilities.parallel_map.parallel_map.parallel_map].\n    For more options, consult the documentation of\n    [parallel_map][great_ai.utilities.parallel_map.parallel_map.parallel_map].\n\n    Examples:\n        &gt;&gt;&gt; import math\n        &gt;&gt;&gt; list(simple_parallel_map(math.sqrt, [9, 4, 1]))\n        [3.0, 2.0, 1.0]\n\n    Args:\n        func: The function that should be applied to each element of `input_values`.\n            It can `async`, in that case, a new event loop is started for each chunk.\n        input_values: An iterable of items that `func` is applied to.\n        chunk_size: Tune the number of items processed in each step. Larger numbers\n            result in smaller communication overhead but less parallelism at the start\n            and end. If `chunk_size` has a `__len__` property, the `chunk_size` is\n            calculated automatically if not given.\n        concurrency: Number of new processes to start. Shouldn't be too much more than\n            the number of physical cores.\n\n    Returns:\n        An iterable of results obtained from applying `func` to each input value.\n\n    Raises:\n        WorkerException: If there was an error in the `func` function in a background\n            process.\n    \"\"\"\n\n    input_values = list(input_values)  # in case the input is mistakenly not a sequence\n    generator = parallel_map(\n        func=func,\n        input_values=input_values,\n        chunk_size=chunk_size,\n        concurrency=concurrency,\n    )\n\n    return list(\n        tqdm(\n            generator,\n            total=len(input_values),\n        )\n    )\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.parallel_map.parallel_map.parallel_map","title":"<code>parallel_map(func, input_values, *, chunk_size=None, ignore_exceptions=False, concurrency=None, unordered=False)</code>","text":"<p>Execute a map operation on an iterable stream.</p> <p>A custom parallel map operation supporting both synchronous and <code>async</code> map functions. The <code>func</code> function is serialised with <code>dill</code>. Exceptions encountered in the map function are sent to the host process where they are either raised (default) or ignored.</p> <p>The new processes are forked if the OS allows it, otherwise, new Python processes are bootstrapped which can incur some start-up cost. Each process processes a single chunk at once.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; list(parallel_map(math.sqrt, [9, 4, 1], concurrency=2))\n[3.0, 2.0, 1.0]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[T], Union[V, Awaitable[V]]]</code> <p>The function that should be applied to each element of <code>input_values</code>. It can <code>async</code>, in that case, a new event loop is started for each chunk.</p> required <code>input_values</code> <code>Union[Iterable[T], Sequence[T]]</code> <p>An iterable of items that <code>func</code> is applied to.</p> required <code>chunk_size</code> <code>Optional[int]</code> <p>Tune the number of items processed in each step. Larger numbers result in smaller communication overhead but less parallelism at the start and end. If <code>chunk_size</code> has a <code>__len__</code> property, the <code>chunk_size</code> is calculated automatically if not given.</p> <code>None</code> <code>ignore_exceptions</code> <code>bool</code> <p>Ignore chunks if <code>next()</code> raises an exception on <code>input_values</code>. And return <code>None</code> if <code>func</code> raised an exception in a worker process.</p> <code>False</code> <code>concurrency</code> <code>Optional[int]</code> <p>Number of new processes to start. Shouldn't be too much more than the number of physical cores.</p> <code>None</code> <code>unordered</code> <code>bool</code> <p>Do not preserve the order of the elements, yield them as soon as they have been processed. This decreases the latency caused by difficult-to-process items.</p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[Optional[V]]</code> <p>The next result obtained from applying <code>func</code> to each input value. May contain <code>None</code>-s if <code>ignore_exceptions=True</code>. May have different order than the input if <code>unordered=True</code>.</p> <p>Raises:</p> Type Description <code>WorkerException</code> <p>If there was an error in the <code>func</code> function in a background process and <code>ignore_exceptions=False</code>.</p> Source code in <code>great_ai/utilities/parallel_map/parallel_map.py</code> <pre><code>def parallel_map(\n    func: Callable[[T], Union[V, Awaitable[V]]],\n    input_values: Union[Iterable[T], Sequence[T]],\n    *,\n    chunk_size: Optional[int] = None,\n    ignore_exceptions: bool = False,\n    concurrency: Optional[int] = None,\n    unordered: bool = False,\n) -&gt; Iterable[Optional[V]]:\n    \"\"\"Execute a map operation on an iterable stream.\n\n    A custom parallel map operation supporting both synchronous and `async` map\n    functions. The `func` function is serialised with `dill`. Exceptions encountered in\n    the map function are sent to the host process where they are either raised (default)\n    or ignored.\n\n    The new processes are forked if the OS allows it, otherwise, new Python processes\n    are bootstrapped which can incur some start-up cost. Each process processes a single\n    chunk at once.\n\n    Examples:\n        &gt;&gt;&gt; import math\n        &gt;&gt;&gt; list(parallel_map(math.sqrt, [9, 4, 1], concurrency=2))\n        [3.0, 2.0, 1.0]\n\n    Args:\n        func: The function that should be applied to each element of `input_values`.\n            It can `async`, in that case, a new event loop is started for each chunk.\n        input_values: An iterable of items that `func` is applied to.\n        chunk_size: Tune the number of items processed in each step. Larger numbers\n            result in smaller communication overhead but less parallelism at the start\n            and end. If `chunk_size` has a `__len__` property, the `chunk_size` is\n            calculated automatically if not given.\n        ignore_exceptions: Ignore chunks if `next()` raises an exception on\n            `input_values`. And return `None` if `func` raised an exception in a worker\n            process.\n        concurrency: Number of new processes to start. Shouldn't be too much more than\n            the number of physical cores.\n        unordered: Do not preserve the order of the elements, yield them as soon as they\n            have been processed. This decreases the latency caused by\n            difficult-to-process items.\n\n    Yields:\n        The next result obtained from applying `func` to each input value. May\n            contain `None`-s if `ignore_exceptions=True`. May have different order than\n            the input if `unordered=True`.\n\n    Raises:\n        WorkerException: If there was an error in the `func` function in a background\n            process and `ignore_exceptions=False`.\n    \"\"\"\n\n    config = get_config(\n        function=func,\n        input_values=input_values,\n        chunk_size=chunk_size,\n        concurrency=concurrency,\n    )\n\n    ctx = (\n        mp.get_context(\"fork\")\n        if \"fork\" in mp.get_all_start_methods()\n        else mp.get_context(\"spawn\")\n    )\n    ctx.freeze_support()\n    manager = ctx.Manager()\n    input_queue = manager.Queue(config.concurrency * 2)\n    output_queue = manager.Queue(config.concurrency * 2)\n\n    should_stop = ctx.Event()\n    serialized_map_function = dill.dumps(func, byref=True, recurse=False)\n\n    processes = [\n        ctx.Process(  # type: ignore\n            name=f\"parallel_map_{config.function_name}_{i}\",\n            target=mapper_function,\n            daemon=True,\n            kwargs=dict(\n                input_queue=input_queue,\n                output_queue=output_queue,\n                should_stop=should_stop,\n                func=serialized_map_function,\n            ),\n        )\n        for i in range(config.concurrency)\n    ]\n\n    for p in processes:\n        p.start()\n\n    try:\n        yield from manage_communication(\n            input_values=input_values,\n            chunk_size=config.chunk_size,\n            input_queue=input_queue,\n            output_queue=output_queue,\n            unordered=unordered,\n            ignore_exceptions=ignore_exceptions,\n        )\n        should_stop.set()\n    except WorkerException:\n        should_stop.set()\n        raise\n    except Exception:\n        for p in processes:\n            p.terminate()\n            p.kill()\n        raise\n    finally:\n        for p in processes:\n            p.join()  # terminated processes have to be joined else they remain zombies\n            p.close()\n\n        manager.shutdown()\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.threaded_parallel_map","title":"<code>threaded_parallel_map(func, input_values, *, chunk_size=None, ignore_exceptions=False, concurrency=None, unordered=False)</code>","text":"<p>Execute a map operation on an iterable stream.</p> <p>Similar to parallel_map but uses threads instead of processes. Hence, it is not helpful in CPU-bound situations.</p> <p>A custom parallel map operation supporting both synchronous and <code>async</code> map functions. Exceptions encountered in the map function are sent to the host thread where they are either raised (default) or ignored. Each process processes a single chunk at once.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(threaded_parallel_map(lambda x: x ** 2, [1, 2, 3]))\n[1, 4, 9]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[T], Union[V, Awaitable[V]]]</code> <p>The function that should be applied to each element of <code>input_values</code>. It can <code>async</code>, in that case, a new event loop is started for each chunk.</p> required <code>input_values</code> <code>Union[Iterable[T], Sequence[T]]</code> <p>An iterable of items that <code>func</code> is applied to.</p> required <code>chunk_size</code> <code>Optional[int]</code> <p>Tune the number of items processed in each step. Larger numbers result in smaller communication overhead but less parallelism at the start and end. If <code>chunk_size</code> has a <code>__len__</code> property, the <code>chunk_size</code> is calculated automatically if not given.</p> <code>None</code> <code>ignore_exceptions</code> <code>bool</code> <p>Ignore chunks if <code>next()</code> raises an exception on <code>input_values</code>. And return <code>None</code> if <code>func</code> raised an exception in a worker process.</p> <code>False</code> <code>concurrency</code> <code>Optional[int]</code> <p>Number of new threads to start.</p> <code>None</code> <code>unordered</code> <code>bool</code> <p>Do not preserve the order of the elements, yield them as soon as they have been processed. This decreases the latency caused by difficult-to-process items.</p> <code>False</code> <p>Yields:</p> Type Description <code>Iterable[Optional[V]]</code> <p>The next result obtained from applying <code>func</code> to each input value. May contain <code>None</code>-s if <code>ignore_exceptions=True</code>. May have different order than the input if <code>unordered=True</code>.</p> <p>Raises:</p> Type Description <code>WorkerException</code> <p>If there was an error in the <code>func</code> function in a background thread and <code>ignore_exceptions=False</code>.</p> Source code in <code>great_ai/utilities/parallel_map/threaded_parallel_map.py</code> <pre><code>def threaded_parallel_map(\n    func: Callable[[T], Union[V, Awaitable[V]]],\n    input_values: Union[Iterable[T], Sequence[T]],\n    *,\n    chunk_size: Optional[int] = None,\n    ignore_exceptions: bool = False,\n    concurrency: Optional[int] = None,\n    unordered: bool = False,\n) -&gt; Iterable[Optional[V]]:\n    \"\"\"Execute a map operation on an iterable stream.\n\n    Similar to [parallel_map][great_ai.utilities.parallel_map.parallel_map.parallel_map]\n    but uses threads instead of processes. Hence, it is not helpful in CPU-bound\n    situations.\n\n    A custom parallel map operation supporting both synchronous and `async` map\n    functions. Exceptions encountered in the map function are sent to the host thread\n    where they are either raised (default) or ignored. Each process processes a single\n    chunk at once.\n\n    Examples:\n        &gt;&gt;&gt; list(threaded_parallel_map(lambda x: x ** 2, [1, 2, 3]))\n        [1, 4, 9]\n\n    Args:\n        func: The function that should be applied to each element of `input_values`.\n            It can `async`, in that case, a new event loop is started for each chunk.\n        input_values: An iterable of items that `func` is applied to.\n        chunk_size: Tune the number of items processed in each step. Larger numbers\n            result in smaller communication overhead but less parallelism at the start\n            and end. If `chunk_size` has a `__len__` property, the `chunk_size` is\n            calculated automatically if not given.\n        ignore_exceptions: Ignore chunks if `next()` raises an exception on\n            `input_values`. And return `None` if `func` raised an exception in a worker\n            process.\n        concurrency: Number of new threads to start.\n        unordered: Do not preserve the order of the elements, yield them as soon as they\n            have been processed. This decreases the latency caused by\n            difficult-to-process items.\n\n    Yields:\n        The next result obtained from applying `func` to each input value. May\n            contain `None`-s if `ignore_exceptions=True`. May have different order than\n            the input if `unordered=True`.\n\n    Raises:\n        WorkerException: If there was an error in the `func` function in a background\n            thread and `ignore_exceptions=False`.\n    \"\"\"\n\n    config = get_config(\n        function=func,\n        input_values=input_values,\n        chunk_size=chunk_size,\n        concurrency=concurrency,\n    )\n\n    input_queue: queue.Queue = queue.Queue(config.concurrency * 2)\n    output_queue: queue.Queue = queue.Queue(config.concurrency * 2)\n    should_stop = threading.Event()\n\n    threads = [\n        threading.Thread(\n            name=f\"threaded_parallel_map_{config.function_name}_{i}\",\n            target=mapper_function,\n            daemon=True,\n            kwargs=dict(\n                input_queue=input_queue,\n                output_queue=output_queue,\n                should_stop=should_stop,\n                func=func,\n            ),\n        )\n        for i in range(config.concurrency)\n    ]\n\n    for t in threads:\n        t.start()\n\n    yield from manage_communication(\n        input_values=input_values,\n        chunk_size=config.chunk_size,\n        input_queue=input_queue,\n        output_queue=output_queue,\n        unordered=unordered,\n        ignore_exceptions=ignore_exceptions,\n    )\n    should_stop.set()\n    for t in threads:\n        t.join(1)\n</code></pre>"},{"location":"reference/utilities/#composable-parallel-processing","title":"Composable parallel processing","text":"<p>Because both threaded_parallel_map and parallel_map have a streaming interface, it is easy to compose them and end up with, for example, a process for each CPU core with its own thread-pool or event-loop. Longer pipelines are also easy to imagine. The chunking methods help in these compositions.</p>"},{"location":"reference/utilities/#great_ai.utilities.chunk.chunk","title":"<code>chunk(values, chunk_size)</code>","text":"<p>Turn an iterable of items into an iterable of lists (chunks) of items.</p> <p>Each returned chunk is of length <code>chunk_size</code> except the last one the length of which is between 1 and <code>chunk_size</code>.</p> <p>Useful for parallel processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(chunk(range(10), chunk_size=3))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Iterable[T]</code> <p>The stream of items to pack into chunks.</p> required <code>chunk_size</code> <code>int</code> <p>Desired length of each (but the last) chunk.</p> required <p>Yields:</p> Type Description <code>Iterable[List[T]]</code> <p>The next chunk.</p> Source code in <code>great_ai/utilities/chunk.py</code> <pre><code>def chunk(values: Iterable[T], chunk_size: int) -&gt; Iterable[List[T]]:\n    \"\"\"Turn an iterable of items into an iterable of lists (chunks) of items.\n\n    Each returned chunk is of length `chunk_size` except the last one the length of\n    which is between 1 and `chunk_size`.\n\n    Useful for parallel processing.\n\n    Examples:\n        &gt;&gt;&gt; list(chunk(range(10), chunk_size=3))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n\n    Args:\n        values: The stream of items to pack into chunks.\n        chunk_size: Desired length of each (but the last) chunk.\n\n    Yields:\n        The next chunk.\n    \"\"\"\n\n    assert chunk_size &gt;= 1\n\n    result: List[T] = []\n    for v in values:\n        result.append(v)\n        if len(result) == chunk_size:\n            yield result\n            result = []\n\n    if len(result) &gt; 0:\n        yield result\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.unchunk.unchunk","title":"<code>unchunk(chunks)</code>","text":"<p>Turn a stream of chunks of items into a stream of items (flatten operation).</p> <p>The inverse operation of chunk. Useful for parallel processing.</p> <p>Similar to itertools.chain but ignores <code>None</code> chunks.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(unchunk([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]))\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>Iterable[Optional[Iterable[T]]]</code> <p>Stream of chunks to unpack.</p> required <p>Yields:</p> Type Description <code>Iterable[T]</code> <p>The next item in the flattened iterable.</p> Source code in <code>great_ai/utilities/unchunk.py</code> <pre><code>def unchunk(chunks: Iterable[Optional[Iterable[T]]]) -&gt; Iterable[T]:\n    \"\"\"Turn a stream of chunks of items into a stream of items (flatten operation).\n\n    The inverse operation of [chunk][great_ai.utilities.chunk.chunk].\n    Useful for parallel processing.\n\n    Similar to itertools.chain but ignores `None` chunks.\n\n    Examples:\n        &gt;&gt;&gt; list(unchunk([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]))\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n    Args:\n        chunks: Stream of chunks to unpack.\n\n    Yields:\n        The next item in the flattened iterable.\n    \"\"\"\n\n    for chunk in chunks:\n        if chunk is not None:\n            yield from chunk\n</code></pre>"},{"location":"reference/utilities/#operations","title":"Operations","text":""},{"location":"reference/utilities/#great_ai.utilities.ConfigFile","title":"<code>ConfigFile</code>","text":"<p>             Bases: <code>Mapping[str, str]</code></p> <p>A small and safe <code>INI</code>-style configuration loader with <code>dict</code> and <code>ENV</code> support.</p> <p>The values can be accessed using both dot- and index-notation. It is compatible with the <code>dict</code> interface.</p> <p>File format example:</p> <pre><code># comments are allowed everywhere\n\nkey = value  # you can leave or omit whitespace around the equal-sign\nmy_hashtag = \"#great_ai\" # the r-value can be quoted with \" or ' or `.\n\nmy_var = my_default_value  # Default values can be given to env-vars,\n                           # see next line. The default value must come first.\n\nmy_var = ENV:MY_ENV_VAR    # If the value starts with the `ENV:` prefix,\n                           # it is looked up from the environment variables.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ConfigFile('tests/utilities/data/simple.conf')\nConfigFile(path=tests/utilities/data/simple.conf) {'zeroth_key': 'test', 'first_key': 'Andr\u00e1s'}\n</code></pre> <pre><code>&gt;&gt;&gt; ConfigFile('tests/utilities/data/simple.conf').zeroth_key\n'test'\n</code></pre> <pre><code>&gt;&gt;&gt; ConfigFile('tests/utilities/data/simple.conf').second_key\nTraceback (most recent call last):\n...\nKeyError: 'Key `second_key` is not found in configuration file ...\n</code></pre> <pre><code>&gt;&gt;&gt; a = ConfigFile('tests/utilities/data/simple.conf')\n&gt;&gt;&gt; {**a}\n{'zeroth_key': 'test', 'first_key': 'Andr\u00e1s'}\n</code></pre> Source code in <code>great_ai/utilities/config_file/config_file.py</code> <pre><code>class ConfigFile(Mapping[str, str]):\n    \"\"\"A small and safe `INI`-style configuration loader with `dict` and `ENV` support.\n\n    The values can be accessed using both dot- and index-notation. It is compatible\n    with the `dict` interface.\n\n    File format example:\n\n    ```toml\n    # comments are allowed everywhere\n\n    key = value  # you can leave or omit whitespace around the equal-sign\n    my_hashtag = \"#great_ai\" # the r-value can be quoted with \" or ' or `.\n\n    my_var = my_default_value  # Default values can be given to env-vars,\n                               # see next line. The default value must come first.\n\n    my_var = ENV:MY_ENV_VAR    # If the value starts with the `ENV:` prefix,\n                               # it is looked up from the environment variables.\n    ```\n\n    Examples:\n        &gt;&gt;&gt; ConfigFile('tests/utilities/data/simple.conf')\n        ConfigFile(path=tests/utilities/data/simple.conf) {'zeroth_key': 'test', 'first_key': 'Andr\u00e1s'}\n\n        &gt;&gt;&gt; ConfigFile('tests/utilities/data/simple.conf').zeroth_key\n        'test'\n\n        &gt;&gt;&gt; ConfigFile('tests/utilities/data/simple.conf').second_key\n        Traceback (most recent call last):\n        ...\n        KeyError: 'Key `second_key` is not found in configuration file ...\n\n        &gt;&gt;&gt; a = ConfigFile('tests/utilities/data/simple.conf')\n        &gt;&gt;&gt; {**a}\n        {'zeroth_key': 'test', 'first_key': 'Andr\u00e1s'}\n\n    \"\"\"\n\n    ENVIRONMENT_VARIABLE_KEY_PREFIX = \"ENV\"\n\n    def __init__(self, path: Union[Path, str], *, ignore_missing: bool = False) -&gt; None:\n        \"\"\"Load and parse a configuration file.\n\n        Everything is eager-loaded, thus, exceptions may be thrown here.\n\n        Args:\n            path: Local path of the configuration file.\n            ignore_missing: Don't raise an exception on missing environment variables.\n\n        Raises:\n            FileNotFoundError: If there is no file at the specified path.\n            ParseError: If the provided file does not conform to the expected format.\n            KeyError: If there is duplication in the keys.\n            ValueError: If an environment variable is referenced but it is not set in\n                the system and `ignore_missing=False`.\n        \"\"\"\n\n        if not isinstance(path, Path):\n            path = Path(path)\n\n        if not path.exists():\n            raise FileNotFoundError(path.absolute())\n\n        self._ignore_missing = ignore_missing\n\n        self._path = path\n        self._key_values: Dict[str, str] = {}\n\n        self._parse()\n\n    @property\n    def path(self) -&gt; Path:\n        \"\"\"Original path from where the configuration was loaded.\"\"\"\n        return self._path\n\n    def _parse(self) -&gt; None:\n        with open(self._path, encoding=\"utf-8\") as f:\n            lines: str = f.read()\n\n        matches = pattern.findall(lines)\n        for key, *values in matches:\n            try:\n                value = next(v for v in values if v)\n            except StopIteration:\n                raise ParseError(\n                    f\"\"\"Cannot parse config file ({\n                        self._path.absolute()\n                        }), error at key `{key}`\"\"\"\n                )\n\n            already_exists = key in self._key_values\n            if already_exists and not value.startswith(\n                f\"{self.ENVIRONMENT_VARIABLE_KEY_PREFIX}:\"\n            ):\n                raise KeyError(\n                    f\"Key `{key}` has been already defined and its value is `{self._key_values[key]}`\"\n                )\n\n            if value.startswith(f\"{self.ENVIRONMENT_VARIABLE_KEY_PREFIX}:\"):\n                _, value = value.split(\":\")\n                if value not in os.environ:\n                    issue = f\"\"\"The value of `{key}` contains the \"{\n                        self.ENVIRONMENT_VARIABLE_KEY_PREFIX\n                    }` prefix but `{value}` is not defined as an environment variable\"\"\"\n                    if already_exists:\n                        logger.warning(\n                            f\"\"\"{issue}, using the default value defined above (`{\n                                self._key_values[key]\n                            }`)\"\"\"\n                        )\n                        continue\n                    elif self._ignore_missing:\n                        logger.warning(issue)\n                    else:\n                        raise ValueError(\n                            f\"{issue} and no default value has been provided\"\n                        )\n                else:\n                    value = os.environ[value]\n\n            self._key_values[key] = value\n\n    def __getattr__(self, key: str) -&gt; str:\n        if key in self._key_values:\n            return self._key_values[key]\n        raise KeyError(\n            f\"Key `{key}` is not found in configuration file ({self._path.absolute()})\"\n        )\n\n    __getitem__ = __getattr__\n\n    def __iter__(self) -&gt; Iterator[str]:\n        return iter(self._key_values)\n\n    def __len__(self) -&gt; int:\n        return len(self._key_values)\n\n    def keys(self) -&gt; KeysView[str]:\n        return self._key_values.keys()\n\n    def values(self) -&gt; ValuesView[str]:\n        return self._key_values.values()\n\n    def items(self) -&gt; ItemsView[str, str]:\n        return self._key_values.items()\n\n    def __repr__(self) -&gt; str:\n        return f\"{type(self).__name__}(path={self._path.as_posix()}) {self._key_values}\"\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.ConfigFile.path","title":"<code>path: Path</code>  <code>property</code>","text":"<p>Original path from where the configuration was loaded.</p>"},{"location":"reference/utilities/#great_ai.utilities.ConfigFile.__init__","title":"<code>__init__(path, *, ignore_missing=False)</code>","text":"<p>Load and parse a configuration file.</p> <p>Everything is eager-loaded, thus, exceptions may be thrown here.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Local path of the configuration file.</p> required <code>ignore_missing</code> <code>bool</code> <p>Don't raise an exception on missing environment variables.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If there is no file at the specified path.</p> <code>ParseError</code> <p>If the provided file does not conform to the expected format.</p> <code>KeyError</code> <p>If there is duplication in the keys.</p> <code>ValueError</code> <p>If an environment variable is referenced but it is not set in the system and <code>ignore_missing=False</code>.</p> Source code in <code>great_ai/utilities/config_file/config_file.py</code> <pre><code>def __init__(self, path: Union[Path, str], *, ignore_missing: bool = False) -&gt; None:\n    \"\"\"Load and parse a configuration file.\n\n    Everything is eager-loaded, thus, exceptions may be thrown here.\n\n    Args:\n        path: Local path of the configuration file.\n        ignore_missing: Don't raise an exception on missing environment variables.\n\n    Raises:\n        FileNotFoundError: If there is no file at the specified path.\n        ParseError: If the provided file does not conform to the expected format.\n        KeyError: If there is duplication in the keys.\n        ValueError: If an environment variable is referenced but it is not set in\n            the system and `ignore_missing=False`.\n    \"\"\"\n\n    if not isinstance(path, Path):\n        path = Path(path)\n\n    if not path.exists():\n        raise FileNotFoundError(path.absolute())\n\n    self._ignore_missing = ignore_missing\n\n    self._path = path\n    self._key_values: Dict[str, str] = {}\n\n    self._parse()\n</code></pre>"},{"location":"reference/utilities/#great_ai.utilities.get_logger","title":"<code>get_logger(name, level=logging.INFO, disable_colors=False)</code>","text":"<p>Return a customised logger used throughout the GreatAI codebase.</p> <p>Uses colors, and only prints timestamps when not running inside notebook.</p> Source code in <code>great_ai/utilities/logger/get_logger.py</code> <pre><code>def get_logger(\n    name: str, level: int = logging.INFO, disable_colors: bool = False\n) -&gt; logging.Logger:\n    \"\"\"Return a customised logger used throughout the GreatAI codebase.\n\n    Uses colors, and only prints timestamps when not running inside notebook.\n    \"\"\"\n\n    if name not in loggers:\n        logger = logging.getLogger(name)\n        logger.setLevel(level)\n\n        try:\n            get_ipython()  # type: ignore\n            log_format = \"%(message)s\"\n        except NameError:\n            # will fail outside of a notebook https://ipython.org/\n            log_format = \"%(asctime)s | %(levelname)8s | %(message)s\"\n\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setLevel(level)\n        if not disable_colors:\n            stdout_handler.setFormatter(CustomFormatter(log_format))\n\n        logger.addHandler(stdout_handler)\n        loggers[name] = logger\n\n    return loggers[name]\n</code></pre>"},{"location":"reference/views/","title":"View models","text":""},{"location":"reference/views/#great_ai.Trace","title":"<code>Trace</code>","text":"<p>             Bases: <code>Generic[T]</code>, <code>HashableBaseModel</code></p> <p>Universal structure for storing prediction traces and training data.</p> <p>Attributes:</p> Name Type Description <code>trace_id</code> <code>str</code> <p>UUID4 identifier for uniquely referring to a trace.</p> <code>created</code> <code>str</code> <p>Timestamp of its (original) construction.</p> <code>original_execution_time_ms</code> <code>float</code> <p>Wall-time elapsed while its generating TracingContext was alive.</p> <code>logged_values</code> <code>Dict[str, Any]</code> <p>Values persisted through using <code>@parameter</code> or <code>log_metric()</code>.</p> <code>models</code> <code>List[Model]</code> <p>Marks left by each encountered <code>@use_model</code> decorated function.</p> <code>exception</code> <code>Optional[str]</code> <p>Exception description if any was encountered.</p> <code>output</code> <code>Optional[T]</code> <p>Return value of the function wrapped by GreatAI.</p> <code>feedback</code> <code>Any</code> <p>Feedback obtained using the REST API of <code>add_ground_truth</code>.</p> <code>tags</code> <code>List[str]</code> <p>Tags used for filtering traces. Contains the name of the original function, value of <code>ENVIRONMENT</code>, its split if has any, and either <code>ground_truth</code> or <code>online</code> depending on the origin of the Trace.</p> Source code in <code>great_ai/views/trace.py</code> <pre><code>class Trace(Generic[T], HashableBaseModel):\n    \"\"\"Universal structure for storing prediction traces and training data.\n\n    Attributes:\n        trace_id: UUID4 identifier for uniquely referring to a trace.\n        created: Timestamp of its (original) construction.\n        original_execution_time_ms: Wall-time elapsed while its generating\n            TracingContext was alive.\n        logged_values: Values persisted through using `@parameter` or `log_metric()`.\n        models: Marks left by each encountered `@use_model` decorated function.\n        exception: Exception description if any was encountered.\n        output: Return value of the function wrapped by GreatAI.\n        feedback: Feedback obtained using the REST API of `add_ground_truth`.\n        tags: Tags used for filtering traces. Contains the name of the original\n            function, value of `ENVIRONMENT`, its split if has any, and either\n            `ground_truth` or `online` depending on the origin of the Trace.\n    \"\"\"\n\n    trace_id: str\n    created: str\n    original_execution_time_ms: float\n    logged_values: Dict[str, Any]\n    models: List[Model]\n    exception: Optional[str]\n    output: Optional[T]\n    feedback: Any = None\n    tags: List[str]\n\n    class Config:\n        extra = Extra.ignore\n\n    @property\n    def input(self) -&gt; Any:\n        return (\n            self.logged_values[\"input\"]\n            if list(self.logged_values.keys()) == [\"input\"]\n            else self.logged_values\n        )\n\n    @property\n    def models_flat(self) -&gt; str:\n        return \", \".join(f\"{m.key}:{m.version}\" for m in self.models)\n\n    @property\n    def output_flat(self) -&gt; str:\n        return pformat(self.output, indent=2, compact=True)\n\n    @property\n    def exception_flat(self) -&gt; str:\n        return (\n            \"null\"\n            if self.exception is None\n            else pformat(self.exception, indent=2, compact=True)\n        )\n\n    @property\n    def feedback_flat(self) -&gt; str:\n        return (\n            \"null\"\n            if self.feedback is None\n            else pformat(self.feedback, indent=2, compact=True)\n        )\n\n    @property\n    def tags_flat(self) -&gt; str:\n        return \",\\n\".join(self.tags)\n\n    def to_flat_dict(self, include_original: bool = True) -&gt; Dict[str, Any]:\n        return {\n            **(\n                self.dict()\n                if include_original\n                else {\n                    \"trace_id\": self.trace_id,\n                    \"created\": self.created,\n                    \"original_execution_time_ms\": self.original_execution_time_ms,\n                }\n            ),\n            **{\n                k: v\n                if (isinstance(v, float) or isinstance(v, int))\n                else pformat(v, indent=2, compact=True)\n                for k, v in self.logged_values.items()\n            },\n            \"models_flat\": self.models_flat,\n            \"exception_flat\": self.exception_flat,\n            \"output_flat\": self.output_flat,\n            \"feedback_flat\": self.feedback_flat,\n            \"tags_flat\": self.tags_flat,\n        }\n\n    def __repr__(self) -&gt; str:\n        return f\"\"\"Trace[{type(self.output).__name__}]({\n            pformat(self.dict(), indent=2, compact=True).replace('{ ', '{', 1)\n        })\"\"\"\n</code></pre>"},{"location":"reference/views/#great_ai.RouteConfig","title":"<code>RouteConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>great_ai/views/route_config.py</code> <pre><code>class RouteConfig(BaseModel):\n    prediction_endpoint_enabled: bool = True\n    docs_endpoints_enabled: bool = True\n    dashboard_enabled: bool = True\n    feedback_endpoints_enabled: bool = True\n    trace_endpoints_enabled: bool = True\n    meta_endpoints_enabled: bool = True\n</code></pre>"},{"location":"reference/views/#great_ai.ClassificationOutput","title":"<code>ClassificationOutput</code>","text":"<p>             Bases: <code>HashableBaseModel</code></p> Source code in <code>great_ai/views/outputs/classification_output.py</code> <pre><code>class ClassificationOutput(HashableBaseModel):\n    label: Union[str, int]\n    confidence: float\n    explanation: Optional[Any]\n</code></pre>"},{"location":"reference/views/#great_ai.MultiLabelClassificationOutput","title":"<code>MultiLabelClassificationOutput</code>","text":"<p>             Bases: <code>HashableBaseModel</code></p> Source code in <code>great_ai/views/outputs/multi_label_classification_output.py</code> <pre><code>class MultiLabelClassificationOutput(HashableBaseModel):\n    labels: List[ClassificationOutput] = []\n</code></pre>"},{"location":"reference/views/#great_ai.RegressionOutput","title":"<code>RegressionOutput</code>","text":"<p>             Bases: <code>HashableBaseModel</code></p> Source code in <code>great_ai/views/outputs/regression_output.py</code> <pre><code>class RegressionOutput(HashableBaseModel):\n    value: Union[int, float]\n    explanation: Optional[Any]\n</code></pre>"},{"location":"reference/views/#great_ai.SequenceLabelingOutput","title":"<code>SequenceLabelingOutput</code>","text":"<p>             Bases: <code>HashableBaseModel</code></p> Source code in <code>great_ai/views/outputs/sequence_labeling_output.py</code> <pre><code>class SequenceLabelingOutput(HashableBaseModel):\n    labeled_tokens: List[LabeledToken]\n    explanation: Optional[Any]\n</code></pre>"},{"location":"surveys/surveys/","title":"Surveys","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n\nbest_practices = pd.read_csv(\"Best practices assessment.csv\")\nbest_practices\n</pre> import pandas as pd  best_practices = pd.read_csv(\"Best practices assessment.csv\") best_practices Out[1]: How many years of software engineering experience do you have? How many years of data science experience do you have? Write reusable scripts for data cleaning and merging Make datasets available on shared infrastructure Use versioning for data, model, configurations and training scripts Continuously monitor the behaviour of deployed models Log production predictions with the model\u2019s version and input data Store models in a single format for ease of use Equip with web interface, package image, provide REST API Provide simple API for serving batch and real-time requests Integration with existing data infrastructure Querying, visualising and understanding metrics and event logging Allow experimentation with the inference code Keep the model\u2019s API and documentation together Parallelise feature extraction Cache predictions Async support for top-down chaining models 0 3 1 Strongly agree Agree Strongly agree Neither agree nor disagree Strongly agree Strongly agree Strongly agree Strongly agree Strongly agree Strongly agree Strongly agree Strongly agree Strongly agree Neither agree nor disagree Strongly agree 1 6 2 Neither agree nor disagree Agree Neither agree nor disagree Agree Agree Neither agree nor disagree Disagree Strongly disagree Disagree Strongly disagree Disagree Strongly disagree Disagree Strongly agree Strongly disagree 2 1 5 Strongly disagree Disagree Disagree Strongly disagree Disagree Strongly disagree Strongly disagree Neither agree nor disagree Strongly disagree Disagree Neither agree nor disagree Agree Strongly disagree Strongly disagree Disagree 3 3 3 Agree Agree Disagree Neither agree nor disagree Agree Not applicable Not applicable Neither agree nor disagree Agree Disagree Agree Strongly agree Not applicable Neither agree nor disagree Agree 4 1 7 Neither agree nor disagree Disagree Neither agree nor disagree Disagree Strongly disagree Not applicable Not applicable Disagree Strongly disagree Disagree Disagree Neither agree nor disagree Strongly disagree Strongly agree Not applicable 5 6 0 Strongly agree Strongly agree Agree Neither agree nor disagree Agree Strongly agree Agree Strongly agree Agree Strongly agree Disagree Strongly agree Agree Strongly agree Not applicable 6 2 2 Disagree Neither agree nor disagree Agree Strongly agree Neither agree nor disagree Disagree Strongly agree Disagree Disagree Agree Neither agree nor disagree Neither agree nor disagree Not applicable Disagree Strongly agree 7 1 2 Disagree Neither agree nor disagree Disagree Agree Disagree Strongly disagree Strongly agree Strongly agree Strongly disagree Disagree Agree Strongly disagree Not applicable Strongly disagree Strongly disagree 8 0 1 Strongly disagree Disagree Strongly disagree Disagree Strongly disagree Disagree Agree Strongly disagree Disagree Strongly disagree Disagree Strongly disagree Disagree Disagree Strongly disagree 9 7 1 Strongly agree Strongly agree Agree Strongly agree Agree Agree Strongly agree Strongly disagree Strongly agree Not applicable Agree Agree Strongly agree Strongly agree Agree In\u00a0[2]: Copied! <pre>m = {\n    \"Strongly agree\": 4 / 4,\n    \"Agree\": 3 / 4,\n    \"Neither agree nor disagree\": 2 / 4,\n    \"Disagree\": 1 / 4,\n    \"Strongly disagree\": 0 / 4,\n}\n\nbest_practices_dicts = [v.to_dict() for _, v in best_practices.iterrows()]\n\nbest_practice_score_ds = []\nbest_practice_score_se = []\n\nfor d in best_practices_dicts:\n    ds_experience = int(d[\"How many years of data science experience do you have?\"])\n    del d[\"How many years of data science experience do you have?\"]\n\n    se_experience = int(\n        d[\"How many years of software engineering experience do you have?\"]\n    )\n    del d[\"How many years of software engineering experience do you have?\"]\n\n    scores = [m[v] for v in d.values() if v != \"Not applicable\"]\n    score = sum(scores) / len(scores)\n    best_practice_score_ds.append((ds_experience, score))\n    best_practice_score_se.append((se_experience, score))\n\nbest_practices[\n    \"How many years of software engineering experience do you have?\"\n].median(), best_practices[\n    \"How many years of data science experience do you have?\"\n].median()\n</pre> m = {     \"Strongly agree\": 4 / 4,     \"Agree\": 3 / 4,     \"Neither agree nor disagree\": 2 / 4,     \"Disagree\": 1 / 4,     \"Strongly disagree\": 0 / 4, }  best_practices_dicts = [v.to_dict() for _, v in best_practices.iterrows()]  best_practice_score_ds = [] best_practice_score_se = []  for d in best_practices_dicts:     ds_experience = int(d[\"How many years of data science experience do you have?\"])     del d[\"How many years of data science experience do you have?\"]      se_experience = int(         d[\"How many years of software engineering experience do you have?\"]     )     del d[\"How many years of software engineering experience do you have?\"]      scores = [m[v] for v in d.values() if v != \"Not applicable\"]     score = sum(scores) / len(scores)     best_practice_score_ds.append((ds_experience, score))     best_practice_score_se.append((se_experience, score))  best_practices[     \"How many years of software engineering experience do you have?\" ].median(), best_practices[     \"How many years of data science experience do you have?\" ].median() Out[2]: <pre>(2.5, 2.0)</pre> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\nfrom scipy import stats\n\nbest_practice_score_ds = sorted(best_practice_score_ds)\nplt.scatter(\n    [x for x, y in best_practice_score_ds], [y for x, y in best_practice_score_ds]\n)\nstats.pearsonr(\n    [x for x, y in best_practice_score_ds], [y for x, y in best_practice_score_ds]\n)\n</pre> import matplotlib.pyplot as plt from scipy import stats  best_practice_score_ds = sorted(best_practice_score_ds) plt.scatter(     [x for x, y in best_practice_score_ds], [y for x, y in best_practice_score_ds] ) stats.pearsonr(     [x for x, y in best_practice_score_ds], [y for x, y in best_practice_score_ds] ) Out[3]: <pre>(-0.5440527060340572, 0.10399880919437814)</pre> In\u00a0[4]: Copied! <pre>import re\n\nbest_practice_score_se = sorted(best_practice_score_se)\n\n%matplotlib inline\nplt.rcParams[\"figure.facecolor\"] = \"white\"\nplt.rcParams[\"font.size\"] = 24\nplt.rcParams[\"figure.figsize\"] = (14, 10)\n\nX = [x for x, y in best_practice_score_se]\nY = [y for x, y in best_practice_score_se]\n\nprint(sum(Y) / len(Y))\nsc = plt.scatter(X, Y, c=\"black\", s=[x * 50 for x, y in best_practice_score_ds])\nplt.ylabel(\"Ratio of implemented deployment best practices\")\nplt.xlabel(\"Years of professional software engineering experience\")\n\n\ndef best_fit(X, Y):\n    xbar = sum(X) / len(X)\n    ybar = sum(Y) / len(Y)\n    n = len(X)\n\n    numer = sum([xi * yi for xi, yi in zip(X, Y)]) - n * xbar * ybar\n    denum = sum([xi**2 for xi in X]) - n * xbar**2\n\n    b = numer / denum\n    a = ybar - b * xbar\n    return a, b\n\n\na, b = best_fit(X, Y)\nyfit = [a + b * xi for xi in X]\nplt.plot(X, yfit, \"b\", label=\"Best fit\")\nplt.legend(\n    sc.legend_elements(\"sizes\", num=5)[0],\n    [\n        re.sub(r\"[^\\d]*(\\d+)[^\\d]*\", lambda v: f\"{int(v.group(1)) // 40} years (DS)\", t)\n        for t in sc.legend_elements(\"sizes\", num=5)[1]\n    ],\n)\nplt.tight_layout()\n\nplt.savefig(\"best-practices\", dpi=150)\n\nstats.pearsonr(\n    [x for x, y in best_practice_score_se], [y for x, y in best_practice_score_se]\n)\n</pre> import re  best_practice_score_se = sorted(best_practice_score_se)  %matplotlib inline plt.rcParams[\"figure.facecolor\"] = \"white\" plt.rcParams[\"font.size\"] = 24 plt.rcParams[\"figure.figsize\"] = (14, 10)  X = [x for x, y in best_practice_score_se] Y = [y for x, y in best_practice_score_se]  print(sum(Y) / len(Y)) sc = plt.scatter(X, Y, c=\"black\", s=[x * 50 for x, y in best_practice_score_ds]) plt.ylabel(\"Ratio of implemented deployment best practices\") plt.xlabel(\"Years of professional software engineering experience\")   def best_fit(X, Y):     xbar = sum(X) / len(X)     ybar = sum(Y) / len(Y)     n = len(X)      numer = sum([xi * yi for xi, yi in zip(X, Y)]) - n * xbar * ybar     denum = sum([xi**2 for xi in X]) - n * xbar**2      b = numer / denum     a = ybar - b * xbar     return a, b   a, b = best_fit(X, Y) yfit = [a + b * xi for xi in X] plt.plot(X, yfit, \"b\", label=\"Best fit\") plt.legend(     sc.legend_elements(\"sizes\", num=5)[0],     [         re.sub(r\"[^\\d]*(\\d+)[^\\d]*\", lambda v: f\"{int(v.group(1)) // 40} years (DS)\", t)         for t in sc.legend_elements(\"sizes\", num=5)[1]     ], ) plt.tight_layout()  plt.savefig(\"best-practices\", dpi=150)  stats.pearsonr(     [x for x, y in best_practice_score_se], [y for x, y in best_practice_score_se] ) <pre>0.5157738095238095\n</pre> Out[4]: <pre>(0.6722543326178704, 0.03321124881554773)</pre> In\u00a0[5]: Copied! <pre>tam = pd.read_csv(\"Technology acceptance model questionnaire.csv\")\ntam\n</pre> tam = pd.read_csv(\"Technology acceptance model questionnaire.csv\") tam Out[5]: I believe the use of GreatAI improves the quality of AI deployments. I believe the use of GreatAI would increase my productivity. I believe the use of GreatAI can lead to robust and trustworthy deployments. Overall, I found GreatAI useful when working with AI. I found the GreatAI easy to learn. I found it is easy to employ GreatAI in practice. I found it is easy to integrate GreatAI into an existing project. Overall, I found GreatAI easy to use. Assuming GreatAI is applicable to my task, I predict that I will use it on a regular basis in the future. Overall, I intend to use the GreatAI in my personal or professional projects. 0 7 7 7 7 7 7 7 7 7 7 1 7 7 6 7 5 6 7 6 7 7 2 4 4 5 6 6 5 4 4 5 4 3 6 7 6 7 7 6 7 6 7 6 4 7 7 6 7 4 5 5 5 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 5 7 6 6 7 7 7 7 6 6 6 4 5 3 5 5 6 8 4 5 5 5 7 2 3 3 3 3 9 7 7 7 7 5 6 5 6 7 7 In\u00a0[6]: Copied! <pre>import pingouin\n\npu = [\n    \"I believe the use of GreatAI improves the quality of AI deployments.\",\n    \"I believe the use of GreatAI would increase my productivity.\",\n    \"I believe the use of GreatAI can lead to robust and trustworthy deployments.\",\n    \"Overall, I found GreatAI useful when working with AI.\",\n]\npingouin.cronbach_alpha(tam[pu])\n</pre> import pingouin  pu = [     \"I believe the use of GreatAI improves the quality of AI deployments.\",     \"I believe the use of GreatAI would increase my productivity.\",     \"I believe the use of GreatAI can lead to robust and trustworthy deployments.\",     \"Overall, I found GreatAI useful when working with AI.\", ] pingouin.cronbach_alpha(tam[pu]) Out[6]: <pre>(0.8813771517996869, array([0.688, 0.967]))</pre> In\u00a0[7]: Copied! <pre>peou = [\n    \"I found the GreatAI easy to learn.\",\n    \"I found it is easy to employ GreatAI in practice.\",\n    \"I found it is easy to integrate GreatAI into an existing project.\",\n    \"Overall, I found GreatAI easy to use.\",\n]\npingouin.cronbach_alpha(tam[peou])\n</pre> peou = [     \"I found the GreatAI easy to learn.\",     \"I found it is easy to employ GreatAI in practice.\",     \"I found it is easy to integrate GreatAI into an existing project.\",     \"Overall, I found GreatAI easy to use.\", ] pingouin.cronbach_alpha(tam[peou]) Out[7]: <pre>(0.7729220222793487, array([0.403, 0.937]))</pre> In\u00a0[8]: Copied! <pre>itu = [\n    \"Assuming GreatAI is applicable to my task, I predict that I will use it on a regular basis in the future.\",\n    \"Overall, I intend to use the GreatAI in my personal or professional projects.\",\n]\npingouin.cronbach_alpha(tam[itu])\n</pre> itu = [     \"Assuming GreatAI is applicable to my task, I predict that I will use it on a regular basis in the future.\",     \"Overall, I intend to use the GreatAI in my personal or professional projects.\", ] pingouin.cronbach_alpha(tam[itu]) Out[8]: <pre>(0.9538950715421304, array([0.814, 0.989]))</pre> In\u00a0[9]: Copied! <pre>tam[\"pu\"] = tam[pu].mean(1)\ntam[\"peou\"] = tam[peou].mean(1)\ntam[\"itu\"] = tam[itu].mean(1)\ntam[[\"pu\", \"peou\", \"itu\"]]\n</pre> tam[\"pu\"] = tam[pu].mean(1) tam[\"peou\"] = tam[peou].mean(1) tam[\"itu\"] = tam[itu].mean(1) tam[[\"pu\", \"peou\", \"itu\"]] Out[9]: pu peou itu 0 7.00 7.00 7.0 1 6.75 6.00 7.0 2 4.75 4.75 4.5 3 6.50 6.50 6.5 4 6.75 4.75 6.0 5 6.00 6.00 6.0 6 5.50 6.00 7.0 7 6.25 4.25 5.5 8 4.75 3.75 3.0 9 7.00 5.50 7.0 In\u00a0[10]: Copied! <pre>tam[[\"pu\", \"peou\", \"itu\"]].mean()\n</pre> tam[[\"pu\", \"peou\", \"itu\"]].mean() Out[10]: <pre>pu      6.125\npeou    5.450\nitu     5.950\ndtype: float64</pre> In\u00a0[11]: Copied! <pre>tam[[\"pu\", \"peou\", \"itu\"]].median()\n</pre> tam[[\"pu\", \"peou\", \"itu\"]].median() Out[11]: <pre>pu      6.375\npeou    5.750\nitu     6.250\ndtype: float64</pre> In\u00a0[12]: Copied! <pre>tam[[\"pu\", \"peou\", \"itu\"]].std()\n</pre> tam[[\"pu\", \"peou\", \"itu\"]].std() Out[12]: <pre>pu      0.859990\npeou    1.039498\nitu     1.321825\ndtype: float64</pre> In\u00a0[13]: Copied! <pre>stats.pearsonr(tam[\"peou\"], tam[\"pu\"])\n</pre> stats.pearsonr(tam[\"peou\"], tam[\"pu\"]) Out[13]: <pre>(0.5515422017785757, 0.09838124227663879)</pre> In\u00a0[14]: Copied! <pre>stats.pearsonr(tam[\"peou\"], tam[\"itu\"])\n</pre> stats.pearsonr(tam[\"peou\"], tam[\"itu\"]) Out[14]: <pre>(0.8066270322592023, 0.004809023073123024)</pre> In\u00a0[15]: Copied! <pre>stats.pearsonr(tam[\"pu\"], tam[\"itu\"])\n</pre> stats.pearsonr(tam[\"pu\"], tam[\"itu\"]) Out[15]: <pre>(0.7880605510627579, 0.006774486564715021)</pre>"},{"location":"tutorial/","title":"Train and deploy a SOTA model","text":"<p>Let's see <code>great-ai</code> in action by going over the lifecycle of a simple service.</p>"},{"location":"tutorial/#objectives","title":"Objectives","text":"<ol> <li>You will see how the great_ai.utilities can integrate into your Data Science workflow.</li> <li>You will use great_ai.large_file to version and store your trained model.</li> <li>You will use GreatAI to prepare your model for a robust and responsible deployment.</li> </ol>"},{"location":"tutorial/#overview","title":"Overview","text":"<p>You will train a field of study (domain) classifier for scientific sentences. The exact task was proposed by the SciBERT paper in which SciBERT achieved an F1-score of 0.6571. We are going to outperform it using a trivial text classification model: a Linear SVM.</p> <p>We use the same synthetic dataset derived from the Microsoft Academic Graph. The dataset is available here.</p> <p>Success</p> <p>You are ready to start the tutorial. Feel free to return to the summary section once you're finished.</p> <p> Train it</p> <p> Deploy it</p>"},{"location":"tutorial/#summary","title":"Summary","text":""},{"location":"tutorial/#training-notebook","title":"Training notebook","text":"<p>We load and preprocess the dataset while relying on great_ai.utilities.clean for doing the heavy-lifting. Additionally, the preprocessing is parallelised using great_ai.utilities.simple_parallel_map</p> <p>After training and evaluating a model, it is exported using great_ai.save_model.</p> Remote storage <p>To store your model remotely, you must set your credentials before calling <code>save_model</code>.</p> <p>For example, to use AWS S3: <pre><code>from great_ai.large_file import LargeFileS3\n\nLargeFileS3.configure(\n    aws_region_name='eu-west-2',\n    aws_access_key_id='MY_AWS_ACCESS_KEY',\n    aws_secret_access_key='MY_AWS_SECRET_KEY',\n    large_files_bucket_name='my_bucket_for_models'\n)\n\nfrom great_ai import save_model\n\nsave_model(model, key='my-domain-predictor')\n</code></pre></p> <p>For more info, checkout the configuration how-to page.</p>"},{"location":"tutorial/#deployment-notebook","title":"Deployment notebook","text":"<p>We create an inference function that can be hardened by wrapping it in a GreatAI instance.</p> <pre><code>from great_ai import GreatAI, use_model\nfrom great_ai.utilities import clean\n\n@GreatAI.create\n@use_model('my-domain-predictor')   #(1)\ndef predict_domain(sentence, model):\n    inputs = [clean(sentence)]\n    return str(model.predict(inputs)[0])\n</code></pre> <ol> <li>@use_model loads and injects your model into the <code>predict_domain</code> function's <code>model</code> argument.     You can freely reference it, knowing that the function is always provided with it.</li> </ol> <p>Finally, we test the model's inference function through the GreatAI dashboard. The only thing left is to deploy the hardened service properly.</p> <p> Learn about all the features</p> <p> Look at more examples</p>"},{"location":"tutorial/deploy/","title":"Harden and deploy your app","text":"In\u00a0[1]: Copied! <pre>from great_ai import GreatAI, use_model\nfrom great_ai.utilities import clean\n\n\n@GreatAI.create\n@use_model(\"my-domain-predictor\")\ndef predict_domain(sentence, model):\n    inputs = [clean(sentence)]\n    return str(model.predict(inputs)[0])\n</pre> from great_ai import GreatAI, use_model from great_ai.utilities import clean   @GreatAI.create @use_model(\"my-domain-predictor\") def predict_domain(sentence, model):     inputs = [clean(sentence)]     return str(model.predict(inputs)[0]) <pre>Environment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\nCannot find credentials files, defaulting to using ParallelTinyDbDriver\nThe selected tracing database (ParallelTinyDbDriver) is not recommended for production\nCannot find credentials files, defaulting to using LargeFileLocal\nGreatAI (v0.1.4): configured \u2705\n  \ud83d\udd29 tracing_database: ParallelTinyDbDriver\n  \ud83d\udd29 large_file_implementation: LargeFileLocal\n  \ud83d\udd29 is_production: False\n  \ud83d\udd29 should_log_exception_stack: True\n  \ud83d\udd29 prediction_cache_size: 512\n  \ud83d\udd29 dashboard_table_size: 50\nYou still need to check whether you follow all best practices before trusting your deployment.\n&gt; Find out more at https://se-ml.github.io/practices\nFetching cached versions of my-domain-predictor\nLatest version of my-domain-predictor is 9 (from versions: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\nFile my-domain-predictor-9 found in cache\n</pre> In\u00a0[2]: Copied! <pre>predict_domain(\"Mountains are just big rocks.\")\n</pre> predict_domain(\"Mountains are just big rocks.\") Out[2]: <pre>Trace[str]({'created': '2022-07-12T13:34:26.743292',\n  'exception': None,\n  'feedback': None,\n  'logged_values': { 'arg:sentence:length': 29,\n                     'arg:sentence:value': 'Mountains are just big rocks.'},\n  'models': [{'key': 'my-domain-predictor', 'version': 9}],\n  'original_execution_time_ms': 6.9699,\n  'output': 'geography',\n  'tags': ['predict_domain', 'online', 'development'],\n  'trace_id': 'c80bdee3-602b-49dd-a84d-6eef80127e5a'})</pre> <p>Notice how the original return value is under the <code>.output</code> key. Additionally, a plethora of metadata has been added which will be useful later on.</p> <p>Running your app in development-mode is as easy as executing <code>great-ai deploy.ipynb</code> from your terminal.</p> In\u00a0[3]: Copied! <pre>!great-ai deploy.ipynb\n# leave this running and open http://127.0.0.1:6060\n</pre> !great-ai deploy.ipynb # leave this running and open http://127.0.0.1:6060 <pre>2022-07-12 15:34:28 |     INFO | Converting notebook to Python script\n2022-07-12 15:34:29 |     INFO | Found `predict_domain` to be the GreatAI app \n2022-07-12 15:34:29 |     INFO | Uvicorn running on http://0.0.0.0:6060 (Press CTRL+C to quit)\n2022-07-12 15:34:31 |  WARNING | Environment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\n2022-07-12 15:34:31 |  WARNING | Cannot find credentials files, defaulting to using ParallelTinyDbDriver\n2022-07-12 15:34:31 |  WARNING | The selected tracing database (ParallelTinyDbDriver) is not recommended for production\n2022-07-12 15:34:31 |  WARNING | Cannot find credentials files, defaulting to using LargeFileLocal\n2022-07-12 15:34:31 |     INFO | GreatAI (v0.1.4): configured \u2705\n2022-07-12 15:34:31 |     INFO |   \ud83d\udd29 tracing_database: ParallelTinyDbDriver\n2022-07-12 15:34:31 |     INFO |   \ud83d\udd29 large_file_implementation: LargeFileLocal\n2022-07-12 15:34:31 |     INFO |   \ud83d\udd29 is_production: False\n2022-07-12 15:34:31 |     INFO |   \ud83d\udd29 should_log_exception_stack: True\n2022-07-12 15:34:31 |     INFO |   \ud83d\udd29 prediction_cache_size: 512\n2022-07-12 15:34:31 |     INFO |   \ud83d\udd29 dashboard_table_size: 50\n2022-07-12 15:34:31 |  WARNING | You still need to check whether you follow all best practices before trusting your deployment.\n2022-07-12 15:34:31 |  WARNING | &gt; Find out more at https://se-ml.github.io/practices\n2022-07-12 15:34:31 |     INFO | Fetching cached versions of my-domain-predictor\n2022-07-12 15:34:31 |     INFO | Latest version of my-domain-predictor is 9 (from versions: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n2022-07-12 15:34:31 |     INFO | File my-domain-predictor-9 found in cache\n2022-07-12 15:34:31 |     INFO | Started server process [199794]\n2022-07-12 15:34:31 |     INFO | Waiting for application startup.\n2022-07-12 15:34:31 |     INFO | Application startup complete.\n^C\n2022-07-12 15:34:33 |     INFO | Shutting down\n2022-07-12 15:34:33 |     INFO | Waiting for application shutdown.\n2022-07-12 15:34:33 |     INFO | Application shutdown complete.\n2022-07-12 15:34:33 |     INFO | Finished server process [199794]\n</pre> <p>Congrats, you've just created your first GreatAI service! \ud83c\udf89</p> <p>Now that you've made sure your application is hardened enough for the intended use case, it is time to deploy it. The responsibilities of GreatAI end when it wraps your inference function and model into a production-ready service. You're given the freedom and responsibility to deploy this service. Fortunately, you (or your organisation) probably already have an established routine for deploying services.</p> <p>There are three main approaches to deploy a GreatAI service: For more info about them, check out the deployment how-to.</p> <p>For more thorough examples, see the examples page.</p>"},{"location":"tutorial/deploy/#harden-and-deploy-your-app","title":"Harden and deploy your app\u00b6","text":"<p>Finally, it's time to deploy your model. But before that, you have to make sure you follow AI deployment best practices. In the past, this step was too often either the source of unexpected struggle, or worse, simply ignored.</p> <p>With <code>GreatAI</code>, it has become a matter of 4 lines of code.</p>"},{"location":"tutorial/deploy/#go-back-to-the-summary","title":"Go back to the summary\u00b6","text":""},{"location":"tutorial/train/","title":"Train your model","text":"In\u00a0[1]: Copied! <pre>%pip install great-ai &gt; /dev/null\n</pre> %pip install great-ai &gt; /dev/null <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> <p>First, we have to get some data. After downloading it from here, we might notice that the dataset is in JSON Lines format (each line is a separate JSON document).</p> <p>Let's write a function which takes a single line and returns the sentence and the corresponding label from it. Before returning, the sentence is also cleaned to remove any LaTeX, XML, unicode, PDF-extraction artifacts.</p> In\u00a0[2]: Copied! <pre>import json\nfrom great_ai.utilities import clean\n\n\ndef preprocess(line):\n    data_point = json.loads(line)\n\n    sentence = data_point[\"text\"]\n    label = data_point[\"label\"]\n\n    return clean(sentence), label\n</pre> import json from great_ai.utilities import clean   def preprocess(line):     data_point = json.loads(line)      sentence = data_point[\"text\"]     label = data_point[\"label\"]      return clean(sentence), label <p>Now, we can load the dataset and extract the training samples from it. Since we're impatient, we can do it in parallel using the <code>simple_parallel_map</code> function.</p> <p>Open files in Python are iterable: in text mode, each iteration returns the next line.</p> In\u00a0[3]: Copied! <pre>from great_ai.utilities import simple_parallel_map\n\nwith open(\"data/train.txt\", encoding=\"utf-8\") as f:\n    training_data = simple_parallel_map(preprocess, f)\n\nX_train = [d[0] for d in training_data]\ny_train = [d[1] for d in training_data]\n</pre> from great_ai.utilities import simple_parallel_map  with open(\"data/train.txt\", encoding=\"utf-8\") as f:     training_data = simple_parallel_map(preprocess, f)  X_train = [d[0] for d in training_data] y_train = [d[1] for d in training_data] <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 84000/84000 [00:09&lt;00:00, 8960.63it/s] \n</pre> <p>Let's do the same for the test data.</p> In\u00a0[4]: Copied! <pre>with open(\"data/test.txt\", encoding=\"utf-8\") as f:\n    test_data = simple_parallel_map(preprocess, f)\n\nX_test = [d[0] for d in test_data]\ny_test = [d[1] for d in test_data]\n</pre> with open(\"data/test.txt\", encoding=\"utf-8\") as f:     test_data = simple_parallel_map(preprocess, f)  X_test = [d[0] for d in test_data] y_test = [d[1] for d in test_data] <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22399/22399 [00:03&lt;00:00, 6078.02it/s]\n</pre> <p>After obtaining some clean data, it's time to create and train a model on it. We are going to use scikit-learn for this purpose.</p> <p>In this case, we opted for a linear Support Vector Machine because it's known to be an adequate baseline for simple classification tasks.</p> In\u00a0[5]: Copied! <pre>from sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n\nmodel = make_pipeline(TfidfVectorizer(), LinearSVC())\n# todo: hyperparameter-optimisation\nmodel.fit(X_train, y_train)\n</pre> from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.svm import LinearSVC  model = make_pipeline(TfidfVectorizer(), LinearSVC()) # todo: hyperparameter-optimisation model.fit(X_train, y_train) Out[5]: <pre>Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('linearsvc', LinearSVC())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('linearsvc', LinearSVC())])</pre>TfidfVectorizer<pre>TfidfVectorizer()</pre>LinearSVC<pre>LinearSVC()</pre> <p>It's already finished. Let's see how well it performs. But first, we need to configure matplotlib to make the charts more legible.</p> <p><code>ConfusionMatrixDisplay</code> relies on <code>matplotlib</code> for rendering.</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (30, 15)\nplt.rcParams[\"figure.facecolor\"] = \"white\"\nplt.rcParams[\"font.size\"] = 12\n</pre> import matplotlib.pyplot as plt  %matplotlib inline plt.rcParams[\"figure.figsize\"] = (30, 15) plt.rcParams[\"figure.facecolor\"] = \"white\" plt.rcParams[\"font.size\"] = 12 <p>Next, we check the quality of the model on the <code>test</code> split. We can use the classification report to check the common metrics, such as the macro F1-score. We also draw the confusion matrix to get some insights into the types of mistakes being made by the model.</p> In\u00a0[7]: Copied! <pre>from sklearn import metrics\n\ny_predicted = model.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_predicted))\nmetrics.ConfusionMatrixDisplay.from_predictions(y_test, y_predicted)\nNone\n</pre> from sklearn import metrics  y_predicted = model.predict(X_test)  print(metrics.classification_report(y_test, y_predicted)) metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_predicted) None <pre>              precision    recall  f1-score   support\n\n    business       0.67      0.69      0.68      3198\n   economics       0.69      0.71      0.70      3189\n   geography       0.70      0.73      0.72      3207\n    medicine       0.90      0.91      0.90      3187\n    politics       0.63      0.57      0.60      3169\n  psychology       0.73      0.73      0.73      3252\n   sociology       0.51      0.51      0.51      3197\n\n    accuracy                           0.69     22399\n   macro avg       0.69      0.69      0.69     22399\nweighted avg       0.69      0.69      0.69     22399\n\n</pre> <p>Great work, we can be rightfully satisfied with our model. Seeing the results, we achieved an F1-score of 0.69 which is about 5% better than SciBERT's 0.6571!</p> <p>You might wonder that \"this is great, but besides some utility functions (<code>clean</code>, <code>simple_parallel_map</code>, ...) what more value does GreatAI add?\". This would be a valid argument because the scope of GreatAI actually only starts here.</p> <p>Not coincidentally, this is the point where the scope of Data Science ends but it's still a grey zone for software engineering.</p> <p>In order to use this model in production, we have to make it available on some possibly shared infrastructure.</p> In\u00a0[8]: Copied! <pre>from great_ai import save_model\n\nsave_model(model, key=\"my-domain-predictor\")\n</pre> from great_ai import save_model  save_model(model, key=\"my-domain-predictor\") <pre>Environment variable ENVIRONMENT is not set, defaulting to development mode \u203c\ufe0f\nCannot find credentials files, defaulting to using ParallelTinyDbDriver\nThe selected tracing database (ParallelTinyDbDriver) is not recommended for production\nCannot find credentials files, defaulting to using LargeFileLocal\nGreatAI (v0.1.4): configured \u2705\n  \ud83d\udd29 tracing_database: ParallelTinyDbDriver\n  \ud83d\udd29 large_file_implementation: LargeFileLocal\n  \ud83d\udd29 is_production: False\n  \ud83d\udd29 should_log_exception_stack: True\n  \ud83d\udd29 prediction_cache_size: 512\n  \ud83d\udd29 dashboard_table_size: 50\nYou still need to check whether you follow all best practices before trusting your deployment.\n&gt; Find out more at https://se-ml.github.io/practices\nFetching cached versions of my-domain-predictor\nCopying file for my-domain-predictor-9\nCompressing my-domain-predictor-9\nModel my-domain-predictor uploaded with version 9\n</pre> Out[8]: <pre>'my-domain-predictor:9'</pre>"},{"location":"tutorial/train/#train-your-model","title":"Train your model\u00b6","text":"<p>The first step is, of course, to install <code>great-ai</code> in your Python environment.</p>"},{"location":"tutorial/train/#lets-continue-by-finishing-the-deployment-in-the-next-notebook","title":"Let's continue by finishing the deployment in the next notebook.\u00b6","text":""}]}